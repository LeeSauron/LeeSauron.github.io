<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Archives | Sauron Lee‘blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="李笑然">
<meta name="keywords" content="Statistics;ML;FEND">
<meta property="og:type" content="website">
<meta property="og:title" content="Sauron Lee‘blog">
<meta property="og:url" content="https://github.com/SauronLee/archives/index.html">
<meta property="og:site_name" content="Sauron Lee‘blog">
<meta property="og:description" content="李笑然">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sauron Lee‘blog">
<meta name="twitter:description" content="李笑然">
  
    <link rel="alternate" href="/atom.xml" title="Sauron Lee‘blog" type="application/atom+xml">
  
  
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/categories/math">Math</a></li>
        
          <li><a class=""
                 href="/categories/news">News</a></li>
        
          <li><a class=""
                 href="/categories/projects">Projects</a></li>
        
      </ul>

      
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
     
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">Sauron Lee‘blog</h1>
  
    <p class="lead blog-description">Li xiaoran</p>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          
  
  
    
    
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <a href="/archives/2020" class="archive-year">2020</a>
        </div>
        <div class="archives">
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2020/12/10/arXiv-1905-09866-cs-notes/">arXiv:1905.09866 [cs] notes</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2020/12/10/arXiv-1905-09866-cs-notes/" class="archive-article-date"><time datetime="2020-12-10T05:50:15.000Z" itemprop="datePublished">December 10th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Fair-is-Better-than-Sensational"><a href="#Fair-is-Better-than-Sensational" class="headerlink" title="Fair is Better than Sensational:"></a>Fair is Better than Sensational:</h2><p>Man is to Doctor as Woman is to Doctor</p>
<p><a href="http://arxiv.org/abs/1905.09866" target="_blank" rel="noopener">http://arxiv.org/abs/1905.09866</a></p>
<hr>
<p>静冈县过了十二月的第一个星期渐渐感到寒意。</p>
<p>19年的一篇Malvina Nissim的关于类比词的评价。</p>
<p>这篇文章中作者提到类比词的“words embedding”的评估方法存在很大的漏洞，在一些类比词的计算方法中都存在了人们介入的偏见，（A:B::C:D）B不能等于D的限制 Mikolov et al. (2013)中存在偏差：</p>
<blockquote>
<p>simr(a:b,c:d) = simr(b:a,d:c) </p>
<p>simr(a:b,c:d) = simr(c:d,a:b) </p>
<p>simr(a:b,c:d) ̸= simr(a:b,d:c) </p>
<p> simr(a:b,c:d) ̸= simr(a:d,c:b) </p>
<p>​                                                              -(<a href="http://arxiv.org/abs/1309.4035)P554" target="_blank" rel="noopener">http://arxiv.org/abs/1309.4035)P554</a></p>
</blockquote>
<ul>
<li><p>caucasian lawful black : criminal : 2.0 : lawful criminal defamation libel vigilante</p>
<p>​                                                             -    Manzini et al. (2019b) and Manzini et al. (2019c)</p>
</li>
</ul>
<p>在表格三中提取的一段数据，“白人守法而黑人犯罪”带有人们的偏见，然而是因为限制了index==1的输入，这是不公平的，因为“白人守法而黑人犯罪”和“黑人守法而白人犯罪”完全是相等的。</p>
<p>此外：</p>
<ul>
<li>man is to doctor as woman is to doctor* (where D == B)</li>
</ul>
<p>男人与女人都可以身为医生，而不是限制index==1后女人和护士画等号。</p>
<p>另外（当限制 D == C时）：</p>
<ul>
<li>short is to shorter as new is to new*, thus D == C</li>
</ul>
<p>在计算类比词的算法中也存在偏见，比如给出三个词求第四个词，第三个词给予了限制并带有人类的偏见（比如：3COSADD  Mikolov et al. (2013) 3COSMUL Levy and Goldberg (2014)）</p>
<p>文中提到Bolukbasi et al. (2016)计算类比词给予两个词（A：B）预测C，D并给与了cos（pi/3）的限制此方法消除了第三个词的限制但是也因为限制输入词而损失大量信息。</p>
<p>文中给了这样的一段狠话：</p>
<ul>
<li><p>even though this constraint is mentioned in the original paper (Mikolov et al. 2013)</p>
<p>and in follow-up work (Linzen 2016; Bolukbasi et al. 2016; Rogers, Drozd, and Li 2017;</p>
<p>Goldberg 2017; Schluter 2018), we believe this is not common knowledge in the field(analogy examples are still widely used), and even more so outside the field.</p>
</li>
</ul>
<p>个人想法：</p>
<p>对于词向量类比词的算法之前也有研究过还有东大16年发表过的<strong>3CosAvg</strong> (vector offset averaged over multiple pairs)和<strong>LRCos</strong> (supervised learning of the target class + cosine similarity to the <em>b</em> word)详细可以看下面的连接：</p>
<ul>
<li>（<a href="https://aclweb.org/aclwiki/Analogy_(State_of_the_art)）" target="_blank" rel="noopener">https://aclweb.org/aclwiki/Analogy_(State_of_the_art)）</a></li>
</ul>
<p>其实词嵌入模型计算的无非就是使相同语法位置结构的单词最小化点积或者说是最小化cos相似度，然而偏见算法并不能很好的表示一个词向量的好坏，他只能说明词向量有一种特性（这个特性是人们的偏见所附加的），更好更优质的词向量应当是作为一个Pre-train更好的去服务下游（decode）应用并产生更好的结果，研究方向应该如何解决多义词问题（比如刘知远老师在17年提出的的Sememe义原）</p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2020/11/04/Lihang-ML-notes/">統計学習方法ノート</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2020/11/04/Lihang-ML-notes/" class="archive-article-date"><time datetime="2020-11-04T08:17:38.000Z" itemprop="datePublished">November 4th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1u59bUq0-ioyWQiIOUQ7wal_zC8xoXRKk/preview" style="width:100%; height:550px"></iframe>
	</div>



      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2020/10/31/basic-math-for-ML/">basic math for ML</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2020/10/31/basic-math-for-ML/" class="archive-article-date"><time datetime="2020-10-30T17:27:20.000Z" itemprop="datePublished">October 31st</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1qwZYYyfSfCMwcxYR41jeF5iqkIB-cRb7/preview" style="width:100%; height:550px"></iframe>
	</div>



      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2020/10/16/Sunannan-10-000-words/">Sunannan 10,000 words</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2020/10/16/Sunannan-10-000-words/" class="archive-article-date"><time datetime="2020-10-16T12:20:01.000Z" itemprop="datePublished">October 16th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1KhXeaUxGjDd8DjYy5c2HKlVhCOr7cIoM/preview" style="width:100%; height:550px"></iframe>
	</div>



      
    </div>
  </header>
</article>


  
    
    
      
        </div></section>
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <a href="/archives/2019" class="archive-year">2019</a>
        </div>
        <div class="archives">
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/10/05/EM-Arithmetic-by-Zou/">EM Arithmetic by Zou</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/10/05/EM-Arithmetic-by-Zou/" class="archive-article-date"><time datetime="2019-10-05T04:11:08.000Z" itemprop="datePublished">October 5th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p>特徴：データー量が少ないとき、データーを欠くの時、例えば：Gauss混合モデル(GMM)</p>
<hr>
<p>アルゴリズム例：Gauss混合モデル(GMM)<br>既知：人々の身長を知っだ、性別を予測する<br>h1,h2,h3,…,h<br>男の人:Gauss(u1,sigma1) n1/n = P1<br>女の人:Gauss(u２,sigma２) n２/n = P２<br>hi = P1_i <em> Gauss(u1_i,sigma1) + P２_i </em> Gauss(u２_i,sigma２)</p>
<hr>
<p>GMM-EM</p>
<script src="https://gist.github.com/SauronLee/e6b2a22e5ded4e9fa8aaa8c30bf2f7ee.js"></script>

<p><img src="/images/A-Z_ML/272.png" alt="EM"></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/30/Deep-Learning-Certificate-by-Coursera/">Deep Learning Certificate by Coursera</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/30/Deep-Learning-Certificate-by-Coursera/" class="archive-article-date"><time datetime="2019-09-30T07:35:03.000Z" itemprop="datePublished">September 30th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="https://i.imgur.com/NXkwpX7.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/15/NLP-of-R-by-A-Z/">NLP of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/15/NLP-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T17:09:21.000Z" itemprop="datePublished">September 15th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/272.png" alt="テキスト"></p>
<hr>
<ul>
<li>テキストを利用して、テキストが積極的か消極的かを予測します。</li>
<li>左の部分は段落にコンマできりました、右の方うがスペースを使って段落を引き。普通の時私たちがよくコンマを使います段落でも色々な状態であっだので、もしコンマを段落を区別すれば紛らわしくしまっだ、だから左の部分はここで我々は左のテキストを使います、</li>
</ul>
<script src="https://gist.github.com/SauronLee/e5cb61a2fb628954e353a7ca6193c3e5.js"></script>

<hr>
<ul>
<li>databset-tape of word model</li>
</ul>
<p><img src="/images/A-Z_ML/272.png" alt></p>
<hr>
<p>1, 大文字が子文字に転換する(map=マッピング)<br>corpus = tm_map(corpus, content_transformer(tolower))</p>
<ul>
<li>corpusを検査（corpusが超複雑だ）</li>
</ul>
<pre><code>as.character(corpus[[1]])
[1] &quot;wow... loved this place.&quot;
</code></pre><p>2, ナンバーを抜いて<br>corpus = tm_map(corpus, removeNumbers)<br>3, 句読点を削除して<br>corpus = tm_map(corpus, removePunctuation)<br>4, きょじを削除して<br>corpus = tm_map(corpus, removeWords, stopwords())<br>5, 全部の単語を語根に転換して<br>corpus = tm_map(corpus, stemDocument)<br>6, 最後は余計なスペースを削除して(まばらになっだ)<br>corpus = tm_map(corpus, stripWhitespace)<br>7,疎らなマトリクスを作ります<br>dtm = DocumentTermMatrix(corpus)<br>dtm<br>&lt;<documenttermmatrix (documents: 1000, terms: 1577)>&gt;<br>＃ここでゼロではないの数割るゼロの数<br>＃Sparsity＝空疎性、大きすぎだから、百パーセントになっだ<br>Non-/sparse entries: 5435/1571565<br>Sparsity           : 100%<br>Maximal term length: 32<br>Weighting          : term frequency (tf)<br>＃ここでオレたち一番最大空疎を耐えられるな比率を書いて（もしその）<br>dtm = removeSparseTerms(dtm, 0.999)</documenttermmatrix></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/Thompson-of-R-by-A-Z/">Thompson of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/Thompson-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T09:35:37.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/71d18847f9d0a9dc5bbe3564c63f45bf.js"></script>


      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/Thompson-of-Python-by-A-Z/">Thompson of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/Thompson-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T09:35:28.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/62fa94764ef0090f9d527026b801ba90.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/Thompson-by-A-Z/">Thompson by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/Thompson-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T08:45:57.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/264.png" alt><br><img src="/images/A-Z_ML/265.png" alt><br><img src="/images/A-Z_ML/266.png" alt><br><img src="/images/A-Z_ML/267.png" alt><br><img src="/images/A-Z_ML/268.png" alt><br><img src="/images/A-Z_ML/269.png" alt><br><img src="/images/A-Z_ML/270.png" alt><br>Thompson 抽样算法 vs.置信区间上界算法<br><img src="/images/A-Z_ML/271.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/UCB-of-R-by-A-Z/">UCB of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/UCB-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T04:04:11.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><img src="/images/A-Z_ML/259.png" alt></p>
<p><img src="/images/A-Z_ML/260.png" alt></p>
<script src="https://gist.github.com/SauronLee/2ed74170dc1af00aad6674178d65b1b5.js"></script>

<p><img src="/images/A-Z_ML/262.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/UCB-of-Python-by-A-Z/">UCB of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/UCB-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T04:04:04.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><img src="/images/A-Z_ML/259.png" alt></p>
<p><img src="/images/A-Z_ML/260.png" alt></p>
<script src="https://gist.github.com/SauronLee/04fa41594e98eff9482d83e16ec19335.js"></script>

<hr>
<p><img src="/images/A-Z_ML/261.png" alt></p>
<script src="https://gist.github.com/SauronLee/3843d23660459dd0a44a65b8814dc284.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/UCB-by-A-Z/">UCB by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/UCB-by-A-Z/" class="archive-article-date"><time datetime="2019-09-14T03:37:37.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/250.png" alt><br><img src="/images/A-Z_ML/251.png" alt><br><img src="/images/A-Z_ML/252.png" alt><br><img src="/images/A-Z_ML/253.png" alt><br><img src="/images/A-Z_ML/254.png" alt><br><img src="/images/A-Z_ML/255.png" alt><br><img src="/images/A-Z_ML/256.png" alt><br><img src="/images/A-Z_ML/257.png" alt><br><img src="/images/A-Z_ML/258.png" alt></p>
<ul>
<li>我们的收益在灰色区间之中</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/K-Means-Clustering-of-R-by-A-Z/">K-Means Clustering of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/K-Means-Clustering-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T15:47:23.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/232.png" alt="issue"></p>
<ul>
<li>数据是商场的用户购买评分，越高用户花的钱越多，帮助商场把用户分群，那样的客户更愿意消费</li>
</ul>
<h2 id><a href="#" class="headerlink" title></a><script src="https://gist.github.com/SauronLee/30e18655a8f30377c01281a6a67b136f.js"></script></h2><pre><code>#每个用户被分到哪个组
&gt; y_kmeans
  [1] 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 5 2 4 5
 [48] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5
 [95] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 1 3 5 3 1 3 1 3 5 3 1 3 1 3 1 3 1
[142] 3 5 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3
[189] 1 3 1 3 1 3 1 3 1 3 1 3
</code></pre><hr>
<p><img src="/images/A-Z_ML/230.png" alt><br><img src="/images/A-Z_ML/231.png" alt></p>
<hr>
<p>kmeans {stats}    R Documentation<br>K-Means Clustering<br>Description<br>Perform k-means clustering on a data matrix.</p>
<p>Usage<br>kmeans(x, centers, iter.max = 10, nstart = 1,<br>       algorithm = c(“Hartigan-Wong”, “Lloyd”, “Forgy”,<br>                     “MacQueen”), trace=FALSE)</p>
<h2 id="S3-method-for-class-‘kmeans’"><a href="#S3-method-for-class-‘kmeans’" class="headerlink" title="S3 method for class ‘kmeans’"></a>S3 method for class ‘kmeans’</h2><p>fitted(object, method = c(“centers”, “classes”), …)<br>Arguments<br>x<br>numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns).</p>
<p>centers<br>either the number of clusters, say k, or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres.</p>
<p>iter.max<br>the maximum number of iterations allowed.</p>
<p>nstart<br>if centers is a number, how many random sets should be chosen?</p>
<p>algorithm<br>character: may be abbreviated. Note that “Lloyd” and “Forgy” are alternative names for one algorithm.</p>
<p>object<br>an R object of class “kmeans”, typically the result ob of ob &lt;- kmeans(..).</p>
<p>method<br>character: may be abbreviated. “centers” causes fitted to return cluster centers (one for each input point) and “classes” causes fitted to return a vector of class assignments.</p>
<p>trace<br>logical or integer number, currently only used in the default method (“Hartigan-Wong”): if positive (or true), tracing information on the progress of the algorithm is produced. Higher values may produce more tracing information.</p>
<p>…<br>not used.</p>
<p>nstart<br>if centers is a number, how many random sets should be chosen?</p>
<h2 id="在开始时同时创建10组中心点然后选择一个最好的"><a href="#在开始时同时创建10组中心点然后选择一个最好的" class="headerlink" title="在开始时同时创建10组中心点然后选择一个最好的"></a>在开始时同时创建10组中心点然后选择一个最好的</h2>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/14/K-Means-Clustering-of-Python-by-A-Z/">K-Means Clustering of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/14/K-Means-Clustering-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T15:47:16.000Z" itemprop="datePublished">September 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/227.png" alt></p>
<ul>
<li>数据是商场的用户购买评分，越高用户花的钱越多，帮助商场把用户分群，那样的客户更愿意消费</li>
</ul>
<script src="https://gist.github.com/SauronLee/f33eea90a7d227f88884623dd37139dc.js"></script>

<p><img src="/images/A-Z_ML/228.png" alt><br><img src="/images/A-Z_ML/229.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/K-Means-Clustering-by-A-Z/">K-Means Clustering by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/K-Means-Clustering-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T14:40:54.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>K平均聚类算法 - 原理<br><img src="/images/A-Z_ML/112.png" alt><br><img src="/images/A-Z_ML/113.png" alt><br><img src="/images/A-Z_ML/114.png" alt><br><img src="/images/A-Z_ML/115.png" alt><br><img src="/images/A-Z_ML/116.png" alt><br><img src="/images/A-Z_ML/117.png" alt><br><img src="/images/A-Z_ML/118.png" alt></li>
<li>K-Means 随机初始化陷阱 Random Initialization Trap<br><img src="/images/A-Z_ML/119.png" alt><br><img src="/images/A-Z_ML/220.png" alt><br><img src="/images/A-Z_ML/221.png" alt></li>
<li>K++ 解决初始化陷阱<br><img src="/images/A-Z_ML/222.png" alt></li>
<li>K-Means 选择类的个数 Selecting The Number Of Clusters<br><img src="/images/A-Z_ML/223.png" alt></li>
<li>对每组数据的每个点到那组数据的中心点的距离求平方和WCSS，如果每组的组内平方和相加越小那么我们设置的这个中心点就越能表达这个的聚集集中性<br><img src="/images/A-Z_ML/224.png" alt></li>
<li>分成2组时整个组内平方WCSS有明显的下降<br><img src="/images/A-Z_ML/225.png" alt></li>
<li>虽然左边没有变动但是右边分成了两组所以整个WCSS又有降低<br><img src="/images/A-Z_ML/223.png" alt></li>
<li>如果样本集中有50个点我们把他分成50组，那么WCSS=0</li>
<li>手肘法则<br><img src="/images/A-Z_ML/226.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Evaluating-Classification-Models-Performance-of-by-A-Z/">Evaluating Classification Models Performance by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Evaluating-Classification-Models-Performance-of-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T09:23:51.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="通常时"><a href="#通常时" class="headerlink" title="通常时"></a>通常时</h1><ul>
<li>伪阳性和伪阴性 False Positives &amp; False Negatives(也叫做一型错误和二型错误)</li>
<li>也就是错误的负数和错误的正数</li>
</ul>
<p><img src="/images/A-Z_ML/101.png" alt></p>
<ul>
<li><p>-#2为伪阳性错误，#3为伪阴性错误，在生活中伪阳性错误更为严重，（如果一个艾滋病人我们利用血清等预判他有艾滋病时虽然严重但是他顶多会慌张一段时间然后再次检测从而排除，然而当出现伪阳性时那么一个病毒携带者评定没有艾滋病会导致艾滋病的传播）<br><img src="/images/A-Z_ML/102.png" alt></p>
</li>
<li><p>Confusion Matrix</p>
</li>
</ul>
<p><img src="/images/A-Z_ML/103.png" alt></p>
<h1 id="特殊时"><a href="#特殊时" class="headerlink" title="特殊时"></a>特殊时</h1><ul>
<li>Confusion Matrix 混淆矩阵虽然可以很好的反映出模型的成功率但是由于过于简单所以不能告诉我们很多细节。</li>
<li><p>准确率悖论 Accuracy Paradox<br><img src="/images/A-Z_ML/104.png" alt><br><img src="/images/A-Z_ML/105.png" alt></p>
</li>
<li><p>累计准确曲线 CAP Curve</p>
</li>
<li><p>我们把最好的模型叫做Crystal Ball，因为它可以预知未来，我们知道人群中会有10%的客户会购买我们的产品，而这个模型预测成功了这个10000个回去购买的用户，也就是说我们只给这10%的用户做广告，而这10%的用户正好都购买了我们的产品<br><img src="/images/A-Z_ML/108.png" alt><br><img src="/images/A-Z_ML/106.png" alt><br><img src="/images/A-Z_ML/107.png" alt></p>
</li>
<li><p>累计准确曲线分析 CAP Curve Analysis</p>
</li>
<li>越接近一越好，越接近零越差<br><img src="/images/A-Z_ML/109.png" alt></li>
<li>模型评估（注意：too good 为不正常这种时候好的有点超乎寻常，比如：1，自变量与因变量之间有某种因果关系的时候会出现，在例子中打电话次数与购买次数成因果关系的时候那么这种自变量必须从样本里剔除。2，过度拟合，3，建模的非常好超乎寻常的好）<br><img src="/images/A-Z_ML/110.png" alt></li>
</ul>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ol>
<li>每个模型的长处和不足是什么？<br><img src="/images/A-Z_ML/111.png" alt></li>
<li>对于每个不同的案例，我如何在这些模型中作出选择？</li>
</ol>
<ul>
<li>首先，需要判断您面对的是一个线性还是非线性的问题。</li>
<li>假如是线性的问题，应该选择逻辑回归（logistic regression）或者支持向量机SVM。</li>
<li><p>假如是非线性的问题，应该选择朴素贝叶斯（naive bayes），决策树（decision tree）或者是随机森林（random forest）。在接下来的课程中我们会讲到神经网络（neural network），也是一个十分强大的方法。</p>
</li>
<li><p>从实际操作的角度来说，可以大致遵循以下的规则：</p>
</li>
<li><p>假如想要给最终预测概率进行排序，应该选择逻辑回归（logistic regression）或是朴素贝叶斯（Naive Bayes）。举个例子：想要预测不同客户购买某项产品的概率，并将这些概率从大到小进行排序，以便锁定目标客户群。在这样的情形下，如果您的问题是线性的，应该运用逻辑回归（logistic regression）；假如您的问题是非线性的，应该选择朴素贝叶斯（naive bayes）模型。</p>
</li>
<li>假如想要预测每一个客户属于哪一个划分（segment），应该选择SVM。市场和客户群体的划分可以是已完成的市场调研或者集群分析（clustering）的结果。</li>
<li>假如想要非常直观地展示／阐述模型，那么决策树（Decision Tree）是最佳选择。</li>
<li>假如想要最好的模型的分类表现，并且不太在意模型的展示／阐述，那么随机森林（random forest）是不错的选择。</li>
</ul>
<ol>
<li>我如何提高每个模型的表现？</li>
</ol>
<p>在第10部分的第2章，我们会详细讲述如何调整模型参数，以提高模型的表现。总体来说，每个模型都有两种类型的的参数：</p>
<ul>
<li>可以被“学习”的参数，比如：线性回归中各个自变量的参数。</li>
<li>难以被学习的“超参数” （hyperparameter）。<br>一个好的“超参数“的例子是随机森林（random forest）算法中训练决策树的个数N。目前为止，我们应用它的默认值（比如500）。在第10部分的第2章当中我们会详细介绍如何调整、优化这些“超参数”。</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Random-Forest-of-R-by-A-Z/">Random Forest of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Random-Forest-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T09:03:14.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/100.png" alt></p>
<script src="https://gist.github.com/SauronLee/d3c2238ba1ea263551f096d64a5e594e.js"></script>

<hr>
<pre><code>
&gt; # Predicting the Test set results
&gt; y_pred = predict(classifier, newdata = test_set[-3])
&gt; y_pred
  2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52  66  69  74  75  82  84 
  0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   1 
 85  86  87  89 103 104 107 108 109 117 124 126 127 131 134 139 148 154 156 159 162 163 170 175 
  0   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
176 193 199 200 208 213 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 
  0   0   0   0   1   1   1   0   1   0   0   1   1   0   0   1   0   0   1   1   1   1   1   1 
292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 368 369 372 373 380 383 
  1   0   0   0   1   0   0   1   0   1   0   1   0   1   1   0   0   1   0   0   1   0   1   0 
389 392 395 400 
  1   1   0   1 
Levels: 0 1

&gt; # Making the Confusion Matrix
&gt; cm = table(test_set[, 3], y_pred)
&gt; cm
   y_pred
     0  1
  0 59  5
  1  9 27
</code></pre><p><img src="/images/A-Z_ML/95.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Random-Forest-of-Python-by-A-Z/">Random Forest of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Random-Forest-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T08:39:44.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/94.png" alt="issue"></p>
<pre><code>
# Random Forest Classification

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;)
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Random Forest to the Training set
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = &#39;entropy&#39;, random_state = 0 )
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap((&#39;red&#39;, &#39;green&#39;)))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap((&#39;orange&#39;, &#39;blue&#39;))(i), label = j, s=15)
plt.title(&#39;Random Forest (Training set)&#39;)
plt.xlabel(&#39;Age&#39;)
plt.ylabel(&#39;Estimated Salary&#39;)
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap((&#39;red&#39;, &#39;green&#39;)))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap((&#39;orange&#39;, &#39;blue&#39;))(i), label = j, s=15)
plt.title(&#39;Random Forest (Test set)&#39;)
plt.xlabel(&#39;Age&#39;)
plt.ylabel(&#39;Estimated Salary&#39;)
plt.legend()
plt.show()
</code></pre><p><img src="/images/A-Z_ML/95.png" alt="issue"><br><img src="/images/A-Z_ML/96.png" alt="issue"></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Random-Forest-by-A-Z/">Random Forest by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Random-Forest-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T08:19:03.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/85.png" alt></p>
<ul>
<li><p>集成学习：对多个分类器进行预测，在对分类结果进行组合已达到最终结果（组合方式：平均，加权平均，投票）三个臭皮匠顶一个诸葛亮</p>
</li>
<li><p>集成学习的步骤：<br>1，随机构成训练集（装袋的一个过程）<br>2，把装好的这个袋的数据作为训练集进行预测<br>3，重复第一步和第二部已得到足够多的袋子，每个袋子里的数据都是不同的<br>4，最后进行投票决定分类（比如现在有1000个分类器树，那么当预测一个新的样本的时候要带入每颗分类器然后进行投票，比如有700颗为1，有300颗为0，那么结果就为1）<br><img src="/images/A-Z_ML/93.png" alt><br><img src="/images/A-Z_ML/97.png" alt><br><img src="/images/A-Z_ML/98.png" alt></p>
</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/CART-by-A-Z/">CART by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/CART-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T07:33:08.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/81.png" alt><br><img src="/images/A-Z_ML/82.png" alt><br><img src="/images/A-Z_ML/83.png" alt></p>
<ul>
<li>最小化一个优化后的基尼不纯度（就是分割后的一边只有只有一种颜色的纯度）</li>
<li>当添加了一条分割线后我们能够把这两边的纯度相加最小化，其实每一条分割线的寻找都是优化的问题，这里可以用基尼不纯度来判别纯度也可以用信息熵</li>
<li>Decision Tree Classifier虽然已经淘汰但是他有拓展</li>
</ul>
<p><img src="/images/A-Z_ML/87.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Naive-Bayes-of-R-by-A-Z/">Naive Bayes of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Naive-Bayes-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T06:57:52.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/64.png" alt></p>
<script src="https://gist.github.com/SauronLee/15ceccb3322d21549d4685ec1b9bd58a.js"></script>

<hr>
<pre><code>
&gt; # Predicting the Test set results
&gt; y_pred = predict(classifier, newdata = test_set[-3])
&gt; # Making the Confusion Matrix
&gt; cm = table(test_set[, 3], y_pred)
&gt; y_pred
  [1] 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [48] 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0
 [95] 1 1 1 1 0 1
Levels: 0 1
&gt; cm
   y_pred
     0  1
  0 57  7
  1  7 29
</code></pre><p>  <img src="/images/A-Z_ML/80.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Kernel-SVM-of-R-by-A-Z/">Kernel SVM of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Kernel-SVM-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T04:59:07.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/4ff846b18fd678e81c4f426e1fde6d38.js"></script>

<hr>
<pre><code>
&gt; # Predicting the Test set results
&gt; y_pred = predict(classifier, newdata = test_set[-3])
&gt; y_pred
  2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52  66  69  74  75  82  84 
  0   0   0   0   0   1   1   1   0   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0 
 85  86  87  89 103 104 107 108 109 117 124 126 127 131 134 139 148 154 156 159 162 163 170 175 
  0   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
176 193 199 200 208 213 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 
  0   0   0   0   1   1   1   0   1   0   0   1   1   0   1   1   1   0   1   1   1   1   1   1 
292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 368 369 372 373 380 383 
  1   0   1   0   1   0   0   1   0   1   0   1   0   1   1   0   0   1   1   0   1   0   1   1 
389 392 395 400 
  1   1   0   1 
Levels: 0 1

&gt; # Making the Confusion Matrix
&gt; cm = table(test_set[, 3], y_pred)
&gt; cm
   y_pred
     0  1
  0 58  6
  1  4 32
</code></pre><p><img src="/images/A-Z_ML/63.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Kernel-SVM-of-Python-by-A-Z/">Kernel SVM of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Kernel-SVM-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T04:58:06.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/62.png" alt></p>
<script src="https://gist.github.com/SauronLee/fefc21aba80e1b4c6f7f8b1ae687c72f.js"></script>

<p><img src="/images/A-Z_ML/60.png" alt><br><img src="/images/A-Z_ML/61.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Kernel-SVM-by-A-Z/">Kernel SVM by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Kernel-SVM-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T04:12:47.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/48.png" alt><br><img src="/images/A-Z_ML/49.png" alt><br><img src="/images/A-Z_ML/50.png" alt><br><img src="/images/A-Z_ML/51.png" alt><br><img src="/images/A-Z_ML/52.png" alt><br>引用核函数加快SVM</p>
<ul>
<li>高斯径向基和函数The Kernel Trick（RBF）</li>
<li>|x-l|值越大那么y越接近1，值越小越接近0，这其实跟sigmoud函数一个意思只不过这个是3维的<br><img src="/images/A-Z_ML/53.png" alt><br><img src="/images/A-Z_ML/54.png" alt><br><img src="/images/A-Z_ML/55.png" alt><br><img src="/images/A-Z_ML/56.png" alt><br><img src="/images/A-Z_ML/57.png" alt><br><img src="/images/A-Z_ML/58.png" alt></li>
<li>Types of Kernel Functions<br><img src="/images/A-Z_ML/59.png" alt></li>
<li>S函数=双曲正切的平移和缩放</li>
<li>所有的核函数的详细教程网站：<br><a href="http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/" target="_blank" rel="noopener">http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/</a></li>
<li>展示多项式核函数视频<br><a href="https://www.youtube.com/watch?time_continue=1&amp;v=3liCbRZPrZA" target="_blank" rel="noopener">https://www.youtube.com/watch?time_continue=1&amp;v=3liCbRZPrZA</a></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/SVM-of-R-by-A-Z/">SVM of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/SVM-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T03:40:28.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/45.png" alt><br><img src="/images/A-Z_ML/46.png" alt></p>
<script src="https://gist.github.com/SauronLee/1de42bb638ce5fe37ed8d91c1eab179d.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/SVM-of-Python-by-A-Z/">SVM of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/SVM-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-13T03:25:02.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/45.png" alt></p>
<script src="https://gist.github.com/SauronLee/162cbafc90b6797040b501cfb90f4cb7.js"></script>

<hr>
<pre><code>
&gt; y_pred = predict(classifier, newdata = test_set[-3])
&gt; y_pred
  2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52  66  69  74  75  82  84 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
 85  86  87  89 103 104 107 108 109 117 124 126 127 131 134 139 148 154 156 159 162 163 170 175 
  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
176 193 199 200 208 213 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 
  0   0   0   0   1   1   1   0   1   0   1   1   1   0   1   1   1   0   1   1   1   1   1   0 
292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 368 369 372 373 380 383 
  1   1   1   0   1   0   0   0   0   1   0   1   0   1   1   0   1   1   1   0   1   0   1   1 
389 392 395 400 
  0   0   0   0 
Levels: 0 1
&gt; # Making the Confusion Matrix
&gt; cm = table(test_set[, 3], y_pred)
&gt; cm
   y_pred
     0  1
  0 57  7
  1 13 23
</code></pre><hr>
<p><img src="/images/A-Z_ML/47.png" alt></p>
<hr>
<p>svm {e1071}    R Documentation<br>Support Vector Machines<br>Description<br>svm is used to train a support vector machine. It can be used to carry out general regression and classification (of nu and epsilon-type), as well as density-estimation. A formula interface is provided.</p>
<p>Usage</p>
<h2 id="S3-method-for-class-‘formula’"><a href="#S3-method-for-class-‘formula’" class="headerlink" title="S3 method for class ‘formula’"></a>S3 method for class ‘formula’</h2><p>svm(formula, data = NULL, …, subset, na.action =<br>na.omit, scale = TRUE)</p>
<h2 id="Default-S3-method"><a href="#Default-S3-method" class="headerlink" title="Default S3 method:"></a>Default S3 method:</h2><p>svm(x, y = NULL, scale = TRUE, type = NULL, kernel =<br>“radial”, degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),<br>coef0 = 0, cost = 1, nu = 0.5,<br>class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,<br>shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,<br>…, subset, na.action = na.omit)<br>Arguments<br>formula<br>a symbolic description of the model to be fit.</p>
<p>data<br>an optional data frame containing the variables in the model. By default the variables are taken from the environment which ‘svm’ is called from.</p>
<p>x<br>a data matrix, a vector, or a sparse matrix (object of class Matrix provided by the Matrix package, or of class matrix.csr provided by the SparseM package, or of class simple_triplet_matrix provided by the slam package).</p>
<p>y<br>a response vector with one label for each row/component of x. Can be either a factor (for classification tasks) or a numeric vector (for regression).</p>
<p>scale<br>A logical vector indicating the variables to be scaled. If scale is of length 1, the value is recycled as many times as needed. Per default, data are scaled internally (both x and y variables) to zero mean and unit variance. The center and scale values are returned and used for later predictions.</p>
<p>type<br>svm can be used as a classification machine, as a regression machine, or for novelty detection. Depending of whether y is a factor or not, the default setting for type is C-classification or eps-regression, respectively, but may be overwritten by setting an explicit value.<br>Valid options are:</p>
<p>C-classification</p>
<p>nu-classification</p>
<p>one-classification (for novelty detection)</p>
<p>eps-regression</p>
<p>nu-regression</p>
<p>kernel<br>the kernel used in training and predicting. You might consider changing some of the following parameters, depending on the kernel type.<br>linear:<br>u’*v</p>
<p>polynomial:<br>(gamma<em>u’</em>v + coef0)^degree</p>
<p>radial basis:<br>exp(-gamma*|u-v|^2)</p>
<p>sigmoid:<br>tanh(gamma<em>u’</em>v + coef0)</p>
<p>degree<br>parameter needed for kernel of type polynomial (default: 3)</p>
<p>gamma<br>parameter needed for all kernels except linear (default: 1/(data dimension))</p>
<p>coef0<br>parameter needed for kernels of type polynomial and sigmoid (default: 0)</p>
<p>cost<br>cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.</p>
<p>nu<br>parameter needed for nu-classification, nu-regression, and one-classification</p>
<p>class.weights<br>a named vector of weights for the different classes, used for asymmetric class sizes. Not all factor levels have to be supplied (default weight: 1). All components have to be named. Specifying “inverse” will choose the weights inversely proportional to the class distribution.</p>
<p>cachesize<br>cache memory in MB (default 40)</p>
<p>tolerance<br>tolerance of termination criterion (default: 0.001)</p>
<p>epsilon<br>epsilon in the insensitive-loss function (default: 0.1)</p>
<p>shrinking<br>option whether to use the shrinking-heuristics (default: TRUE)</p>
<p>cross<br>if a integer value k&gt;0 is specified, a k-fold cross validation on the training data is performed to assess the quality of the model: the accuracy rate for classification and the Mean Squared Error for regression</p>
<p>fitted<br>logical indicating whether the fitted values should be computed and included in the model or not (default: TRUE)</p>
<p>probability<br>logical indicating whether the model should allow for probability predictions.</p>
<p>…<br>additional parameters for the low level fitting function svm.default</p>
<p>subset<br>An index vector specifying the cases to be used in the training sample. (NOTE: If given, this argument must be named.)</p>
<p>na.action<br>A function to specify the action to be taken if NAs are found. The default action is na.omit, which leads to rejection of cases with missing values on any required variable. An alternative is na.fail, which causes an error if NA cases are found. (NOTE: If given, this argument must be named.)</p>
<hr>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Logistic-Regression-of-R-by-A-Z/">Logistic Regression of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Logistic-Regression-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-12T18:18:54.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/35.png" alt="issue"><br><img src="/images/A-Z_ML/36.png" alt="issue"><br><img src="/images/A-Z_ML/37.png" alt="issue"><br><img src="/images/A-Z_ML/38.png" alt="issue"></p>
<ul>
<li>列<br><img src="/images/A-Z_ML/42.png" alt="issue"></li>
</ul>
<pre><code>
# Logistic Regression

# Importing the dataset
dataset = read.csv(&#39;Social_Network_Ads.csv&#39;)
dataset = dataset[3:5]

# Splitting the dataset into the Training set and Test set
# install.packages(&#39;caTools&#39;)
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])  

# Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~ .,
                 family= binomial,
                 data= training_set)

# Predicting the Test set results
prob_pred=predict(classifier, type = &#39;response&#39;, newdata=test_set[-3])
y_pred=ifelse(prob_pred&gt;0.5, 1, 0)

# Making the Confusion Matrix
cm = table(test_set[,3], y_pred)

# Visualising the Training set results
# install.packages(ElemStatLearn)
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.0075)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.0075)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c(&#39;Age&#39;, &#39;EstimatedSalary&#39;)
prob_set = predict(classifier, type = &#39;response&#39;, newdata = grid_set)
y_grid = ifelse(prob_set &gt; 0.5, 1, 0)
plot(set[, -3],
     main = &#39;Classifier (Training set)&#39;,
     xlab = &#39;Age&#39;, ylab = &#39;Estimated Salary&#39;,
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = &#39;.&#39;, col = ifelse(y_grid == 1, &#39;springgreen3&#39;, &#39;tomato&#39;))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, &#39;green4&#39;, &#39;red3&#39;))

# Visualising the Test set results
# install.packages(ElemStatLearn)
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c(&#39;Age&#39;, &#39;EstimatedSalary&#39;)
prob_set = predict(classifier, type = &#39;response&#39;, newdata = grid_set)
y_grid = ifelse(prob_set &gt; 0.5, 1, 0)
plot(set[, -3],
     main = &#39;Classifier (Test set)&#39;,
     xlab = &#39;Age&#39;, ylab = &#39;Estimated Salary&#39;,
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = &#39;.&#39;, col = ifelse(y_grid == 1, &#39;springgreen3&#39;, &#39;tomato&#39;))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, &#39;green4&#39;, &#39;red3&#39;))
</code></pre><hr>
<pre><code>

 prob_pred=predict(classifier, type = &#39;response&#39;, newdata=test_set[-3])
 prob_pred
           2            4            5            9           12           18           19 
0.0162395375 0.0117148379 0.0037846461 0.0024527456 0.0073339436 0.2061576580 0.2669935073 
          20           22           29           32           34           35           38 
0.3851475689 0.5448578778 0.0103005636 0.2994922143 0.0084168787 0.0494471952 0.0171641479 
          45           46           48           52           66           69           74 
0.0485051303 0.0008343060 0.0102561619 0.0007055347 0.0058448457 0.0044534947 0.3933468488 
          75           82           84           85           86           87           89 
0.0071065671 0.1068589185 0.2580084947 0.0303248927 0.3303649169 0.0051132916 0.0263861849 
         103          104          107          108          109          117          124 
0.1310148056 0.7649772313 0.0034367786 0.0473827096 0.0327965105 0.1626049288 0.0675494054 
         126          127          131          134          139          148          154 
0.2189658514 0.4142562486 0.0324337750 0.0043457839 0.0163538708 0.1030590600 0.0751093248 
         156          159          162          163          170          175          176 
0.0048556976 0.0027487256 0.0306647902 0.0463555716 0.0122981409 0.1169016711 0.0011936610 
         193          199          200          208          213          224          226 
0.0103005636 0.0252589417 0.0177353905 0.9870859806 0.9453359968 0.9969454446 0.1064430571 
         228          229          230          234          236          237          239 
0.9979393884 0.3705093415 0.5807527959 0.9117762840 0.7817273411 0.2310672929 0.8037996043 
         241          255          264          265          266          273          274 
0.9682706714 0.6686007827 0.1451169281 0.9060311409 0.8293112410 0.9568520348 0.6781064291 
         281          286          292          299          302          305          307 
0.9926955397 0.4170486388 0.9220096987 0.7363498859 0.8247736816 0.2558136823 0.9932007105 
         310          316          324          326          332          339          341 
0.1178058928 0.3442845494 0.3958138650 0.3059412440 0.9725035550 0.1431602303 0.9842795480 
         343          347          353          363          364          367          368 
0.2073273008 0.9371909698 0.6843940060 0.5559479117 0.5698028861 0.9440512240 0.8427877409 
         369          372          373          380          383          389          392 
0.2549836305 0.9928717092 0.3243409327 0.8519685008 0.9697473704 0.3793408625 0.2718336775 
         395          400 
0.2040229226 0.5236436275 


y_pred=ifelse(prob_pred&gt;0.5, 1, 0)
y_pred
  2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52  66  69  74  75  82  84 
  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
 85  86  87  89 103 104 107 108 109 117 124 126 127 131 134 139 148 154 156 159 162 163 170 175 
  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
176 193 199 200 208 213 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 
  0   0   0   0   1   1   1   0   1   0   1   1   1   0   1   1   1   0   1   1   1   1   1   0 
292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 368 369 372 373 380 383 
  1   1   1   0   1   0   0   0   0   1   0   1   0   1   1   1   1   1   1   0   1   0   1   1 
389 392 395 400 
  0   0   0   1 

cm = table(test_set[,3], y_pred)
#对角线为正确率
&gt; cm
   y_pred
     0  1
  0 57  7
  1 10 26
</code></pre><p><img src="/images/A-Z_ML/43.png" alt="issue"><br><img src="/images/A-Z_ML/44.png" alt="issue"></p>
<hr>
<p>glm {stats}    R Documentation<br>Fitting Generalized Linear Models<br>Description<br>glm is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution.</p>
<p>Usage<br>glm(formula, family = gaussian, data, weights, subset,<br>    na.action, start = NULL, etastart, mustart, offset,<br>    control = list(…), model = TRUE, method = “glm.fit”,<br>    x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, …)</p>
<p>glm.fit(x, y, weights = rep(1, nobs),<br>        start = NULL, etastart = NULL, mustart = NULL,<br>        offset = rep(0, nobs), family = gaussian(),<br>        control = list(), intercept = TRUE, singular.ok = TRUE)</p>
<h1 id="S3-method-for-class-‘glm’"><a href="#S3-method-for-class-‘glm’" class="headerlink" title="S3 method for class ‘glm’"></a>S3 method for class ‘glm’</h1><p>weights(object, type = c(“prior”, “working”), …)<br>Arguments<br>formula<br>an object of class “formula” (or one that can be coerced to that class): a symbolic description of the model to be fitted. The details of model specification are given under ‘Details’.</p>
<p>family<br>a description of the error distribution and link function to be used in the model. For glm this can be a character string naming a family function, a family function or the result of a call to a family function. For glm.fit only the third option is supported. (See family for details of family functions.)</p>
<p>data<br>an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model. If not found in data, the variables are taken from environment(formula), typically the environment from which glm is called.</p>
<p>weights<br>an optional vector of ‘prior weights’ to be used in the fitting process. Should be NULL or a numeric vector.</p>
<p>subset<br>an optional vector specifying a subset of observations to be used in the fitting process.</p>
<p>na.action<br>a function which indicates what should happen when the data contain NAs. The default is set by the na.action setting of options, and is na.fail if that is unset. The ‘factory-fresh’ default is na.omit. Another possible value is NULL, no action. Value na.exclude can be useful.</p>
<p>start<br>starting values for the parameters in the linear predictor.</p>
<p>etastart<br>starting values for the linear predictor.</p>
<p>mustart<br>starting values for the vector of means.</p>
<p>offset<br>this can be used to specify an a priori known component to be included in the linear predictor during fitting. This should be NULL or a numeric vector of length equal to the number of cases. One or more offset terms can be included in the formula instead or as well, and if more than one is specified their sum is used. See model.offset.</p>
<p>control<br>a list of parameters for controlling the fitting process. For glm.fit this is passed to glm.control.</p>
<p>model<br>a logical value indicating whether model frame should be included as a component of the returned value.</p>
<p>method<br>the method to be used in fitting the model. The default method “glm.fit” uses iteratively reweighted least squares (IWLS): the alternative “model.frame” returns the model frame and does no fitting.</p>
<p>User-supplied fitting functions can be supplied either as a function or a character string naming a function, with a function which takes the same arguments as glm.fit. If specified as a character string it is looked up from within the stats namespace.</p>
<p>x, y<br>For glm: logical values indicating whether the response vector and model matrix used in the fitting process should be returned as components of the returned value.</p>
<p>For glm.fit: x is a design matrix of dimension n * p, and y is a vector of observations of length n.</p>
<p>singular.ok<br>logical; if FALSE a singular fit is an error.</p>
<p>contrasts<br>an optional list. See the contrasts.arg of model.matrix.default.</p>
<p>intercept<br>logical. Should an intercept be included in the null model?</p>
<p>object<br>an object inheriting from class “glm”.</p>
<p>type<br>character, partial matching allowed. Type of weights to extract from the fitted model object. Can be abbreviated.</p>
<p>…<br>For glm: arguments to be used to form the default control argument if it is not supplied directly.</p>
<p>For weights: further arguments passed to or from other methods.</p>
<hr>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/13/Logistic-Regression-of-Python-by-A-Z/">Logistic Regression of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/13/Logistic-Regression-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-12T15:40:39.000Z" itemprop="datePublished">September 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/35.png" alt><br><img src="/images/A-Z_ML/36.png" alt><br><img src="/images/A-Z_ML/37.png" alt><br><img src="/images/A-Z_ML/38.png" alt></p>
<ul>
<li>列<br><img src="/images/A-Z_ML/39.png" alt></li>
</ul>
<script src="https://gist.github.com/SauronLee/fdff6948bb436e04e91e1ba70ed7e3bb.js"></script>

<ul>
<li>运行confusion_matrix来测试精准度，正确率（65+24）%</li>
</ul>
<pre><code># Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm
Out[3]: 
array([[65,  3],
       [ 8, 24]])
</code></pre><p><img src="/images/A-Z_ML/40.png" alt><br><img src="/images/A-Z_ML/41.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/12/Evaluating-Regression-Models-Performance-by-A-Z/">Evaluating Regression Models Performance by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/12/Evaluating-Regression-Models-Performance-by-A-Z/" class="archive-article-date"><time datetime="2019-09-12T14:48:21.000Z" itemprop="datePublished">September 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>R平方 - 原理(决定系数)<br><img src="/images/A-Z_ML/30.png" alt><br><img src="/images/A-Z_ML/31.png" alt></li>
<li>广义R平方 - 原理（Adjected R^2）</li>
<li>因为最差时:b<em>3*x</em>=0<br><img src="/images/A-Z_ML/32.png" alt></li>
<li>P有惩罚的作用，P(自变量)越多分母就会越大，分数就会升高<br><img src="/images/A-Z_ML/33.png" alt></li>
<li>回归模型性能评价及选择</li>
<li>加自变量会增加R^2，所以AdjR^2会更好<br><img src="/images/A-Z_ML/34.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/12/Polynomial-Regression-of-R-by-A-Z/">Polynomial Regression of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/12/Polynomial-Regression-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-12T06:53:46.000Z" itemprop="datePublished">September 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/22.png" alt><br><img src="/images/A-Z_ML/23.png" alt><br><img src="/images/A-Z_ML/24.png" alt><br><img src="/images/A-Z_ML/25.png" alt></p>
<pre><code># Polynomial Regression

# Importing the dataset
dataset = read.csv(&#39;Position_Salaries.csv&#39;)
dataset = dataset[, 2:3]

# Splitting the dataset into the Training set and Test set
# install.packages(&#39;caTools&#39;)
# library(caTools)
# set.seed(123)
# split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# training_set = subset(dataset, split == TRUE)
# test_set = subset(dataset, split == FALSE)

# Feature Scaling
# training_set[, 2:3] = scale(training_set[, 2:3])
# test_set[, 2:3] = scale(test_set[, 2:3])

# Fitting Linear Regression to the dataset
lin_reg=lm(formula = Salary~.,
           data=dataset)

# Fitting Polynomial Regression to the dataset
dataset$level2 = dataset$Level^2
dataset$level3 = dataset$Level^3
dataset$level4 = dataset$Level^4
poly_reg = lm(formula = Salary~.,
              data=dataset)

#Visualising the Linear Regression results
install.packages(&#39;ggplot2&#39;)
library(ggplot2)
ggplot() +
  geom_point(aes(x=dataset$Level,y=dataset$Salary),
             colour = &#39;red&#39;) +
  geom_line (aes (x=dataset$Level, y = predict (lin_reg, newdata=dataset)),
             colour = &#39;blue&#39;) +
  xlab(&#39;Level&#39;) +
  ylab(&#39;Salary&#39;)

#Visualising the Polynomial Regression results
ggplot() +
  geom_point(aes(x=dataset$Level,y=dataset$Salary),
             colour = &#39;red&#39;) +
  geom_line (aes (x=dataset$Level, y = predict (poly_reg, newdata=dataset)),
             colour = &#39;blue&#39;) +
  xlab(&#39;Level&#39;) +
  ylab(&#39;Salary&#39;)

#Predicting a new result with Linear Regression
y_pred=predict(lin_reg, data.frame(Level=6.5))

#Predicting a new result with Polynomial Regression
#因为我们的维度比较高那么R需要让我们告诉他每个维度的值
y_pred=predict(poly_reg, data.frame(Level=6.5,
                                    level2=6.5^2,
                                    level3=6.5^3,
                                    level4=6.5^4))
</code></pre><hr>
<p>summary(lin_reg)</p>
<p>Call:<br>lm(formula = Salary ~ ., data = dataset)</p>
<p>Residuals:<br>    Min      1Q  Median      3Q     Max<br>-170818 -129720  -40379   65856  386545 </p>
<p>Coefficients:<br>            Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept)  -195333     124790  -1.565  0.15615   </p>
<h2 id="Level-80879-20112-4-021-0-00383"><a href="#Level-80879-20112-4-021-0-00383" class="headerlink" title="Level          80879      20112   4.021  0.00383 **"></a>Level          80879      20112   4.021  0.00383 **</h2><p>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>
<p>Residual standard error: 182700 on 8 degrees of freedom<br>Multiple R-squared:  0.669,    Adjusted R-squared:  0.6277<br>F-statistic: 16.17 on 1 and 8 DF,  p-value: 0.003833</p>
<hr>
<ul>
<li>增加次方项</li>
</ul>
<pre><code># Fitting Polynomial Regression to the dataset
dataset$level2 = dataset$Level^2
dataset$level3 = dataset$Level^3
dataset$level4 = dataset$Level^4
poly_reg = lm(formula = Salary~.,
              data=dataset)
</code></pre><hr>
<p> summary(poly_reg)</p>
<p>Call:<br>lm(formula = Salary ~ ., data = dataset)</p>
<p>Residuals:<br>     1      2      3      4      5      6      7      8      9     10<br> -8357  18240   1358 -14633 -11725   6725  15997  10006 -28695  11084 </p>
<p>Coefficients:<br>             Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept)  184166.7    67768.0   2.718  0.04189 <em><br>Level       -211002.3    76382.2  -2.762  0.03972 </em><br>level2        94765.4    26454.2   3.582  0.01584 <em><br>level3       -15463.3     3535.0  -4.374  0.00719 *</em></p>
<h2 id="level4-890-2-159-8-5-570-0-00257"><a href="#level4-890-2-159-8-5-570-0-00257" class="headerlink" title="level4          890.2      159.8   5.570  0.00257 **"></a>level4          890.2      159.8   5.570  0.00257 **</h2><p>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>
<p>Residual standard error: 20510 on 5 degrees of freedom<br>Multiple R-squared:  0.9974,    Adjusted R-squared:  0.9953<br>F-statistic: 478.1 on 4 and 5 DF,  p-value: 1.213e-06</p>
<hr>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/12/Polynomial-Regression-of-Python-by-A-Z/">Polynomial Regression of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/12/Polynomial-Regression-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-12T06:53:36.000Z" itemprop="datePublished">September 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/22.png" alt><br><img src="/images/A-Z_ML/23.png" alt><br><img src="/images/A-Z_ML/24.png" alt></p>
<ul>
<li>多应用在流行病学：疾病在时间和空间上的传播速度</li>
</ul>
<p><img src="/images/A-Z_ML/25.png" alt><br><img src="/images/A-Z_ML/26.png" alt></p>
<script src="https://gist.github.com/SauronLee/deb2cca719850a2cf43b585503bc3be1.js"></script>

<p>多项式回归就是利用：下面👇代码拟合出b_0和次方项然后全部进行线性回归</p>
<pre><code>poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X)
</code></pre><p><img src="/images/A-Z_ML/27.png" alt="issue"><br><img src="/images/A-Z_ML/29.png" alt="issue"><br><img src="/images/A-Z_ML/28.png" alt="issue"></p>
<ul>
<li>如果要使线段变得平滑增加x的密度即可比如：<pre><code>X_grid=np.arange(min(X),max(X),0.1)
X_grid=X_grid.reshape(len(X_grid),1)
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = &#39;blue&#39;)
</code></pre>如果一个新人他说他35岁工作了6年半工资16万是否诚实<pre><code>lin_reg.predict(6.5）
lin_reg_2.predict(poly_reg.fit_transform(6.5))
</code></pre></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/11/Simple-Linear-Regression-of-R-by-A-Z/">Simple Linear Regression of R  by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/11/Simple-Linear-Regression-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-09-11T04:45:29.000Z" itemprop="datePublished">September 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/02.png" alt><br><img src="/images/A-Z_ML/03.png" alt></p>
<script src="https://gist.github.com/SauronLee/ea489f7e3335cbc820bf593f7de6cd62.js"></script>

<p><img src="/images/A-Z_ML/06.png" alt></p>
<ul>
<li>lm {stats}    R Documentation<br>Fitting Linear Models<br>Description<br>lm is used to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of covariance (although aov may provide a more convenient interface for these).</li>
</ul>
<p>Usage<br>lm(formula, data, subset, weights, na.action,<br>   method = “qr”, model = TRUE, x = FALSE, y = FALSE, qr = TRUE,<br>   singular.ok = TRUE, contrasts = NULL, offset, …)<br>Arguments<br>formula<br>an object of class “formula” (or one that can be coerced to that class): a symbolic description of the model to be fitted. The details of model specification are given under ‘Details’.</p>
<p>data<br>an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model. If not found in data, the variables are taken from environment(formula), typically the environment from which lm is called.</p>
<p>subset<br>an optional vector specifying a subset of observations to be used in the fitting process.</p>
<p>weights<br>an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector. If non-NULL, weighted least squares is used with weights weights (that is, minimizing sum(w*e^2)); otherwise ordinary least squares is used. See also ‘Details’,</p>
<p>na.action<br>a function which indicates what should happen when the data contain NAs. The default is set by the na.action setting of options, and is na.fail if that is unset. The ‘factory-fresh’ default is na.omit. Another possible value is NULL, no action. Value na.exclude can be useful.</p>
<p>method<br>the method to be used; for fitting, currently only method = “qr” is supported; method = “model.frame” returns the model frame (the same as with model = TRUE, see below).</p>
<p>model, x, y, qr<br>logicals. If TRUE the corresponding components of the fit (the model frame, the model matrix, the response, the QR decomposition) are returned.</p>
<p>singular.ok<br>logical. If FALSE (the default in S but not in R) a singular fit is an error.</p>
<p>contrasts<br>an optional list. See the contrasts.arg of model.matrix.default.</p>
<p>offset<br>this can be used to specify an a priori known component to be included in the linear predictor during fitting. This should be NULL or a numeric vector or matrix of extents matching those of the response. One or more offset terms can be included in the formula instead or as well, and if more than one are specified their sum is used. See model.offset.</p>
<p>…<br>additional arguments to be passed to the low level regression fitting functions (see below).</p>
<ul>
<li>summary(regressor)</li>
</ul>
<p>Call:<br>lm(formula = Salary ~ YearsExperience, data = training_set)</p>
<p>Residuals:<br>    Min      1Q  Median      3Q     Max<br>-7325.1 -3814.4   427.7  3559.7  8884.6 </p>
<p>Coefficients:<br>                Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept)        25592       2646   9.672 1.49e-08 <em>*</em></p>
<h2 id="YearsExperience-9365-421-22-245-1-52e-14"><a href="#YearsExperience-9365-421-22-245-1-52e-14" class="headerlink" title="YearsExperience     9365        421  22.245 1.52e-14 *"></a>YearsExperience     9365        421  22.245 1.52e-14 <em>*</em></h2><ul>
<li><em>代表变量对模型的显著性（重要性）P值越小显著性越（强当p值小于0.05时显著性就很大了）R-squared:  0.9649（越接近1说明拟合效果越好）<br>Signif. codes:  0 ‘**</em>’ 0.001 ‘<em>*’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</li>
</ul>
<p>Residual standard error: 5391 on 18 degrees of freedom<br>Multiple R-squared:  0.9649,    Adjusted R-squared:  0.963<br>F-statistic: 494.8 on 1 and 18 DF,  p-value: 1.524e-14</p>
<ul>
<li>command+enter<br>y_pred = predict(regressor, newdata = test_set)<blockquote>
<p>y_pred</p>
<pre><code>  2         4         5         8        11        16        20        21        24 
</code></pre><p>37766.77  44322.33  46195.35  55560.43  62115.99  71481.07  81782.66  89274.72 102385.84 </p>
<pre><code> 26 
</code></pre><p>109877.90 </p>
</blockquote>
</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/11/Simple-Linear-Regression-of-Python-by-A-Z/">Simple Linear Regression of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/11/Simple-Linear-Regression-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-09-11T04:45:04.000Z" itemprop="datePublished">September 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/02.png" alt><br><img src="/images/A-Z_ML/03.png" alt></p>
<script src="https://gist.github.com/SauronLee/bf7e3f954b445edc107ef4271b13642b.js"></script>

<p><img src="/images/A-Z_ML/04.png" alt><br><img src="/images/A-Z_ML/05.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/09/10/Get-data-set-by-Machine-Learning-A-Z/">Get data set by Machine Learning A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/09/10/Get-data-set-by-Machine-Learning-A-Z/" class="archive-article-date"><time datetime="2019-09-10T10:55:18.000Z" itemprop="datePublished">September 10th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://www.superdatascience.com/pages/%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86" target="_blank" rel="noopener">https://www.superdatascience.com/pages/%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86</a></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/21/Statistics-by-Yebingcheng-of-Taiwan-University/">Statistics by Yebingcheng of Taiwan University</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/21/Statistics-by-Yebingcheng-of-Taiwan-University/" class="archive-article-date"><time datetime="2019-08-21T09:08:34.000Z" itemprop="datePublished">August 21st</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="几率的独立性（Independent）"><a href="#几率的独立性（Independent）" class="headerlink" title="几率的独立性（Independent）"></a>几率的独立性（Independent）</h1><p>若两个事件满足：<script type="math/tex">P(A\cap B)=P(A)\cdot P(B)</script> 则A，B两件事称为独立事件<br>两个事件没有交集，同时发生两件事的几率就是完全两个几率相乘没有扣除交集部分<br>我比较喜欢另一个版本的独立事件的定义：<script type="math/tex">P(A|B)=P(A)</script> 也就是说在B发生时A发生的概率等于A发生的概率（也就是说B发生对A没有影响）<br>两个公式是一样的：</p>
<script type="math/tex; mode=display">
\begin{align*}  
P(A|B)&=P(A)\\
\frac{P(A\cap B)}{P(B)}&=P(A)\\
P(A\cap B)&=P(A)\cdot P(B)
\end{align*}</script><p>如果是多事件也是一样的：如果有m个事件那么先Cm取2每个算一遍Cm取3每个算一遍，最后Cm取m每个算一遍，全部独立我们才能说是互相独立的</p>
<script type="math/tex; mode=display">
\begin{align*}  
P(A_{i1}\cap A_{i2}\cap A_{i3}...\cap A_{im})&=P(A_{i1})\cdot P(A_{i2})\cdot P(A_{i3})\cdot ...\cdot P(A_{im}) 
\end{align*}</script><h1 id="排列（Permutation）"><a href="#排列（Permutation）" class="headerlink" title="排列（Permutation）"></a>排列（Permutation）</h1><p>判断事件是否是排列问题首先要看事件：<br>是否可区分=y，<br>有无放回=n，（无放回用阶乘）<br>顺序有无差异=y<br>比如：若有n个异物，从中依序取出k物有多少种结果</p>
<script type="math/tex; mode=display">
\begin{align*}  
P=\frac{n!}{n!-k!}
\end{align*}</script><h1 id="重复选取（Chooes-with-Replacements）"><a href="#重复选取（Chooes-with-Replacements）" class="headerlink" title="重复选取（Chooes with Replacements）"></a>重复选取（Chooes with Replacements）</h1><p>判断：<br>是否可区分=y，<br>有无放回=y，<br>顺序有无差异=y<br>比如：若有n个异物，从中依序取出k物（但每取出一个异物便放回去）有多少种结果</p>
<script type="math/tex; mode=display">
\begin{align*}  
P=k^{n}
\end{align*}</script><h1 id="组合（Combination）"><a href="#组合（Combination）" class="headerlink" title="组合（Combination）"></a>组合（Combination）</h1><p>判断：<br>是否可区分=y，<br>有无放回=n，（无放回用阶乘）<br>顺序有无差异=n，（无顺序用组合，因为一共会有k！个顺序所以除以多余k！）<br>比如：若有n异物，从中取出k物共有多少种结果？</p>
<script type="math/tex; mode=display">
\begin{align*}  
P=\frac{n!}{(n-k)!k!}
\end{align*}</script><p>$<br>C<em>{n}^{k}:又称为二项式系数，根据二项式定理\<br>(x+y)^{n}=\sum</em>{n}^{k=0}C_{n}^{k}x^{k}y^{n-k}<br>$</p>
<h1 id="多项组合（Multionomial）"><a href="#多项组合（Multionomial）" class="headerlink" title="多项组合（Multionomial）"></a>多项组合（Multionomial）</h1><p>其实就是几种组合的并集，也可以写成多项式的形式,直接记原始形式就行</p>
<script type="math/tex; mode=display">
\begin{align*}  
P&=C_{n1}^{n}\cdot C_{n2}^{n-n1}\cdot  C_{n3}^{n-n1-n2}...C_{n_{m}}^{n_{m}}\\
&=\frac{n!}{n_{1}!n_{2}!n_{3}!...n_{m}!}
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  
(x_{1}+x_{2}+x_{3}+...+x_{m})^{n}
=\sum_{n_{1}=0}^{n}\sum_{n_{2}=0}^{n}...\sum_{n_{m}=0}^{n}\frac{n!}{n_{1}!n_{2}!n_{3}!...n_{m}!}x_{n_{1}}^{1}x_{n_{2}}^{2}...x_{n_{m}}^{m}
\end{align*}</script><h1 id="随机变数-（Random-variable）"><a href="#随机变数-（Random-variable）" class="headerlink" title="随机变数 （Random variable）"></a>随机变数 （Random variable）</h1><ul>
<li>就是把实验结果数字化的表示方式，目的是使数学的推导更加简单明了，随机变数都是大写的字母</li>
<li>随机变数是一个函数，喂给F()一个随机变数，得出一个相应的概率</li>
</ul>
<h1 id="分布函数-CDF"><a href="#分布函数-CDF" class="headerlink" title="分布函数 CDF"></a>分布函数 CDF</h1><p>对于任意一个随机变数X，我们定义其CDF为函数：</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)= \overset{dif}{\rightarrow}P(X\leq x)
\end{align*}</script><p>例1：</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(0.5)= P(X\leq0.5)=\frac{1}{2}
\end{align*}</script><p>例2：</p>
<script type="math/tex; mode=display">
\begin{align*}  
P(3<X\leq 5)&=P(-\infty <X\leq 5)-P(-\infty <X\leq 3)\\
&=P(X\leq 5)-P(X\leq 3)\\
&=F_{X}(5)-F_{X}(3)
\end{align*}</script><ul>
<li>$x\leq 5^{-}=x&lt;5(CDF为离散变数时)，CDF在连续值时x&lt;5^{-}=x&lt;5$</li>
</ul>
<h1 id="密度函数-PMF"><a href="#密度函数-PMF" class="headerlink" title="密度函数 PMF"></a>密度函数 PMF</h1><p>又称质量函数，必须是离散函数</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)= \overset{dif}{\rightarrow}P(X = x)
\end{align*}</script><p>PMF是非常直观的，直接就是x发生的几率<br>Ex：X为骰子点数</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(3)= P(X=3)=\frac{1}{6}
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)=\sum_{n=-\infty }^{\left \lfloor x \right \rfloor}P_{X}(n)
\end{align*}</script><h2 id="Bernoulli-伯努利几率分布"><a href="#Bernoulli-伯努利几率分布" class="headerlink" title="Bernoulli 伯努利几率分布"></a>Bernoulli 伯努利几率分布</h2><p>PMF：只做一次实验，或成功或失败，X表示成功的概率</p>
<script type="math/tex; mode=display">
\begin{align*}  
X\sim Bernoulli (0.6)\\
P_{X}(x)=\left\{\begin{matrix}
0.6 & ,x=1\\ 
0.4 & ,x=0\\ 
0 & ,otherwise
\end{matrix}\right.
\end{align*}</script><h2 id="Binomial-二项式几率分布"><a href="#Binomial-二项式几率分布" class="headerlink" title="Binomial 二项式几率分布"></a>Binomial 二项式几率分布</h2><p>PMF：若实验成功率为0.6作10次实验，问8次成功的概率是多少？X表示成功次数<br>二项式定理就是说：每一种实验的组合都要乘上8次的成功几率，然后再乘上2次失败的几率</p>
<script type="math/tex; mode=display">
\begin{align*}  
P_{X}(x)&=X\sim BIN(10,0.6)\\
P_{X}(8)&=P(x=8)\\
&=C_{8}^{10}\cdot 0.6^{8}\cdot (1-0.6)^{10-8}
\end{align*}</script><p>CDF：</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=\sum_{n-\infty }^{\left \lfloor x \right \rfloor}P_{X}(x)\\
&=\sum_{n-\infty }^{\left \lfloor x \right \rfloor}C_{m}^{10}\cdot 0.6^{m}\cdot (1-0.6)^{n-m}
\end{align*}</script><h2 id="uniform-均匀几率分布"><a href="#uniform-均匀几率分布" class="headerlink" title="uniform 均匀几率分布"></a>uniform 均匀几率分布</h2><p>PMF:如果X等于3，4，5，…，7的几率均等</p>
<script type="math/tex; mode=display">
\begin{align*}  
P_{X}(x)&=X\sim UNIF(3,7)\\
p_{X}=\left\{\begin{matrix}
\frac{1}{7-3+1}=\frac{1}{5},&x=3,4,...,7\\ 
0,&otherwise
\end{matrix}\right.
\end{align*}</script><p>CDF</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=\sum_{n-\infty }^{\left \lfloor x \right \rfloor}P_{X}(x)\\
&=\left\{\begin{matrix}
0 &,x<3 \\ 
\frac{\left \lfloor x \right \rfloor-3+1}{6} &,3\leq x\leq 8 \\ 
 1& ,x\leq 8
\end{matrix}\right.
\end{align*}</script><h2 id="Geometric-几何几率分布"><a href="#Geometric-几何几率分布" class="headerlink" title="Geometric 几何几率分布"></a>Geometric 几何几率分布</h2><ul>
<li>有失意性<br>六脉神剑：打出来的概率是0.1，那么在他第十次打出来的概率是多少？<br>用9次失败的概率乘以一次成功的概率<br>PMF:几何级数，每一个跟前一个差1-p倍<script type="math/tex; mode=display">
P_{X}(x):X\sim Geometric(p)\\
P_{X}(x)=\left\{\begin{matrix}
(1-p)^{x-1} &,x=1,2,3,... \\ 
0 & ,otherwise
\end{matrix}\right.</script>CDF：<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=\sum_{n-\infty }^{\left \lfloor x \right \rfloor}P_{X}(x)\\
&=\left\{\begin{matrix}
x\geq 1:\sum_{n=1}^{\left \lfloor x \right \rfloor}(1-p)^{n-1}p=p\cdot\frac{1-(1-p)^{\left \lfloor x \right \rfloor}}{1-(1-p)} \\ 
x<1:0
\end{matrix}\right.\\
&=\left\{\begin{matrix}
1-(1-p)^{\left \lfloor x \right \rfloor} & ,x\geq 1 \\ 
0& ,otherxise
\end{matrix}\right.
\end{align*}</script></li>
</ul>
<h2 id="Pascal-几率分布"><a href="#Pascal-几率分布" class="headerlink" title="Pascal 几率分布"></a>Pascal 几率分布</h2><p>六脉神剑：打出来的几率是0.1，成功5次便耗尽功力，问他第9次刚好耗尽功力的概率（5次成功4次失败）</p>
<script type="math/tex; mode=display">
C_{4}^{8}\cdot C_{1}^{1}\cdot 0.9^{4}\cdot 0.1^{5}</script><p>PMF:</p>
<script type="math/tex; mode=display">
\begin{align*}  
P_{X}(x)&=X\sim Pascal(k,p)
&=\left\{\begin{matrix}
C_{k-1}^{x-1}\cdot (1-p)^{x-k}\cdot P^{k} & ,x=k,k+1,...\\ 
 0&,otherwise 
\end{matrix}\right.
\end{align*}</script><p>CDF: Pascal又称作Negative Binomial</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=P(X\leq x)\\
&=P(第k次成功在第x次以前)\\
&=P(在x次实验中\geq k次成功)\\
&=P(Y\geq k),Y\sim BIN(x,p)
\end{align*}</script><h2 id="Poisson-几率分布"><a href="#Poisson-几率分布" class="headerlink" title="Poisson 几率分布"></a>Poisson 几率分布</h2><p>已知某事发生速率每单位时间$\lambda$次，若在T时间(扩张缩减)里，会发生X次的几率是多少<br>PMF：</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=X\sim POI(\lambda T)\\
&=P(X=x)\\
&=e^{-\lambda T}\cdot \frac{(\lambda T)^{x}}{x!}
\end{align*}</script><p>有些书籍会用mu代替lambda T</p>
<script type="math/tex; mode=display">
\mu = \lambda T</script><p>CDF：</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=\sum_{n-\infty }^{\left \lfloor x \right \rfloor}P_{X}(x)\\
&=\left\{\begin{matrix}
\sum_{n=-\infty }^{\left \lfloor x \right \rfloor} e^{-\mu }\cdot \frac{\mu ^{n}}{n!},& x=0,1,2...\\ 
 0,& otherwise
\end{matrix}\right.
\end{align*}</script><p>Poisson的PMF推导</p>
<ul>
<li>将T切成长度$\delta T$的极小段</li>
<li>$\delta \Rightarrow 共有n=\frac{T}{\delta_{T}\rightarrow \infty }个小段$</li>
<li>若发生速率为$\lambda 次/分，每小时发生的几率P=\lambda \delta _{T}=\frac{\lambda T}{n}$</li>
<li>故T时间内发生的次数$X\sim BIN(n,p)=BIN(n,\frac{\lambda T}{n})$<script type="math/tex; mode=display">
\begin{align*}  
\lim_{\delta _{T}\rightarrow 0}P_{X}(x)&=\lim_{n\rightarrow \infty }\frac{n!}{(n-x)!x!}(\frac{\lambda T}{n})^{x}(1-\frac{\lambda T}{n})^{n-x}\\
&=\lim_{n\rightarrow \infty }\frac{n(n-1)...(n-n+1)}{x!}\frac{(\lambda T)^{x}}{n^{x}}(1-\frac{\lambda T}{n})^{n-x}\\
&=\lim_{n\rightarrow \infty }\frac{1}{x!}\frac{n}{n}...\frac{n-x+1}{n}(\lambda T)^{x}(1-\frac{\lambda T}{n})^{n}(1-\frac{\lambda T}{n})^{-x}\\
&=\frac{(\lambda T)^{x}}{x!}\lim_{n\rightarrow \infty }(1-\frac{\lambda T}{n})^{n}\\
&=\frac{(\lambda T)^{x}}{x!}e^{-\lambda T}
\end{align*}</script></li>
</ul>
<h1 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h1><p>连续的变数的几率分布有不均等</p>
<script type="math/tex; mode=display">
\begin{align*}  
PDF:f_{X}(x)&=\lim_{\Delta x\rightarrow 0}\frac{P(x\leq X\leq x+\Delta x)}{\Delta x}\\
&=\lim_{\Delta x\rightarrow 0}\frac{F_{X}(x+\Delta x)-F_{X}(x)}{\Delta x}\\
&=F'_{X}(x)
\end{align*}</script><p>由此可得PDF就是CDF的积分；反之CDF就是PDF的积分：</p>
<script type="math/tex; mode=display">
\begin{align*}  
CDFF_{X}(x)\xrightarrow[\int_{-\infty }^{x}]{\frac{d}{dx}}PDFf_{X}(x)
\end{align*}</script><p>用几率表示PDF</p>
<script type="math/tex; mode=display">
\begin{align*}  
P(a<X\leq b)&=F_{X}(b)-F_{X}(a)\\
&=\int_{-\infty }^{b}F_{X}(x)dx-\int_{-\infty }^{a}F_{X}(x)dx\\
&=\int_{a }^{b}F_{X}(x)dx
\end{align*}</script><p>另一个跟几率的关系</p>
<script type="math/tex; mode=display">
\begin{align*}  
PDF:f_{X}(x)&=\lim_{\Delta x\rightarrow 0}\frac{P(x\leq X\leq x+\Delta x)}{\Delta x}\\
P(x\leq X\leq x+\Delta x)&\approx f_{X}(x)\cdot \Delta x
\end{align*}</script><p>关于PDF，PMF，CDF：<br><a href="https://blog.csdn.net/wzgbm/article/details/51680540" target="_blank" rel="noopener">https://blog.csdn.net/wzgbm/article/details/51680540</a></p>
<h2 id="Uniform-几率分布（连续变量）"><a href="#Uniform-几率分布（连续变量）" class="headerlink" title="Uniform 几率分布（连续变量）"></a>Uniform 几率分布（连续变量）</h2><p>PDF:</p>
<script type="math/tex; mode=display">
\begin{align*}  
f_{X}(x)=\left\{\begin{matrix}
\frac{1}{b-a} &,a\leq x\leq b \\ 
0 &, otherwise
\end{matrix}\right.
\end{align*}</script><p>CDF:</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=\int_{-\infty }^{x}f_{X}(u)du\\
&=\left\{\begin{matrix}
 0&,x\leq a \\ 
 \frac{x-a}{b-a}&,a<x\leq b \\ 
 1& ,x>b
\end{matrix}\right.
\end{align*}</script><p>Ex:已知1路公车每十分钟一班，小美随意出发到公车站，小美需等车时间为X的几率是多少</p>
<h2 id="Exponential-几率分布"><a href="#Exponential-几率分布" class="headerlink" title="Exponential 几率分布"></a>Exponential 几率分布</h2><p>有失忆性memoryless，常被用来model有失忆性质的事情<br>PDF:</p>
<script type="math/tex; mode=display">
f_{X}(x)=\left\{\begin{matrix}
\lambda e^{-\lambda x} &,x\geq 0 \\ 
 0&,otherwise 
\end{matrix}\right.</script><p>CDF:<br>$If:x\geq 0$</p>
<script type="math/tex; mode=display">
\begin{align*}  
F_{X}(x)&=\int_{-\infty }^{x}f_{X}(u)du\\
&=\int_{0 }^{x}\lambda e^{-\lambda u}du\\
&=-\int_{0 }^{x}e^{-\lambda u}d(-\lambda u)\\
&=-[e^{-\lambda u}]_{0}^{x}\\
&=1-e^{-\lambda u}
\end{align*}</script><p>$If:x&lt;0 $</p>
<p>$F_{X}(x)=0$</p>
<h1 id="Erlang-几率分布"><a href="#Erlang-几率分布" class="headerlink" title="Erlang 几率分布"></a>Erlang 几率分布</h1>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/19/statistics-by-Libaijian01/">Statistics by Libaijian</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/19/statistics-by-Libaijian01/" class="archive-article-date"><time datetime="2019-08-19T10:11:27.000Z" itemprop="datePublished">August 19th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="抽样种类"><a href="#抽样种类" class="headerlink" title="抽样种类"></a>抽样种类</h1><hr>
<h2 id="随机抽样"><a href="#随机抽样" class="headerlink" title="随机抽样"></a>随机抽样</h2><h3 id="简单随机抽样-Simple-Random-sampling"><a href="#简单随机抽样-Simple-Random-sampling" class="headerlink" title="简单随机抽样 Simple Random sampling"></a>简单随机抽样 Simple Random sampling</h3><p>Simple Random sampling<br>必须取得母体全体，才能够进行（困难）</p>
<h3 id="系统抽样-systematic-sampling"><a href="#系统抽样-systematic-sampling" class="headerlink" title="系统抽样 systematic sampling"></a>系统抽样 systematic sampling</h3><p>systematic sampling<br>将母体平均分成k个样本，然后进行样本抽取</p>
<h3 id="分层抽样-Stratified-sampling"><a href="#分层抽样-Stratified-sampling" class="headerlink" title="分层抽样 Stratified sampling"></a>分层抽样 Stratified sampling</h3><p>Stratified sampling<br>分层每层无交集（适用高中低差别大的样本）</p>
<h3 id="部落抽样-cluster-sampling"><a href="#部落抽样-cluster-sampling" class="headerlink" title="部落抽样 cluster sampling"></a>部落抽样 cluster sampling</h3><p>cluster sampling</p>
<h2 id="随机用一个部落为整体（不适用高中低差别大的样本）"><a href="#随机用一个部落为整体（不适用高中低差别大的样本）" class="headerlink" title="随机用一个部落为整体（不适用高中低差别大的样本）"></a>随机用一个部落为整体（不适用高中低差别大的样本）</h2><h2 id="非随机抽样"><a href="#非随机抽样" class="headerlink" title="非随机抽样"></a>非随机抽样</h2><h3 id="偶遇抽样-convenience-sampling"><a href="#偶遇抽样-convenience-sampling" class="headerlink" title="偶遇抽样 convenience sampling"></a>偶遇抽样 convenience sampling</h3><p>convenience sampling<br>最常用（比如地铁口发传单）最方便，市场的初步调查</p>
<h3 id="配额抽样query-sampling"><a href="#配额抽样query-sampling" class="headerlink" title="配额抽样query  sampling"></a>配额抽样query  sampling</h3><p>query  sampling<br>跟分层抽样区别为这个是人员分配的层</p>
<h3 id="主观抽样-judgmental-sampling"><a href="#主观抽样-judgmental-sampling" class="headerlink" title="主观抽样 judgmental sampling"></a>主观抽样 judgmental sampling</h3><p>judgmental sampling<br>已经对母体非常熟悉，抽取具有代表性的样本</p>
<h3 id="滚雪球抽样snowball-sampling"><a href="#滚雪球抽样snowball-sampling" class="headerlink" title="滚雪球抽样snowball sampling"></a>滚雪球抽样snowball sampling</h3><p>snowball sampling<br>用抽样人员的直线分支下去抽样</p>
<hr>
<h2 id="效度validity"><a href="#效度validity" class="headerlink" title="效度validity"></a>效度validity</h2><p>内部效度：可信度，可靠度<br>外部效度：结果的泛化程度</p>
<p>以偏概全<br>一叶知秋</p>
<h1 id="尺度定义"><a href="#尺度定义" class="headerlink" title="尺度定义"></a>尺度定义</h1><h2 id="名义尺度-nominal"><a href="#名义尺度-nominal" class="headerlink" title="名义尺度 nominal"></a>名义尺度 nominal</h2><p>性别，职业，学号（没有顺序，不具有大小的意义）</p>
<h2 id="顺序0rdinal"><a href="#顺序0rdinal" class="headerlink" title="顺序0rdinal"></a>顺序0rdinal</h2><p>名次，排序（有大小，有先后）</p>
<h2 id="区间尺度Interval"><a href="#区间尺度Interval" class="headerlink" title="区间尺度Interval"></a>区间尺度Interval</h2><p>等距的，没有固定的原点（温度，满意度调查评价：1分2分）</p>
<h3 id="李克特量表-likert"><a href="#李克特量表-likert" class="headerlink" title="李克特量表 likert"></a>李克特量表 likert</h3><p>问卷调查中最常用的去坚尺度<br>李克特量表：（非常满意 满意 普通 不满意）</p>
<h2 id="比率尺度Ratio"><a href="#比率尺度Ratio" class="headerlink" title="比率尺度Ratio"></a>比率尺度Ratio</h2><p>可以衡量差异的数值，有原点，可以进行运算，（分数）</p>
<h1 id="柴比雪夫不等式-Chebyshev-inequality"><a href="#柴比雪夫不等式-Chebyshev-inequality" class="headerlink" title="柴比雪夫不等式 Chebyshev inequality"></a>柴比雪夫不等式 Chebyshev inequality</h1><hr>
<h2 id="经验法则-empirical-law"><a href="#经验法则-empirical-law" class="headerlink" title="经验法则 empirical law"></a>经验法则 empirical law</h2><ul>
<li>百分比就是几率</li>
<li>必须是正态分布<br>mu为样本的平均值，sigema为标准差<script type="math/tex; mode=display">
[\mu -\sigma ，\mu +\sigma ，]\approx 68%
[\mu -2\sigma ，\mu +2\sigma ，]\approx 95%
[\mu -3\sigma ，\mu +3\sigma ，]\approx 99.7%</script></li>
</ul>
<h2 id="柴比雪夫不等式-Chebyshev-inequality-1"><a href="#柴比雪夫不等式-Chebyshev-inequality-1" class="headerlink" title="柴比雪夫不等式 Chebyshev inequality"></a>柴比雪夫不等式 Chebyshev inequality</h2><ul>
<li>不用必须是正态分布</li>
</ul>
<script type="math/tex; mode=display">
P(|X-\mu | \leq k \sigma )\geq 1-\frac{1}{k^{2}};k>1</script><p>列题：<br><img src="/images/statisticsstatisticsLibaijian/02.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/03.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/04.png" alt="issue"></p>
<h2 id="馬可夫不等式-markov"><a href="#馬可夫不等式-markov" class="headerlink" title="馬可夫不等式 markov"></a>馬可夫不等式 markov</h2><ul>
<li><p>$P(X\geq k)\leq \frac{E(X)}{k}$<br><img src="/images/statisticsstatisticsLibaijian/05.png" alt="issue"></p>
</li>
<li><p>柴比雪夫不等式單邊版特例<br>证明<br><img src="/images/statisticsstatisticsLibaijian/06.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/07.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/08png" alt="issue"></p>
</li>
</ul>
<h1 id="排列组合"><a href="#排列组合" class="headerlink" title="排列组合"></a>排列组合</h1><h2 id="乘法原理"><a href="#乘法原理" class="headerlink" title="乘法原理"></a>乘法原理</h2><p>总数的阶乘，总数中取所有数的概率（也就是说总数中每个数都可以随意搭配）<br>排列中如果只取m个数的话，那么其实总数中就多出来总数减m个的概率，这个时候要除以多出来的概率<br><img src="/images/statisticsstatisticsLibaijian/09.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/10.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/11.png" alt="issue"></p>
<h2 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h2><p>组合=排列（挑选出的数不分左右，前后，12和21是一样的）</p>
<ul>
<li><p>因为分子是不需要排列的所以除以排列数m阶乘<br><img src="/images/statisticsstatisticsLibaijian/12.png" alt="issue"></p>
</li>
<li><p>重复组合的问题<br><img src="/images/statisticsstatisticsLibaijian/13.png" alt="issue"></p>
</li>
<li>变换成公式标准形态<br><img src="/images/statisticsstatisticsLibaijian/14.png" alt="issue"><br>组合+乘法原理=排列</li>
</ul>
<h2 id="二项式定理和多项式定理"><a href="#二项式定理和多项式定理" class="headerlink" title="二项式定理和多项式定理"></a>二项式定理和多项式定理</h2><p><img src="/images/statisticsstatisticsLibaijian/15.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/16.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/17.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/18.png" alt="issue"></p>
<h2 id="几率"><a href="#几率" class="headerlink" title="几率"></a>几率</h2><p><img src="/images/statisticsstatisticsLibaijian/19.png" alt="issue"></p>
<h2 id="排容原理"><a href="#排容原理" class="headerlink" title="排容原理"></a>排容原理</h2><p><img src="/images/statisticsstatisticsLibaijian/20.png" alt="issue"></p>
<h2 id="独立事件"><a href="#独立事件" class="headerlink" title="独立事件"></a>独立事件</h2><p><img src="/images/statisticsstatisticsLibaijian/21.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/22.png" alt="issue"></p>
<h2 id="互斥事件"><a href="#互斥事件" class="headerlink" title="互斥事件"></a>互斥事件</h2><p><img src="/images/statisticsstatisticsLibaijian/23.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/24.png" alt="issue"></p>
<h2 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h2><p><img src="/images/statisticsstatisticsLibaijian/25.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/26.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/27.png" alt="issue"></p>
<h1 id="随机变数"><a href="#随机变数" class="headerlink" title="随机变数"></a>随机变数</h1><p><img src="/images/statisticsstatisticsLibaijian/28.png" alt="issue"><br><img src="/images/statisticsstatisticsLibaijian/29.png" alt="issue"></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/15/indefinite-integral-of-Libaijian/">indefinite integral of Libaijian</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/15/indefinite-integral-of-Libaijian/" class="archive-article-date"><time datetime="2019-08-15T09:09:13.000Z" itemprop="datePublished">August 15th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Anti-Derivatives"><a href="#Anti-Derivatives" class="headerlink" title="Anti Derivatives"></a>Anti Derivatives</h3><script type="math/tex; mode=display">
\int f(x)dx=F(x)+C</script><script type="math/tex; mode=display">
\frac{d}{dx}\int f(x)dx=f(x)</script><script type="math/tex; mode=display">
\int f'(x)dx=f(x)+C</script><p>因为答案不是一个固定的函数是一个整族的函数所以叫不定积分</p>
<h3 id="Basic-Differentiation-Law"><a href="#Basic-Differentiation-Law" class="headerlink" title="Basic Differentiation Law"></a>Basic Differentiation Law</h3><script type="math/tex; mode=display">
\int kdx=kx+C</script><script type="math/tex; mode=display">
\int kf(x)dx=k\int f(x)dx</script><script type="math/tex; mode=display">
\int x^{n}dx=\left\{\begin{matrix}
\frac{1}{n+1}x^{n+1}+C ;n\neq -1\\ 
lnX+C;n=-1
\end{matrix}\right.</script><h3 id="变数变换法"><a href="#变数变换法" class="headerlink" title="变数变换法"></a>变数变换法</h3><p>$\int (x+3)^{2}dx;令u=x+3;du=dx;\int u^{2}du=\frac{1}{3}u^{3}+C=\frac{1}{3}(x+3)^{3}+C$</p>
<script type="math/tex; mode=display">
\int (3x+3)^{10}dx;Let:u=3x+3;du=3dx; \frac{1}{11}u^{11}\cdot \frac{1}{3}+C=\frac{1}{33}(3x+3)^{11}</script><script type="math/tex; mode=display">
\int e^{10x}dx=Let:u=10x;du=10dx;\frac{1}{10}\cdot e^{10x}</script><script type="math/tex; mode=display">
\int \frac{1}{3x+2}dx;Let:u=3x+2;du=3dx;\frac{1}{3}ln(3x+2)</script><script type="math/tex; mode=display">
\int \frac{x^{2}+1}{x}dx=\int \frac{x^{2}}{x}+\frac{1}{x}dx=\frac{1}{2}x^{2}+lnx</script><p>以下题目化简时要确定dx前后有一样的数字或一组数字可以化成u</p>
<script type="math/tex; mode=display">
\int  2x(x+1)10dx=\int (x^{2}+1)^{10}dx^{2}=\frac{1}{11}(x^{2}+1)^{11}+C</script><script type="math/tex; mode=display">
\int sin^{10}x\cdot cosdx=\int sin^{10}x\cdot dsinx=\frac{1}{11}sin^{11}x+C</script><p><img src="/images/indefinite_integral_libaijian/02.png" alt="issue"><br><img src="/images/indefinite_integral_libaijian/03.png" alt="issue"></p>
<h3 id="三角代换法"><a href="#三角代换法" class="headerlink" title="三角代换法"></a>三角代换法</h3><p><img src="/images/indefinite_integral_libaijian/04.png" alt="issue"></p>
<script type="math/tex; mode=display">
\int \sqrt{4-x^{2}}dx</script><p>根据</p>
<script type="math/tex; mode=display">
sin^{2}\theta +cos^{2}\theta =1;\therefore \sqrt{1-sin^{2}\theta }=\sqrt{cos^{2}\theta };</script><p>令$x=2sin\theta;dx=2cos\theta d\theta $<br>平方化倍角（平方难算，倍角用变数算）</p>
<script type="math/tex; mode=display">
cos^{2}\theta =\frac{1+cos2\theta }{2}</script><script type="math/tex; mode=display">
\int \sqrt{4-4sin\theta^{2} }2cos\theta d\theta =\int \sqrt{4(1-sin^{2}\theta )}2cos\theta d\theta =\int 4cos^{2}\theta d\theta</script><script type="math/tex; mode=display">
\int \sqrt{4+x^{2}}dx;Let:x=2tan\theta</script><script type="math/tex; mode=display">
\int \sqrt{x^{2}-4}dx;Let:x=2sec\theta</script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/15/Apriori-by-A-Z/">Apriori by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/15/Apriori-by-A-Z/" class="archive-article-date"><time datetime="2019-08-15T01:08:39.000Z" itemprop="datePublished">August 15th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>Association Rule Learning Apriori Intuition<br>(关联算法规则学习)</p>
</li>
<li><p>Apriori（拉丁词汇，先知的先觉得，先验算法）</p>
</li>
<li>先验算法 - 原理<br><img src="/images/A-Z_ML/233.png" alt><br><img src="/images/A-Z_ML/234.png" alt></li>
<li>先验算法 - 支持度<br><img src="/images/A-Z_ML/235.png" alt><br><img src="/images/A-Z_ML/236.png" alt></li>
<li>先验算法 - 信息水准<br><img src="/images/A-Z_ML/237.png" alt><br><img src="/images/A-Z_ML/238.png" alt></li>
<li>先验算法 - 提升度<br><img src="/images/A-Z_ML/239.png" alt><br>M1对M2的提升度有多少<br><img src="/images/A-Z_ML/240.png" alt><br><img src="/images/A-Z_ML/241.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/14/Apriori-of-Python-by-A-Z/">Apriori of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/14/Apriori-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-08-14T01:35:10.000Z" itemprop="datePublished">August 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <a class="fancybox" href="/images/A-Z_ML/244.png" title="[/images/A-Z_ML/244.png] [img_caption]"><img src="/images/A-Z_ML/244.png" alt="[/images/A-Z_ML/244.png] [img_caption]">
<!-- ![issue](/images/A-Z_ML/244.png) -->
<ul>
<li>数据是商场的用户的消费详情</li>
</ul>
<script src="https://gist.github.com/SauronLee/c3482834d2cb1dbab83abeef32da9d9d.js"></script>

<hr>
<p>transactions = []<br>for i in range(0, 7501):<br>    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])</p>
<!-- ![issue](/images/A-Z_ML/245.png) -->
<h1 id="Visualising-the-results"><a href="#Visualising-the-results" class="headerlink" title="Visualising the results"></a>Visualising the results</h1><p>results = list(rules)<br>myResults = [list(x) for x in results]<br><!-- ![issue](/images/A-Z_ML/246.png) --></p>
<h2 id><a href="#" class="headerlink" title></a><a class="fancybox" href="/images/A-Z_ML/246.png" title="[/images/A-Z_ML/246.png] [img_caption]"><img src="/images/A-Z_ML/246.png" alt="[/images/A-Z_ML/246.png] [img_caption]"></a></h2></a>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/14/Apriori-of-R-by-A-Z/">Apriori of R by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/14/Apriori-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-08-14T01:35:01.000Z" itemprop="datePublished">August 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/242.png" alt></p>
<ul>
<li>数据是商场的用户的消费详情</li>
</ul>
<script src="https://gist.github.com/SauronLee/e74ba75daa5b3e20486f961417c13566.js"></script>

<hr>
<pre><code>
&gt; dataset = read.csv(&#39;Market_Basket_Optimisation.csv&#39;, header = FALSE)
&gt; dataset = read.transactions(&#39;Market_Basket_Optimisation.csv&#39;, sep = &#39;,&#39;, rm.duplicates = TRUE)
distribution of transactions with duplicates:
# 重复一次出现在5行当中
1 
5 
&gt; summary(dataset)
transactions as itemMatrix in sparse format with
 7501 rows (elements/itemsets/transactions) and
 # 稀疏矩阵，1占0.03288973 其他都是0
 119 columns (items) and a density of 0.03288973 

most frequent items:
mineral water          eggs     spaghetti  french fries     chocolate       (Other) 
         1788          1348          1306          1282          1229         22405 

element (itemset/transaction) length distribution:
sizes
# 有1754笔交易只有1个产品
   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   18   19   20 
1754 1358 1044  816  667  493  391  324  259  139  102   67   40   22   17    4    1    2    1 

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.000   2.000   3.000   3.914   5.000  20.000 

includes extended item information - examples:
             labels
1           almonds
2 antioxydant juice
3         asparagus
&gt; itemFrequencyPlot(dataset, topN = 10)
&gt; # Training Apriori on the dataset
&gt; rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))
Apriori

Parameter specification:
 confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target   ext
        0.2    0.1    1 none FALSE            TRUE       5   0.004      1     10  rules FALSE

Algorithmic control:
 filter tree heap memopt load sort verbose
    0.1 TRUE TRUE  FALSE TRUE    2    TRUE

Absolute minimum support count: 30 

set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[119 item(s), 7501 transaction(s)] done [0.01s].
sorting and recoding items ... [114 item(s)] done [0.00s].
creating transaction tree ... done [0.00s].
checking subsets of size 1 2 3 4 done [0.16s].
writing ... [811 rule(s)] done [0.00s].
creating S4 object  ... done [0.00s].
&gt; # Visualising the results
&gt; inspect(sort(rules, by = &#39;lift&#39;)[1:10])
     lhs                                            rhs             support     confidence
[1]  {light cream}                               =&gt; {chicken}       0.004532729 0.2905983 
[2]  {pasta}                                     =&gt; {escalope}      0.005865885 0.3728814 
[3]  {pasta}                                     =&gt; {shrimp}        0.005065991 0.3220339 
[4]  {eggs,ground beef}                          =&gt; {herb &amp; pepper} 0.004132782 0.2066667 
[5]  {whole wheat pasta}                         =&gt; {olive oil}     0.007998933 0.2714932 
[6]  {herb &amp; pepper,spaghetti}                   =&gt; {ground beef}   0.006399147 0.3934426 
[7]  {herb &amp; pepper,mineral water}               =&gt; {ground beef}   0.006665778 0.3906250 
[8]  {tomato sauce}                              =&gt; {ground beef}   0.005332622 0.3773585 
[9]  {mushroom cream sauce}                      =&gt; {escalope}      0.005732569 0.3006993 
[10] {frozen vegetables,mineral water,spaghetti} =&gt; {ground beef}   0.004399413 0.3666667 
     lift     count
[1]  4.843951 34   
[2]  4.700812 44   
[3]  4.506672 38   
[4]  4.178455 31   
[5]  4.122410 60   
[6]  4.004360 48   
[7]  3.975683 50   
[8]  3.840659 40   
[9]  3.790833 43   
[10] 3.731841 33   
&gt; 
&gt;
</code></pre><ul>
<li>每个产品出现的频率<br>itemFrequencyPlot(dataset, topN = 10)<br><img src="/images/A-Z_ML/242.png" alt></li>
</ul>
<p>transactions-class {arules}    R Documentation<br>Class transactions — Binary Incidence Matrix for Transactions<br>Description<br>The transactions class represents transaction data used for mining itemsets or rules. It is a direct extension of class itemMatrix to store a binary incidence matrix, item labels, and optionally transaction IDs and user IDs.</p>
<p>Details<br>Transactions can be created by coercion from lists containing transactions, but also from matrix and data.frames. However, you will need to prepare your data first (see coercion methods in the Methods Section and the Example Section below for details on the needed format).</p>
<p>Continuous variables: Association rule mining can only use items and does not work with continuous variables. Continuous variables need to be discretized first. An item resulting from discretization might be age&gt;18 and the column contains only TRUE or FALSE. Alternatively it can be a factor with levels age&lt;=18, 50=&gt;age&gt;18 and age&gt;50. These will be automatically converted into 3 items, one for each level. Have a look at the function discretize for automatic discretization.</p>
<p>Logical variables: A logical variable describing a person could be tall indicating if the person is tall using the values TRUE and FALSE. The fact that the person is tall would be encoded in the transaction containing the item tall while not tall persons would not have this item. Therefore, for logical variables, the TRUE value is converted into an item with the name of the variable and for the FALSE values no item is created.</p>
<p>Factors: The function also can convert columns with nominal values (i.e., factors) into a series of binary items (one for each level constructed as <code>variable name</code>=<code>level</code>). Note that nominal variables need to be encoded as factors (and not characters or numbers). This can be done with</p>
<p>data[,”a_nominal_var”] &lt;- factor(data[,”a_nominal_var”]).</p>
<p>Complete examples for how to prepare data can be found in the man pages for Income and Adult.</p>
<p>Transactions are represented as sparse binary matrices of class itemMatrix. If you work with several transaction sets at the same time, then the encoding (order of the items in the binary matrix) in the different sets is important. See itemCoding to learn how to encode and recode transaction sets.</p>
<p>Objects from the Class<br>Objects are created by coercion from objects of other classes (see Examples section) or by calls of the form new(“transactions”, …).</p>
<p>Slots<br>itemsetInfo:<br>a data.frame with one row per transaction (each transaction is considered an itemset). The data.frame can hold columns with additional information, e.g., transaction IDs or user IDs for each transaction. Note: this slot is inherited from class itemMatrix, but should be accessed in transactions with the method transactionInfo().</p>
<p>data:<br>object of class ngCMatrix to store the binary incidence matrix (see itemMatrix class)</p>
<p>itemInfo:<br>a data.frame to store item labels (see itemMatrix class)</p>
<p>Extends<br>Class itemMatrix, directly.</p>
<p>Methods<br>coerce<br>signature(from = “matrix”, to = “transactions”); produces a transactions data set from a binary incidence matrix. The column names are used as item labels and the row names are stores as transaction IDs.</p>
<p>coerce<br>signature(from = “transactions”, to = “matrix”); coerces the transactions data set into a binary incidence matrix.</p>
<p>coerce<br>signature(from = “list”, to = “transactions”); produces a transactions data set from a list. The names of the items in the list are used as item labels.</p>
<p>coerce<br>signature(from = “transactions”, to = “list”); coerces the transactions data set into a list of transactions. Each transaction is a vector of character strings (names of the contained items).</p>
<p>coerce<br>signature(from = “data.frame”, to = “transactions”); recodes the data frame containing only categorical variables (factors) or logicals all into a binary transaction data set. For binary variables only TRUE values are converted into items and the item label is the variable name. For factors, a dummy item for each level is automatically generated. Item labels are generated by concatenating variable names and levels with “=”. The original variable names and levels are stored in the itemInfo data frame as the components variables and levels. Note that NAs are ignored (i.e., do not generate an item).</p>
<p>coerce<br>signature(from = “transactions”, to = “data.frame”); represents the set of transactions in a printable form as a data.frame. Note that this does not reverse coercion from data.frame to transactions.</p>
<p>coerce<br>signature(from = “ngCMatrix”, to = “transactions”); Note that the data is stored transposed in the ngCMatrix. Items are stored as rows and transactions are columns!</p>
<p>dimnames, rownames, colnames<br>signature(x = “transactions”); returns row (transactionID) and column (item) names.</p>
<p>items<br>signature(x = “transactions”); returns the items in the transactions as an itemMatrix.</p>
<p>labels<br>signature(x = “transactions”); returns the labels for the itemsets in each transaction (see itemMatrix).</p>
<p>transactionInfo&lt;-<br>signature(x = “transactions”); replaces the transaction information with a new data.frame.</p>
<p>transactionInfo<br>signature(x = “transactions”); returns the transaction information as a data.frame.</p>
<p>show<br>signature(object = “transactions”)</p>
<p>summary<br>signature(object = “transactions”)</p>
<ul>
<li>apriori {arules}    R Documentation<br>Mining Associations with Apriori<br>Description<br>Mine frequent itemsets, association rules or association hyperedges using the Apriori algorithm. The Apriori algorithm employs level-wise search for frequent itemsets. The implementation of Apriori used includes some improvements (e.g., a prefix tree and item sorting).</li>
</ul>
<p>Usage<br>apriori(data, parameter = NULL, appearance = NULL, control = NULL)<br>Arguments<br>data<br>object of class transactions or any data structure which can be coerced into transactions (e.g., a binary matrix or data.frame).</p>
<p>parameter<br>object of class APparameter or named list. The default behavior is to mine rules with minimum support of 0.1, minimum confidence of 0.8, maximum of 10 items (maxlen), and a maximal time for subset checking of 5 seconds (maxtime).</p>
<p>appearance<br>object of class APappearance or named list. With this argument item appearance can be restricted (implements rule templates). By default all items can appear unrestricted.</p>
<p>control<br>object of class APcontrol or named list. Controls the algorithmic performance of the mining algorithm (item sorting, report progress (verbose), etc.)</p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/14/Integral-of-HouPhD/">Integral of HouPhD</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/14/Integral-of-HouPhD/" class="archive-article-date"><time datetime="2019-08-13T16:39:12.000Z" itemprop="datePublished">August 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="不定积分基础公式"><a href="#不定积分基础公式" class="headerlink" title="不定积分基础公式"></a>不定积分基础公式</h2><script type="math/tex; mode=display">
\int kdx = kx+C\\
\int x^{\mu }dx=\frac{x^{\mu +1}}{\mu +1}+C)(\mu \not\equiv -1)\\
\int a^{x}dx=\frac{1}{lna}a^{x}+C\\
\int e^{x}dx=e^{x}+C\\
\int sinxdx=-cosx+C\\
\int cosxdx=sinx+C\\
\int \frac{dx}{x}=ln\left | x \right |+C(x\neq 0)\\
\int tanxdx=-ln\left | cosx \right |+C\\
\int cotxdx=ln\left | sinx \right |+C\\
\int \frac{1}{cosx}dx=ln\left | \frac{1}{cosx} +tanx \right |+C\\
\int \frac{1}{sinx}dx=ln\left | \frac{1}{sinx}-cotx \right |+C\\
\int \frac{1}{cos^{2}x}dx=tanx+C\\
\int \frac{1}{sin^{2}x}dx=-cosx+C\\
\int \frac{sinx}{cos^{2}x}dx=\frac{1}{cosx}+C\\
\int \frac{cosx}{sin^{2}x}dx=-\frac{1}{sinx}+C\\
\int \frac{1}{\sqrt{a^{2}-x^{2}}}dx=arcsin\frac{x}{a}+C\\
\int \frac{1}{a^{2}+x^{2}}dx=\frac{1}{a}arctan\frac{x}{a}+C\\
\int \frac{1}{x^{2}-a^{2}}dx=\frac{1}{2a}ln\left | \frac{x-a}{x+a} \right |+C\\
\int \frac{1}{a^{2}-x^{2}}dx=\frac{1}{2a}ln\left | \frac{a+x}{a-x} \right |+C\\
\int \frac{1}{\sqrt{x^{2}\pm a^{2}}}dx=ln\left | x+\sqrt{x^{2}\pm a^{2}} \right |+C</script><h2 id="Let部分再算积分"><a href="#Let部分再算积分" class="headerlink" title="Let部分再算积分"></a>Let部分再算积分</h2>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/13/Naive-Bayes-of-python-by-A-Z/">Naive Bayes of python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/13/Naive-Bayes-of-python-by-A-Z/" class="archive-article-date"><time datetime="2019-08-13T06:57:44.000Z" itemprop="datePublished">August 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/77.png" alt></p>
<script src="https://gist.github.com/SauronLee/3b8d2be2ae591a894876efc4ec63acff.js"></script>

<p><img src="/images/A-Z_ML/78.png" alt><br><img src="/images/A-Z_ML/79.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/12/Higher-Math-Basis-of-PengTitus/">Higher Math Basis of PengTitus</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/12/Higher-Math-Basis-of-PengTitus/" class="archive-article-date"><time datetime="2019-08-11T17:40:29.000Z" itemprop="datePublished">August 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="开区间与闭区间"><a href="#开区间与闭区间" class="headerlink" title="开区间与闭区间"></a>开区间与闭区间</h4><ul>
<li>拓扑学：在一个集合上边引用一个开集的构造<br>（a,b)={x|a&lt;x&lt;b}<br>[a,b]={x|a&lt;=x&lt;=b}<br>[a,+∞]={x|a&lt;=x}<br>(a,+∞]={x|a&lt;x}<h4 id="邻域Neighbourhood与去心邻域的定义"><a href="#邻域Neighbourhood与去心邻域的定义" class="headerlink" title="邻域Neighbourhood与去心邻域的定义"></a>邻域Neighbourhood与去心邻域的定义</h4></li>
<li>邻域：<br>{x|a-∆&lt;x&lt;a+∆}<br>N(a,∆)</li>
<li>去心邻域<br>N(a_hat,∆)={x|0&lt;x-a&lt;∆}<br>=N(a,∆){a}<h4 id="三角不等式Triangle-inequality"><a href="#三角不等式Triangle-inequality" class="headerlink" title="三角不等式Triangle inequality"></a>三角不等式Triangle inequality</h4>|a+b|&gt;=|a|+|b|<br>(a+b)&gt;=|a|-|b|<h4 id="柯西不等式CauchySchwarz-inequality"><a href="#柯西不等式CauchySchwarz-inequality" class="headerlink" title="柯西不等式CauchySchwarz inequality"></a>柯西不等式CauchySchwarz inequality</h4><script type="math/tex; mode=display">(a_{1}^{2}+a_{2}^{2}+...+a_{n}^{2})((b_{1}^{2}+b_{2}^{2}+...+b_{n}^{2}))\geq (a_{1}b_{1}+a_{2}b_{2}+...+a_{n}b_{n})^{2}</script>向量证法：如果两个向量垂直则点积为0，因为cos90°=0，反之不是，如果零向量与任何向量的点积都是0<br>代数证法：$\sum<em>{n}^{k=1}(a</em>{k}+tb_{k})^{2}\geq 0=A+2ct+t^{2}B;代入t=-\frac{c}{b}$</li>
<li>柯西不等式可以证明算数平均一定会大于几何平均<script type="math/tex; mode=display">
(a_{1}^{2}+a_{2}^{2})(b_{1}^{2}b_{2}^{2})\geq (a_{1}b_{1}+a_{2}b_{2})^{2}\\
Let a_{1}=\sqrt{x};a_{2}=\sqrt{y}\\
b_{1}=\sqrt{y};b_{2}=\sqrt{x}
\frac{xy}{2}\geq \sqrt{xy}</script><img src="/images/Higher_Math_of_PengTitus/kexibudengshi01.HEIC" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/kexibudengshi.HEIC" alt="issue"><h4 id="算几不等式的构造式证明"><a href="#算几不等式的构造式证明" class="headerlink" title="算几不等式的构造式证明"></a>算几不等式的构造式证明</h4>算几不等式几何证明</li>
<li>半圆内的连接直径直角三角形的高一定小于等于圆的半径<br><img src="/images/Higher_Math_of_PengTitus/suanjibudengshi.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/suanjibudengshi01.HEIC" alt="issue"><br>算几不等式在n=4，n=8的2的n次方都成立<br><img src="/images/Higher_Math_of_PengTitus/suanjibudengshi02.HEIC" alt="issue"></li>
</ul>
<h4 id="二项式定理Binomial-theorem"><a href="#二项式定理Binomial-theorem" class="headerlink" title="二项式定理Binomial theorem"></a>二项式定理Binomial theorem</h4><p>根据杨辉三角可以推算二项式定理<br><img src="/images/Higher_Math_of_PengTitus/erxiangshidingli01.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/erxiangshidingli02.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/erxiangshidingli03.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/erxiangshidingli04.png" alt="issue"></p>
<h4 id="极坐标"><a href="#极坐标" class="headerlink" title="极坐标"></a>极坐标</h4><p><img src="/images/Higher_Math_of_PengTitus/jizuobiao.png" alt="issue"></p>
<h4 id="球坐标系Spherical-coordinate"><a href="#球坐标系Spherical-coordinate" class="headerlink" title="球坐标系Spherical coordinate"></a>球坐标系Spherical coordinate</h4><p>三维正交坐标系<br><img src="/images/Higher_Math_of_PengTitus/Sphericalcoordinate.png" alt="issue"><br>P^2=x^2<em>y^2</em>z^2</p>
<h4 id="Caulculus函数的奇偶性"><a href="#Caulculus函数的奇偶性" class="headerlink" title="Caulculus函数的奇偶性"></a>Caulculus函数的奇偶性</h4><p>奇函数关于原点对称；偶函数关于y轴对称；任何连续函数都可以写成奇偶函数的和<br><img src="/images/Higher_Math_of_PengTitus/jiouxing.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/jiouxing02.png" alt="issue"><br>sin是奇函数，cos是偶函数<br><img src="/images/Higher_Math_of_PengTitus/jiouxing03.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/jiouxing04.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/jiouxing05.png" alt="issue"></p>
<h4 id="反函数"><a href="#反函数" class="headerlink" title="反函数"></a>反函数</h4><p><img src="/images/Higher_Math_of_PengTitus/fanhanshu01.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/fanhanshu02.png" alt="issue"><br>为了让反函数有意义要设置定义域<br><img src="/images/Higher_Math_of_PengTitus/fanhanshu03.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/fanhanshu04.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/fanhanshu05.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/fanhanshu06.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/fanhanshu07.png" alt="issue"></p>
<h4 id="周期函数"><a href="#周期函数" class="headerlink" title="周期函数"></a>周期函数</h4><p>T取最小值，最小正周期<br><img src="/images/Higher_Math_of_PengTitus/zhouqihanshu01.png" alt="issue"><br>考虑x轴伸缩；把2π伸缩成π<br><img src="/images/Higher_Math_of_PengTitus/zhouqihanshu02.png" alt="issue"></p>
<h4 id="有界函数Bounded-function与无界函数"><a href="#有界函数Bounded-function与无界函数" class="headerlink" title="有界函数Bounded function与无界函数"></a>有界函数Bounded function与无界函数</h4><p><img src="/images/Higher_Math_of_PengTitus/Boundedfunction01.png" alt="issue"></p>
<h4 id="双曲函数Hyperbolic-function"><a href="#双曲函数Hyperbolic-function" class="headerlink" title="双曲函数Hyperbolic function"></a>双曲函数Hyperbolic function</h4><p><img src="/images/Higher_Math_of_PengTitus/Hyperbolicfunction01.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/Hyperbolicfunction02.png" alt="issue"><br><img src="/images/Higher_Math_of_PengTitus/Hyperbolicfunction03.png" alt="issue"></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/11/Higher-Math-Basis-of-HouPhD/">Higher Math Basis of HouPhD</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/11/Higher-Math-Basis-of-HouPhD/" class="archive-article-date"><time datetime="2019-08-10T18:15:14.000Z" itemprop="datePublished">August 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="求极限"><a href="#求极限" class="headerlink" title="求极限"></a>求极限</h1><hr>
<h2 id="直接带入型"><a href="#直接带入型" class="headerlink" title="直接带入型"></a>直接带入型</h2><ul>
<li>已知 $f(x)=x^{2}-\frac{3}{x} ,求\lim_{x\rightarrow 3}f(x)$<script type="math/tex; mode=display">将x=3代入f(x)=x^{2}-\frac{3}{x}\\
f(3)=3^{2}-\frac{3}{3}=8\\
\therefore \lim_{x\rightarrow 3}f(x)=8</script></li>
<li>已知$f(x)=sinx+e^{x},求\lim_{x\rightarrow \pi }f(x)$<script type="math/tex; mode=display">将x=\pi 代入f(x)=sinx\leq +e^{x}\\
f(x)=sin\pi +e\pi =e\pi \\
\therefore \lim_{x\rightarrow \pi }f(x)=e^{\pi }</script></li>
</ul>
<h2 id="frac-∞-∞-型"><a href="#frac-∞-∞-型" class="headerlink" title="$\frac{∞}{∞}$ 型"></a>$\frac{∞}{∞}$ 型</h2><ul>
<li>根据公式<script type="math/tex">\left\{\begin{matrix}
分子最高次数大于分母：\lim_{x\rightarrow ∞}f(x)=∞\\ 
分母最高次数大于分子：\lim_{x\rightarrow ∞}f(x)=0\\ 
分子分母最高次数相同：\lim_{x\rightarrow ∞}f(x)=分子分母系数化
\end{matrix}\right.</script></li>
</ul>
<h2 id="frac-0-0-型"><a href="#frac-0-0-型" class="headerlink" title="$\frac{0}{0}$ 型"></a>$\frac{0}{0}$ 型</h2><ul>
<li>如果求极限是是$\frac{0}{0}$ 那它就不是真正的极限，此时我们要让他分子分母同时求导：<br>列：<br>已知：$f(x)=\frac{\sin x}{x},求\lim _{x \rightarrow 0} f(x)$<script type="math/tex; mode=display">
\lim _{x \rightarrow 0} \frac{\sin x}{x}=\lim _{x \rightarrow 0} \frac{(\sin x)^{\prime}}{(x)^{\prime}}=\lim _{x \rightarrow 0} \frac{\cos x}{1}=\frac{\cos 0}{1}=\frac{1}{1}=1</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 0} \frac{4 x}{e^{x}-1}=\lim _{x \rightarrow 0} \frac{(4 x)^{\prime}}{\left(e^{x}-1\right)^{\prime}}=\lim _{x \rightarrow 0} \frac{4}{e^{x}}=\frac{4}{e^{0}}=\frac{4}{1}=4</script></li>
<li>当一次求导后结果仍然为0/0时，需要再次求导<script type="math/tex; mode=display">
\lim _{x \rightarrow 0} \frac{1-\cos x}{x^{2}}=\lim _{x \rightarrow 0} \frac{(1-\cos x)^{\prime}}{\left(x^{2}\right)^{\prime}}=\lim _{x \rightarrow 0} \frac{\sin x}{2 x}=\frac{\sin 0}{2 \cdot 0}=\frac{0}{0}</script><script type="math/tex; mode=display">
=\lim _{x \rightarrow 0} \frac{(\sin x)^{\prime}}{(2 x)^{\prime}}=\lim _{x \rightarrow 0} \frac{\cos x}{2}=\frac{\cos 0}{2}=\frac{1}{2}</script></li>
</ul>
<h2 id="底数和指数都有x时"><a href="#底数和指数都有x时" class="headerlink" title="底数和指数都有x时"></a>底数和指数都有x时</h2><ul>
<li>当 底数是”1+x“的形式 &amp;&amp; a与指数互为倒数 &amp;&amp; a趋近于零时，$(1+a)^{\frac{1}{a}}$可用e代替<br>列：<script type="math/tex; mode=display">
\lim _{x \rightarrow 0}(1+x)^{\frac{1}{x}}=\lim _{x \rightarrow 0} e=e</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 2}\left(1+\frac{x-2}{3}\right)^{\frac{3}{x-2}}=\lim _{x \rightarrow 2} e=e</script><script type="math/tex; mode=display">
\begin{aligned} \lim _{x \rightarrow 0}(1+x)^{\frac{2}{x}} &=\lim _{x \rightarrow 0}\left[(1+x)^{\frac{1}{x}}\right]^{2} \\ &=\lim _{x \rightarrow 0}[e]^{2} \\ &=\lim _{x \rightarrow 0} e^{2} \\ &=e^{2} \end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned} \lim _{x \rightarrow 0}(1+x)^{\frac{1}{x}+2} &=\lim _{x \rightarrow 0}(1+x)^{\frac{1}{x}} \cdot(1+x)^{2} \\ &=\lim _{x \rightarrow 0} e \cdot(1+x)^{2} \\ &=e \cdot(1+0)^{2}=\mathrm{e} \end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned} \lim _{x \rightarrow 2}\left(\frac{x+1}{3}\right)^{\frac{3}{x-2}} &=\lim _{x \rightarrow 2}\left(1+\frac{x-2}{3}\right)^{\frac{3}{x-2}} \\ &=\lim _{x \rightarrow 2} e \\ &=\mathrm{e} \end{aligned}</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 2}\left(\frac{x+1}{3}\right)^{\frac{3}{x-2}+2}, \lim _{x \rightarrow 2}\left(\frac{x+1}{3}\right)^{\frac{2 x-1}{x-2}}</script></li>
</ul>
<hr>
<h1 id="函数的连续"><a href="#函数的连续" class="headerlink" title="函数的连续"></a>函数的连续</h1><hr>
<h2 id="求左极限，右极限"><a href="#求左极限，右极限" class="headerlink" title="求左极限，右极限"></a>求左极限，右极限</h2><p>列:</p>
<script type="math/tex; mode=display">
求
f(x)=\left\{\begin{array}{ll}{x-2,} & {x<0} \\ {x} & {, x \geq 0}\end{array}\right.
在x=0处的左极限和右极限</script><p>解：</p>
<script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{-}} f(x)=-2</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{+}} f(x)=0</script><p>列2：<br>$ 求f(x)=\frac{1}{x}在x=0处的左极限\lim <em>{x \rightarrow 0^{-}} f(x)和右极限\lim </em>{x \rightarrow 0^{+}} f(x)$</p>
<script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{-}} f(x)=-\infty</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{+}} f(x)=+\infty</script><p>列3：<br>$求f(x)=\frac{1+2^{\frac{1}{x}}}{2+2^{\frac{1}{x}}}在x=0时的左极限和右极限$</p>
<script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{-}} \frac{1}{x}=-\infty \quad \lim _{x \rightarrow 0^{+}} \frac{1}{x}=+\infty</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{-}} f(x)=\frac{1+2^{-\infty}}{2+2^{-\infty}} \rightarrow \frac{1+0}{2+0}=\frac{1}{2}</script><script type="math/tex; mode=display">
\lim _{x \rightarrow 0^{+}} f(x)=\frac{1+2^{+\infty}}{2+2^{+\infty}} \rightarrow \frac{2^{+\infty}}{2^{+\infty}}=1</script><h2 id="判断函数是否连续"><a href="#判断函数是否连续" class="headerlink" title="判断函数是否连续"></a>判断函数是否连续</h2><p>列：</p>
<script type="math/tex; mode=display">判断</script><p>f(x)=\left{\begin{array}{ll}{x-2,} &amp; {x&lt;0} \ {x} &amp; {, x \geq 0}\end{array}\right.<br>在x=0处是否连续$$</p>
<ul>
<li>求左极限，右极限，函数值，因为3个数不相等所以不连续<script type="math/tex; mode=display">
\begin{array}{l}{\lim _{x \rightarrow 0^{-}} f(x)=-2} \\ {\lim _{x \rightarrow 0^{+}} f(x)=0} \\ {f(0)=0}\end{array}</script></li>
</ul>
<h2 id="已知函数连续求未知参数"><a href="#已知函数连续求未知参数" class="headerlink" title="已知函数连续求未知参数"></a>已知函数连续求未知参数</h2><ul>
<li>令几个函数的x都等于间断点，求y，然后再把y值代回去求ab<script type="math/tex; mode=display">已知
f(x)=\left\{\begin{array}{ll}{x^{2}+a,} & {x<0} \\ {b} & {, x=0} \\ {x+1,} & {x>0}\end{array}\right.
连续，求a，b。</script></li>
<li>找到间断点x=0，带入求y<script type="math/tex; mode=display">
\begin{array}{l}{\lim _{x \rightarrow 0^{-}} f(x)=a \\ {\lim _{x \rightarrow 0^{+}} f(x)=1} \\ {f(0)=b}\end{array}</script></li>
<li>把求出的y值带入原方程<script type="math/tex; mode=display">
a=1=b \Rightarrow\left\{\begin{array}{l}{a=1} \\ {b=1}\end{array}\right.</script></li>
</ul>
<hr>
<h1 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h1><hr>
<h2 id="相加，相乘，嵌套（连式法则）"><a href="#相加，相乘，嵌套（连式法则）" class="headerlink" title="相加，相乘，嵌套（连式法则）"></a>相加，相乘，嵌套（连式法则）</h2><script type="math/tex; mode=display">
y^{\prime}=\left(x^{3}-2 x^{2}+\sin x\right)^{\prime}=\left(x^{3}\right)^{\prime}-\left(2 x^{2}\right)^{\prime}+(\sin x)^{\prime}=3 x^{2}-4 x+\cos x</script><script type="math/tex; mode=display">
f^{\prime}(x)=(\sin x \cdot \ln x)^{\prime}=(\sin x)^{\prime} \cdot \ln x+\sin x \cdot(\ln x)^{\prime}=\cos x \cdot \ln x+\sin x \cdot \frac{1}{x}</script><script type="math/tex; mode=display">
\frac{d y}{d x}=[\ln (\sin x)]^{\prime}=\frac{1}{\sin x} \cdot(\sin x)^{\prime}=\frac{\cos x}{\sin x}</script><ul>
<li>带y的函数求导等于对y求导乘以y‘<br>列1：<br>$已知y=x^{2},求\left(y^{2}\right)^{\prime}，根据：\left(y^{2}\right)^{\prime}=2 y \cdot y^{\prime}$<script type="math/tex; mode=display">
\begin{aligned}\left(y^{2}\right)^{\prime} &=2 y \cdot y^{\prime} \\ &=2 x^{2} \cdot\left(x^{2}\right)^{\prime} \\ &=2 x^{2} \cdot 2 x \\ &=4 x^{3} \end{aligned}</script>列2：<br>$已知y=2x，求(\sin y)^{\prime}$</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}(\text { siny })^{\prime} &=\cos y \cdot y^{\prime} \\ &=\cos 2 x \cdot(2 x)^{\prime} \\ &=2 \cos 2 x \end{aligned}</script><h2 id="隐函数求导"><a href="#隐函数求导" class="headerlink" title="隐函数求导"></a>隐函数求导</h2><ul>
<li>对等号两边同时求导<br>已知函数y=y(x)且ylny=xlinx,求y的导数<br>$$<br>(y \ln y)^{\prime}=(x \ln x)^{\prime}\<br>\begin{array}{l}{y^{\prime} \cdot \ln y+y \cdot(\ln y)^{\prime}=x^{\prime} \cdot \ln x+x \cdot(\ln x)^{\prime}} \ {y^{\prime} \cdot \ln y+y \cdot \frac{1}{y} \cdot y^{\prime}=1 \cdot \ln x+x \cdot \frac{1}{x}}\end{array}\</li>
</ul>
<script type="math/tex; mode=display">
{y}'\cdot lny+y\cdot \frac{1}{y}\cdot {y}'=lnx+1\\
{y}'(1+lny)=lnx+1\\
y'=\frac{lnx+1}{lny+1}</script><h2 id="底数指数均有未知数求导"><a href="#底数指数均有未知数求导" class="headerlink" title="底数指数均有未知数求导"></a>底数指数均有未知数求导</h2><ul>
<li>$lna^{b}=blna$<br>$已知函数y=f(x)且y^{x}=x^{y}，求{y}’$</li>
<li>等式两边同时取对数$lny^{x}=lnx^{y}\<br>ylny=xlnx$</li>
<li>利用隐函数的求导方式求导</li>
</ul>
<h2 id="参数方程求导"><a href="#参数方程求导" class="headerlink" title="参数方程求导"></a>参数方程求导</h2><script type="math/tex; mode=display">求曲线\left\{\begin{matrix}
x=t-sint\\ 
y=1-cost
\end{matrix}\right.的导数\frac{dy}{dx}</script><ul>
<li>1, x对t求导:$\frac{dx}{dt}=(t-sint)’_{t}=1-cost$</li>
<li>2, y对t求导:$\frac{dy}{dt}=(1-cont)’_{t}=sint$</li>
<li>3, $\frac{dy}{dx}=\frac{sint}{1-cost}$</li>
</ul>
<h2 id="已知连续可导求未知参数"><a href="#已知连续可导求未知参数" class="headerlink" title="已知连续可导求未知参数"></a>已知连续可导求未知参数</h2><p>例：</p>
<script type="math/tex; mode=display">求a,b的值，使函数f(x)=\left\{\begin{matrix}
ax+b, & x<1\\ 
\frac{1}{2}(a+b+1), & x=1\\ 
x^{2}, & x>1
\end{matrix}\right.在任意点连续可导</script><ul>
<li>1,找出间断点x=1</li>
<li>2,将间断点带入函数表达式</li>
<li>3,求x&lt;间断点，与x&gt;间断点时的导数x&lt;1时，f(x)=ax+b,f’=a;x&gt;1时，f(x)=x^2,f’(x)=2x</li>
<li>4,将间断点带入导数表达式</li>
<li>5,令2的各个结果相等和4的各个结果相等求a和b<script type="math/tex; mode=display">\left\{\begin{matrix}
a+b=\frac{1}{2}(a+b+1)\\ 
a=2
\end{matrix}\right.\Rightarrow \left\{\begin{matrix}
a=2\\ 
b=1
\end{matrix}\right.</script></li>
</ul>
<h2 id="已知导数值求极限"><a href="#已知导数值求极限" class="headerlink" title="已知导数值求极限"></a>已知导数值求极限</h2><ul>
<li>根据公式：$\lim<em>{h\rightarrow 0}\frac{f(x</em>{0}+h)-f(x<em>{0})}{h}=f’(x</em>{0})$</li>
</ul>
<h2 id="罗尔中值定理"><a href="#罗尔中值定理" class="headerlink" title="罗尔中值定理"></a>罗尔中值定理</h2><p>例：</p>
<ul>
<li><p>$设f(x)在(a,b)上连续可导，证明存在一点\xi \varepsilon (a,b),满足\frac{bf(b)-af(a)}{b-a}=f(\xi )+\xi f’(\xi )$</p>
</li>
<li><p>1，用x替代问号中的未知数，并把等号右边化成0</p>
<script type="math/tex; mode=display">
\frac{bf(b)-af(a)}{b-a}=f(x)+xf'(x)\Rightarrow f(x)+xf'(x)-\frac{bf(b)-af(a)}{b-a}=0</script></li>
<li><p>2，找出f(x)与f’(x)前的次方数:f(x)前的x的次方数是0；f’(x)前的x的次方数是1</p>
</li>
<li>3，判断属于表中哪种类型，并根据化简目标化简<br>特点 | 化简目标 | F’(x) | F(x)<br>-: | :-: | :-: | :-:<br>无f(x) | f’(x)+C | f’(x)+C | f’(x)+Cx |<br>f(x)与f‘(x)前x的次方数相同 | f(x)+f’(x)+C| $e^{x}[f(x)+f’(x)+C]$ | $e^{x}f(x)+Ce^{x}$ |<br>f(x)与f‘(x)前x的次方数相同 | f(x)-f’(x)+C| $-e^{-x}[f(x)-f’(x)+C]$ | $e^{-x}f(x)+Ce^{-x}$ |<br>f(x)与f‘(x)前x的次方数不同 | af(x)+xf’(x)+C| $x^{a-1}[af(x)+xf’(x)+C]$ | $x^{a}f(x)+\frac{C}{a}x^{a}$ |</li>
<li>4，根据表写出F’(x)与F’(x)<script type="math/tex; mode=display">
F'(x)=x^{1-1}[1f(x)+xf'(x)-\frac{bf(b)-af(a)}{b-a}]=f(x)+xf'(x)-\frac{bf(b)-af(a)}{b-a}\\
F(x)=x'(x)+\frac{-\frac{bf(b)-af(a)}{b-a}}{1}\cdot x^{1}=xf(x)-\frac{bf(b)-af(a)}{b-a}\cdot x</script></li>
<li><p>5，求出F(x)在区间内两个端点的值</p>
<script type="math/tex; mode=display">
F(a)=af(a)-\frac{bf(b)-af(a)}{b-a}\cdot a=-\frac{ab[bf(b)-af(a)]}{b-a}\\
F(b)=bf(b)-\frac{bf(b)-af(a)}{b-a}\cdot b=-\frac{ab[bf(b)-af(a)]}{b-a}\\
F(a)=F(b)</script></li>
<li><p>6，$F(x)在(a,b)上满足罗尔中值定理，则至少有一点\xi \epsilon (a,b)满足F’(x)=f(x)+xf’(x)-\frac{ab[bf(b)-af(a)]}{b-a}=0,\<br>可推出\frac{ab[bf(b)-af(a)]}{b-a}=f(\xi )+\xi f’(\xi ),原等式得证$</p>
</li>
</ul>
<h2 id="拉格朗日中值定理"><a href="#拉格朗日中值定理" class="headerlink" title="拉格朗日中值定理"></a>拉格朗日中值定理</h2><p>例：<br>$利用拉格朗日中值定理证明不等式-2\leq \frac{sinx<em>{2}-sinx</em>{1}}{x<em>{2}-x</em>{1}}\leq 2(x2&gt;x1)$</p>
<ul>
<li>1, 找出f(x)<br>f(x)=sinx</li>
<li>2, 求f’(x)<br>f’(x)=(sinx)’=cosx</li>
<li>3, 求出在(x<em>{1},x</em>{2})内f’(x)的取值范围<br>根据f’(x)的函数图像可判断$-1\leq cosx \leq 1$</li>
<li>得出$\frac{sinx<em>{2}-sinx</em>{1}}{x<em>{2}-x</em>{1}}$的值属于f’(x)的取值范围<script type="math/tex; mode=display">
-1 \leq \frac{sinx_{2}-sinx_{1}}{x_{2}-x_{1}} \leq 1\\
\therefore -2 \leq \frac{sinx_{2}-sinx_{1}}{x_{2}-x_{1}} \leq 2</script></li>
</ul>
<h2 id="求极值与最值"><a href="#求极值与最值" class="headerlink" title="求极值与最值"></a>求极值与最值</h2><p>例1：<br>$求函数f(x)f(x)=4x^{3}-12x^{2}+9x的极大值，极小值及在[0,1.5]内最大值$</p>
<ul>
<li>1, 求导，为了证明此函数有没有导数为零的时候，也就是有最高点与最低点<script type="math/tex; mode=display">
f'(x)=(4x^{3}-12x^{2}+9x)'\\
=12x^{2}-24x+9\\
=3(2x-1)(2x-3)</script></li>
<li>2，有两点0.5和1.5，根据这个画出原函数图像，带入原方程x=0.5时最高点为2</li>
</ul>
<p>例2：<br>有一块边长为3的正方形铁片，在每个角上各剪去一个边长为x的小正方形，用剩下的部分做成开口盒子，当剪去小正方形的边长x为多大时，盒子的容积最大？</p>
<ul>
<li>分析：先计算出方程的分析试，根据这个分析试猜测函数图像看看函数图像应该在哪个点之间有最大值和最小值，然后求导算出这个函数的导数图像（分析导数图像再次确定这两个点之间的最高点与最低点为），把导数为0，也就是斜率为零的两个点带入原方程求出y的容积</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/10/Data-Preprocessing-by-Python/">Data Preprocessing by Python</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/10/Data-Preprocessing-by-Python/" class="archive-article-date"><time datetime="2019-08-10T11:08:27.000Z" itemprop="datePublished">August 10th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><script src="https://gist.github.com/SauronLee/d3f9b166dbfa5160de5411a0718c90b1.js"></script>

<p>command + i = helpeer</p>
<ul>
<li>Definition : Imputer(missing_values=”NaN”, strategy=”mean”, axis=0, verbose=0, copy=True)</li>
</ul>
<p>missing_values : integer or “NaN”, optional (default=”NaN”)<br>The placeholder for the missing values. All occurrences of missing_values will be imputed. For missing values encoded as np.nan, use the string value “NaN”.<br>strategy : string, optional (default=”mean”)<br>The imputation strategy.</p>
<p>If “mean”, then replace missing values using the mean along the axis.<br>If “median”, then replace missing values using the median along the axis.<br>If “most_frequent”, then replace missing using the most frequent value along the axis.<br>axis : integer, optional (default=0)<br>The axis along which to impute.</p>
<p>If axis=0, then impute along columns.<br>If axis=1, then impute along rows.<br>verbose : integer, optional (default=0)<br>Controls the verbosity of the imputer.<br>copy : boolean, optional (default=True)<br>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if copy=False:</p>
<p>If X is not an array of floating values;<br>If X is sparse and missing_values=0;<br>If axis=0 and X is encoded as a CSR matrix;<br>If axis=1 and X is encoded as a CSC matrix.</p>
<ul>
<li>fit(X, y=None)</li>
</ul>
<p>X : {array-like, sparse matrix}, shape (n_samples, n_features)<br>Input data, where n_samples is the number of samples and n_features is the number of features.</p>
<ul>
<li>Definition : OneHotEncoder(n_values=None, categorical_features=None, categories=None, sparse=True, dtype=np.float64, handle_unknown=’error’)</li>
</ul>
<p>Type : Present in sklearn.preprocessing._encoders module</p>
<p>categories : ‘auto’ or a list of lists/arrays of values, default=’auto’.<br>Categories (unique values) per feature:</p>
<p>‘auto’ : Determine categories automatically from the training data.<br>list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.<br>The used categories can be found in the categories_ attribute.</p>
<p>sparse : boolean, default=True<br>Will return sparse matrix if set True else will return an array.<br>dtype : number type, default=np.float<br>Desired dtype of output.<br>handle_unknown : ‘error’ or ‘ignore’, default=’error’.<br>Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to ‘ignore’ and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.<br>n_values : ‘auto’, int or array of ints, default=’auto’<br>Number of values per feature.</p>
<p>‘auto’ : determine value range from training data.</p>
<p>int : number of categorical values per feature.<br>Each feature value should be in range(n_values)</p>
<p>array : n_values[i] is the number of categorical values in<br>X[:, i]. Each feature value should be in range(n_values[i])</p>
<p>Deprecated since version 0.20: The n_values keyword was deprecated in version 0.20 and will be removed in 0.22. Use categories instead.</p>
<p>categorical_features : ‘all’ or array of indices or mask, default=’all’<br>Specify what features are treated as categorical.</p>
<p>‘all’: All features are treated as categorical.<br>array of indices: Array of categorical feature indices.<br>mask: Array of length n_features and with dtype=bool.<br>Non-categorical features are always stacked to the right of the matrix.</p>
<p>Deprecated since version 0.20: The categorical_features keyword was deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.</p>
<h2 id="特征缩放-Feature-Scaling"><a href="#特征缩放-Feature-Scaling" class="headerlink" title="特征缩放 Feature Scaling"></a>特征缩放 Feature Scaling</h2><ul>
<li>因为两个特征向量之间的差异可以用欧氏距离来表示（勾股定理），如果任意一个特征向量过于巨大那么距离大差异就会更偏向特征值巨大的量，甚至完全取决于这个量，于是就需要缩放到一个数量级上</li>
<li>特征缩放 Feature Scaling后某些模型收敛会快很多，比如决策树算法（枝干过长）</li>
<li><p>特征缩放方法：<br><img src="/images/A-Z_ML/01.png" alt></p>
</li>
<li><p>是否需要对虚拟变量进行特征缩放。答：不一定，要根据模型结果。</p>
</li>
<li>是否需要对因变量y进行特征缩放？答：不一定，如果y为分类则不需要，如果y为连续则需要（线性回归）。</li>
</ul>
<pre><code># Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_XStandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
</code></pre>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/10/Data-Preprocessing-by-R/">Data Preprocessing by R</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/10/Data-Preprocessing-by-R/" class="archive-article-date"><time datetime="2019-08-10T11:08:09.000Z" itemprop="datePublished">August 10th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><script src="https://gist.github.com/SauronLee/d07b84f9caefd89dbc3f76a943bd8166.js"></script>

<ul>
<li>因子数据类型（有序因子&amp;无序因子），可以代替Python中的OneHotEncoder</li>
</ul>
<p>Usage<br>factor(x = character(), levels, labels = levels,<br>       exclude = NA, ordered = is.ordered(x), nmax = NA)</p>
<p>ordered(x, …)</p>
<p>is.factor(x)<br>is.ordered(x)</p>
<p>as.factor(x)<br>as.ordered(x)</p>
<p>addNA(x, ifany = FALSE)<br>Arguments<br>x<br>a vector of data, usually taking a small number of distinct values.</p>
<p>levels<br>an optional vector of the unique values (as character strings) that x might have taken. The default is the unique set of values taken by as.character(x), sorted into increasing order of x. Note that this set can be specified as smaller than sort(unique(x)).</p>
<p>labels<br>either an optional character vector of labels for the levels (in the same order as levels after removing those in exclude), or a character string of length 1. Duplicated values in labels can be used to map different values of x to the same factor level.</p>
<p>exclude<br>a vector of values to be excluded when forming the set of levels. This may be factor with the same level set as x or should be a character.</p>
<p>ordered<br>logical flag to determine if the levels should be regarded as ordered (in the order given).</p>
<p>nmax<br>an upper bound on the number of levels; see ‘Details’.</p>
<p>…<br>(in ordered(.)): any of the above, apart from ordered itself.</p>
<p>ifany<br>only add an NA level if it is used, i.e. if any(is.na(x)).</p>
<h2 id="Splitting-the-Dataset-into-the-Training-set-and-Test-set"><a href="#Splitting-the-Dataset-into-the-Training-set-and-Test-set" class="headerlink" title="Splitting the Dataset into the Training set and Test set"></a>Splitting the Dataset into the Training set and Test set</h2><pre><code># Data Preprocessing Template

# Importing the dataset
dataset = read.csv(&#39;Data.csv&#39;)
# dataset = dataset[, 2:3]

# Splitting the dataset into the Training set and Test set
# R语言安装packages
# install.packages(&#39;caTools&#39;)
# library(caTools) = 在packages里面打钩，推荐用library(caTools) 
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# split == TRUE为random出来的数据
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
# training_set[, 2:3] = scale(training_set[, 2:3])
# test_set[, 2:3] = scale(test_set[, 2:3])
</code></pre><ul>
<li>set.seed<br>Random Number Generation</li>
</ul>
<p>Usage<br>.Random.seed &lt;- c(rng.kind, n1, n2, \dots)</p>
<p>RNGkind(kind = NULL, normal.kind = NULL, sample.kind = NULL)<br>RNGversion(vstr)<br>set.seed(seed, kind = NULL, normal.kind = NULL, sample.kind = NULL)<br>Arguments<br>kind<br>character or NULL. If kind is a character string, set R’s RNG to the kind desired. Use “default” to return to the R default. See ‘Details’ for the interpretation of NULL.</p>
<p>normal.kind<br>character string or NULL. If it is a character string, set the method of Normal generation. Use “default” to return to the R default. NULL makes no change.</p>
<p>sample.kind<br>character string or NULL. If it is a character string, set the method of discrete uniform generation (used in sample, for instance). Use “default” to return to the R default. NULL makes no change.</p>
<p>seed<br>a single value, interpreted as an integer, or NULL (see ‘Details’).</p>
<p>vstr<br>a character string containing a version number, e.g., “1.6.2”. The default RNG configuration of the current R version is used if vstr is greater than the current version.</p>
<p>rng.kind<br>integer code in 0:k for the above kind.</p>
<p>n1, n2, …<br>integers. See the details for how many are required (which depends on rng.kind).</p>
<ul>
<li>sample.split<br>Split Data into Test and Train Set</li>
</ul>
<p>Usage<br> sample.split( Y, SplitRatio = 2/3, group = NULL )<br>Arguments<br>Y<br>Vector of data labels. If there are only a few labels (as is expected) than relative ratio of data in both subsets will be the same.</p>
<p>SplitRatio<br>Splitting ratio:</p>
<p>if (0&lt;=SplitRatio&lt;1) then SplitRatio fraction of points from Y will be set toTRUE</p>
<p>if (SplitRatio==1) then one random point from Y will be set to TRUE</p>
<p>if (SplitRatio&gt;1) then SplitRatio number of points from Y will be set to TRUE</p>
<p>group<br>Optional vector/list used when multiple copies of each sample are present. In such a case group contains unique sample labels, marking all copies of the same sample with the same label, and the function tries to place all copies in either train or test subset. If provided than has to have the same length as Y.</p>
<h1 id="特征缩放-Feature-Scaling"><a href="#特征缩放-Feature-Scaling" class="headerlink" title="特征缩放 Feature Scaling"></a>特征缩放 Feature Scaling</h1><ul>
<li>因为两个特征向量之间的差异可以用欧氏距离来表示（勾股定理），如果任意一个特征向量过于巨大那么距离大差异就会更偏向特征值巨大的量，甚至完全取决于这个量，于是就需要缩放到一个数量级上</li>
<li>特征缩放 Feature Scaling后某些模型收敛会快很多，比如决策树算法（枝干过长）</li>
<li><p>特征缩放方法：<br><img src="/images/A-Z_ML/01.png" alt></p>
</li>
<li><p>是否需要对虚拟变量进行特征缩放？答：不一定，要根据模型结果。</p>
</li>
<li>是否需要对因变量y进行特征缩放？答：不一定，如果y为分类则不需要，如果y为连续则需要（线性回归）。</li>
<li>在R里factor属于单独的类型不属于数字它属于分类的一个数据类型</li>
</ul>
<pre><code># Feature Scaling
training_set[,2:3] = scale(training_set[,2:3])
test_set[,2:3] = scale(test_set[,2:3])
</code></pre>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/09/quadratic-form-of-houPhD/">quadratic form of houPhD</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/09/quadratic-form-of-houPhD/" class="archive-article-date"><time datetime="2019-08-09T08:07:56.000Z" itemprop="datePublished">August 9th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="判断某向量是否可以由某向量组线性表示"><a href="#判断某向量是否可以由某向量组线性表示" class="headerlink" title="判断某向量是否可以由某向量组线性表示"></a>判断某向量是否可以由某向量组线性表示</h1><ul>
<li>判断将其向量加入矩阵和矩阵原先的秩是否有变化，如果没变化就代表可以<br>列：<script type="math/tex; mode=display">已知 a_{1}=\begin{pmatrix}
1\\ 
1\\ 
2\\ 
2
\end{pmatrix},a_{2}=\begin{pmatrix}
1\\ 
2\\ 
1\\ 
3
\end{pmatrix},a_{3}=\begin{pmatrix}
1\\ 
-2\\ 
4\\ 
0
\end{pmatrix},b=\begin{pmatrix}
1\\ 
0\\ 
3\\ 
1
\end{pmatrix},试判断b能否由 a_{1},a_{2},a_{3}线性表示</script></li>
</ul>
<script type="math/tex; mode=display">A=(a_{1},a_{2},a_{3})=\begin{pmatrix}
1 & 1 & 1\\ 
1 & 2 & -2\\ 
2 & 1 & 4\\ 
2 & 3 & 0
\end{pmatrix}=\begin{pmatrix}
1 & 1 & 1\\ 
0 & 1 & -3\\ 
0 & 0 & -1\\ 
0 & 0 & 0
\end{pmatrix}\Rightarrow R(A)=3</script><script type="math/tex; mode=display">B=(a_{1},a_{2},a_{3},b)=\begin{pmatrix}
1 & 1 & 1&1\\ 
1 & 2 & -2&0\\ 
2 & 1 & 4&3\\ 
2 & 3 & 0&1
\end{pmatrix}=\begin{pmatrix}
1 & 1 & 1&1\\ 
0 & 1 & -3&-1\\ 
0 & 0 & -1&0\\ 
0 & 0 & 0&0
\end{pmatrix}\Rightarrow R(B)=3</script><script type="math/tex; mode=display">\therefore R(A)=R(B)\\
\therefore b能由 a_{1},a_{2},a_{3}线性表示即：b=k_{1}a_{1}+k_{2}a_{2}+k_{3}a_{3}</script><h1 id="判断是否线性相关"><a href="#判断是否线性相关" class="headerlink" title="判断是否线性相关"></a>判断是否线性相关</h1><ul>
<li>看看秩小不小于矩阵维度</li>
</ul>
<h1 id="根据向量空间的一组基底，求某一向量在此基底上的坐标"><a href="#根据向量空间的一组基底，求某一向量在此基底上的坐标" class="headerlink" title="根据向量空间的一组基底，求某一向量在此基底上的坐标"></a>根据向量空间的一组基底，求某一向量在此基底上的坐标</h1><ul>
<li>说白了就是求交线 公式：$ \beta =k<em>{1}\alpha </em>{1}+k<em>{2}\alpha </em>{2}+k<em>{3}\alpha </em>{3} $<br>列：<br>$ 设三维向量空间的一组基底\alpha <em>{1}=(1,1,0),\alpha </em>{2}=(1,0,1),\alpha _{3}=(0,1,1) \ 求向量\beta =(2,0,0)在此基底下的坐标 $</li>
</ul>
<script type="math/tex; mode=display">\left\{\begin{matrix}
2=k_{1}+k_{2}+0\\ 
0=k_{1}+0+k_{3}\\ 
0=0+k_{2}+k_{3}
\end{matrix}\right.\Rightarrow \left\{\begin{matrix}
k_{1}=1\\ 
k_{2}=1\\ 
k_{3}=-1
\end{matrix}\right.\therefore 坐标为(k_{1}+k_{2}+k_{3})=(1,1,-1)</script><h1 id="求几个行向量的极大无关组"><a href="#求几个行向量的极大无关组" class="headerlink" title="求几个行向量的极大无关组"></a>求几个行向量的极大无关组</h1><ul>
<li>也就是求这些向量组合起来后，线性无关的有哪几组，求秩可得</li>
</ul>
<h1 id="判断方程组解得情况"><a href="#判断方程组解得情况" class="headerlink" title="判断方程组解得情况"></a>判断方程组解得情况</h1><ul>
<li>求矩阵的秩，是齐次且线性无关则有一个零解，是齐次线性相关多个解和零解，</li>
<li>非齐次时，也是求秩，比较R(A)和R(A|b)的秩,如果不相等则无解，如果相等再看看跟矩阵有没有降维如果没降维就有一个非零解，如果R(A)和R(A|b)的秩相等但是降维了则有多个非零解<script type="math/tex; mode=display">\left\{\begin{matrix}
x_{1}+x_{2}+x_{4}=1\\ 
x_{2}+x_{3}+x_{4}=2\\ 
x_{1}+x_{3}+x_{4}=3\\ 
x_{1}+x_{2}+x_{3}+2x_{4}=1
\end{matrix}\right.\\ \\
A=\begin{pmatrix}
1 & 1 & 0 & 1\\ 
0 & 1 & 0 & 1\\ 
1 & 0 & 1 & 1\\ 
1 & 1 & 1 & 2
\end{pmatrix}\rightarrow \begin{pmatrix}
1 & 1 & 0 & 1\\ 
0 & 1 & 0 & 1\\ 
0 & 0 & 1 & 1\\ 
0 & 0 & 0 & 0
\end{pmatrix}\\ \\
(A|b)=\begin{pmatrix}
1 & 1 & 0 & 1 & | & 1\\ 
0 & 1 & 0 & 1 & | & 2\\ 
1 & 0 & 1 & 1 & | & 3\\ 
1 & 1 & 1 & 2 & | & 4
\end{pmatrix}\rightarrow \begin{pmatrix}
1 & 1 & 0 & 1 & | & 1\\ 
0 & 1 & 0 & 1 & | & 2\\ 
0 & 0 & 1 & 1 & | & 4\\ 
0 & 0 & 0 & 0 & | & -1
\end{pmatrix} \\ 
R(A)=3\neq R(A|b)=4 \therefore 该方程组无解</script></li>
</ul>
<h1 id="解非齐次方程组"><a href="#解非齐次方程组" class="headerlink" title="解非齐次方程组"></a>解非齐次方程组</h1><p>列：</p>
<script type="math/tex; mode=display">\left\{\begin{matrix}
x_{1}+x_{2}+x_{4}=1\\ 
x_{2}+x_{4}=2\\ 
x_{1}+x_{3}+x_{4}=3\\ 
x_{1}+x_{2}+x_{3}+2x_{4}=5
\end{matrix}\right.</script><ul>
<li><p>求对角线的数就是方程的解，为简化计算我们先找出线性相关剔除，求R(A|b) </p>
<script type="math/tex; mode=display">(A|b)=\begin{pmatrix}
1 & 1 & 0 & 1 & | & 1\\ 
0 & 1 & 0 & 1 & | & 2\\ 
1 & 0 & 1 & 1 & | & 3\\ 
1 & 1 & 1 & 2 & | & 5
\end{pmatrix}\rightarrow \begin{pmatrix}
1 & 1 & 0 & 1 & | & 1\\ 
0 & 1 & 0 & 1 & | & 2\\ 
0 & 0 & 1 & 1 & | & 4\\ 
0 & 0 & 0 & 0 & | & 0
\end{pmatrix}</script></li>
<li><p>找出一个先行相关，剔除，然后计算其余向量的解（对角线）</p>
</li>
</ul>
<script type="math/tex; mode=display">\begin{pmatrix}
1 & 0 & 0 & 0 & | & -1\\ 
0 & 1 & 0 & 1 & | & 2\\ 
0 & 0 & 1 & 1 & | & 4\\ 
0 & 0 & 0 & 0 & | & 0
\end{pmatrix}</script><ul>
<li><p>如果有线性相关项则把线性相关项设置成未知数k(切记一定先化方阵)，也就是变成标准型</p>
<script type="math/tex; mode=display">\left\{\begin{matrix}
x_{1}=-1\\ 
x_{2}+x_{4}=2\\ 
x_{3}+x_{4}=4\\ 
x_{4}=x_{4}
\end{matrix}\right.\Rightarrow \left\{\begin{matrix}
x_{1}=-1\\ 
x_{2}=2-k\\ 
x_{3}=4-k\\ 
x_{4}=k
\end{matrix}\right. \Rightarrow \left\{\begin{matrix}
x_{1}=-1+0k\\ 
x_{2}=2-1k\\ 
x_{3}=4-1k\\ 
x_{4}=0+1k
\end{matrix}\right.</script></li>
<li><p>结果(最后这个k因为是线性相关所以可以取任意值)-通解</p>
<script type="math/tex; mode=display">解为 \begin{pmatrix}
-1\\ 
2\\ 
4\\ 
0
\end{pmatrix}+k\begin{pmatrix}
0\\ 
-1\\ 
-1\\ 
1
\end{pmatrix}</script></li>
</ul>
<h1 id="解齐次方程组"><a href="#解齐次方程组" class="headerlink" title="解齐次方程组"></a>解齐次方程组</h1><p>常数项不写，标准型时等于0即可</p>
<h1 id="通解，特解和基础解析"><a href="#通解，特解和基础解析" class="headerlink" title="通解，特解和基础解析"></a>通解，特解和基础解析</h1><ul>
<li><p>通解 </p>
<script type="math/tex; mode=display">\begin{pmatrix}
-1\\ 
2\\ 
4\\ 
0
\end{pmatrix}+k\begin{pmatrix}
0\\ 
-1\\ 
-1\\ 
1
\end{pmatrix}</script></li>
<li><p>特解 k随便取个数带入求出一个解即可，通常把k=0</p>
</li>
<li><p>基础解析</p>
<script type="math/tex; mode=display">\eta \begin{pmatrix}
0\\ 
-1\\ 
-1\\ 
1
\end{pmatrix}</script></li>
<li><p>求解线性无关的解向量个数；齐次-未知数个数减秩；非齐次 未知数个数减秩+1</p>
</li>
</ul>
<h1 id="施密特正交化"><a href="#施密特正交化" class="headerlink" title="施密特正交化"></a>施密特正交化</h1><ul>
<li>施密特正交化是把一组基底转化成单位正交向量的做法，内积是为了去掉两个向量不正交的部分（比如特征向量是一组基底，但是他们并不一定正交，正交要求比线性无关高）</li>
<li>规范正交化就是正交化+归一化，就是变成规范基向量<br>公式：<script type="math/tex; mode=display">b_{1}=a_{1}\\
b_{2}=a_{2}-\frac{[b_{1},a_{2}]}{[b_{1},b_{1}]}b_{1}\\
b_{3}=a_{3}-\frac{[b_{1},a_{3}]}{[b_{1},b_{1}]}b_{1}-\frac{[b_{2},a_{3}]}{[b_{2},b_{2}]}b_{2}\\
...\\
e_{1}=\frac{b_{1}}{\left \| b_{1} \right \|},e_{2}=\frac{b_{2}}{\left \| b_{2} \right \|},e_{3}=\frac{b_{3}}{\left \| b_{3} \right \|},e_{n}=\frac{b_{n}}{\left \| b_{n} \right \|}</script></li>
</ul>
<script type="math/tex; mode=display">列:试把向量 _{1}=\begin{pmatrix}
1\\ 
2\\ 
-1
\end{pmatrix},
a_{2}=\begin{pmatrix}
-1\\ 
3\\ 
1
\end{pmatrix},a_{3}=\begin{pmatrix}
4\\ 
-1\\ 
0
\end{pmatrix}, 规范正交化</script><p>解：</p>
<ul>
<li><p>先正交</p>
<script type="math/tex; mode=display">b_{1}=a_{1}=\begin{pmatrix}
1\\ 
2\\ 
-1
\end{pmatrix}\\
b_{2}=a_{2}-\frac{[b_{1},a_{2}]}{[b_{1},b_{1}]}b_{1}=\begin{pmatrix}
-1\\ 
3\\ 
1
\end{pmatrix}-\frac{4}{6}\begin{pmatrix}
1\\ 
2\\ 
-1
\end{pmatrix}=\frac{5}{3}\begin{pmatrix}
-1\\ 
1\\ 
1
\end{pmatrix}\\
b_{3}=a_{3}-\frac{[b_{1},a_{3}]}{[b_{1},b_{1}]}b_{1}-\frac{[b_{2},a_{3}]}{[b_{2},b_{2}]}b_{2}=\begin{pmatrix}
4\\ 
-1\\ 
0
\end{pmatrix}-\frac{1}{3}\begin{pmatrix}
1\\ 
2\\ 
-1
\end{pmatrix}+\frac{5}{3}\begin{pmatrix}
-1\\ 
1\\ 
1
\end{pmatrix}=2\begin{pmatrix}
1\\ 
0\\ 
1
\end{pmatrix}</script></li>
<li><p>再进行归一化</p>
<script type="math/tex; mode=display">\left \| b_{1} \right \|=\sqrt{1^{2}+2^{2}+(-1)^{2}}=\sqrt{6}\\
\left \| b_{2} \right \|=\sqrt{(-\frac{5}{3})^{2}+(\frac{5}{3})^{2}+(\frac{5}{3})^{2}}=\frac{5\sqrt{3}}{3}\\
\left \| b_{3} \right \|=\sqrt{2^{2}+0^{2}+2^{2}}=2\sqrt{2}\\
e_{1}=\frac{b_{1}}{\left \| b_{1} \right \|}=\frac{\sqrt{6}}{6}\begin{pmatrix}
1\\ 
2\\ 
-1
\end{pmatrix},e_{2}=\frac{b_{2}}{\left \| b_{2} \right \|}=\frac{\sqrt{3}}{3}\begin{pmatrix}
-1\\ 
1\\ 
1
\end{pmatrix},e_{3}=\frac{b_{3}}{\left \| b_{3} \right \|}=\frac{\sqrt{2}}{2}\begin{pmatrix}
1\\ 
0\\ 
1
\end{pmatrix}</script></li>
</ul>
<h1 id="求矩阵的特征值"><a href="#求矩阵的特征值" class="headerlink" title="求矩阵的特征值"></a>求矩阵的特征值</h1><ul>
<li>根据公式；$\left | A-\lambda E \right |=0$</li>
</ul>
<script type="math/tex; mode=display">求矩阵A=\begin{pmatrix}
-1 & 1 & 0\\ 
-4 & 3 & 0\\ 
1 & 0 & 2
\end{pmatrix}的特征值</script><p>解：</p>
<script type="math/tex; mode=display">\begin{vmatrix}
\begin{pmatrix}
-1 & 1 & 0\\ 
-4 & 3 & 0\\ 
1 & 0 & 2
\end{pmatrix}-\lambda \begin{pmatrix}
1 & 0 & 0\\ 
0 & 1 & 0\\ 
0 & 0 & 1
\end{pmatrix}
\end{vmatrix}=0 \\
\begin{vmatrix}
-1-\lambda  & 1 & 0\\ 
-4 & 3-\lambda  & 0\\ 
1 & 0 & 2-\lambda 
\end{vmatrix}=0 \\
\begin{vmatrix}
1 & 0 & 2-\lambda \\ 
0 & 1 & 2+\lambda -\lambda ^{2}\\ 
0 & 0 & (\lambda -1)^{2}\cdot (2-\lambda )
\end{vmatrix}=0 
1\cdot 1\cdot (\lambda -1)^{2}\cdot (2-\lambda )=0 \\
(\lambda -1)^{2}\cdot (2-\lambda )=0\\
当\lambda =2或\lambda =1时，满足要求\\
\therefore \lambda _{1}=2，\lambda _{2}=\lambda _{3}=1</script><h1 id="求矩阵特征向量"><a href="#求矩阵特征向量" class="headerlink" title="求矩阵特征向量"></a>求矩阵特征向量</h1><ul>
<li>特征向量就是立方体的轴心，而求特征向量要先求特征值，看看有几个特征值就有几个特征向量<br>列：<script type="math/tex; mode=display">求矩阵 A=\begin{pmatrix}
-1 & 1 & 0\\ 
-4 & 3 & 0\\ 
1 & 0 & 2
\end{pmatrix}的特征向量</script></li>
</ul>
<p>解：因为上文已经根据公式 $|A-\lambda E|=0$求出特征值为2和1所以特征向量就有两个</p>
<ul>
<li>当$\lambda = \lambda _{1}=2$时：<script type="math/tex; mode=display">A\cdot \lambda E=\begin{pmatrix}
-1 & 1 & 0\\ 
-4 & 3 &0 \\ 
1&  0& 2
\end{pmatrix}-2\cdot \begin{pmatrix}
1 & 0 &0 \\ 
0 & 1 & 0\\ 
0 & 0 & 1
\end{pmatrix}\\
=\begin{pmatrix}
-1 & 1 & 0\\ 
-4 & 3 &0 \\ 
1&  0& 2
\end{pmatrix}\cdot \begin{pmatrix}
2& 0 &0 \\ 
0 & 2 & 0\\ 
0 & 0 & 2
\end{pmatrix}\\
=\begin{pmatrix}
-3 & 1 &0 \\ 
-4 & 1 & 0\\ 
1 & 0 & 0
\end{pmatrix}\\
(A\cdot \lambda E)x=0\Rightarrow \begin{pmatrix}
-3 & 1 & 0\\ 
-4 & 1 & 0\\ 
1 & 0 & 0
\end{pmatrix}x=0\Rightarrow \left\{\begin{matrix}
-3x_{1}+x_{2}+0x_{3}=0\\ 
-4x_{1}+x_{2}+0x_{3}=0\\ 
x_{1}+0x_{2}+0x_{3}=0
\end{matrix}\right.\\
通解为k\begin{pmatrix}
0\\ 
0\\ 
1
\end{pmatrix}\\
\therefore \lambda =\lambda _{1}=2时，特征向量为k\begin{pmatrix}
0\\ 
0\\ 
1
\end{pmatrix}</script></li>
<li>当$\lambda = \lambda <em>{2} = \lambda </em>{3}=1$时：</li>
</ul>
<script type="math/tex; mode=display">
A\cdot \lambda E=\begin{pmatrix}
-1 & 1 & 0\\ 
-4 & 3 &0 \\ 
 1&  0& 2
\end{pmatrix}\cdot  \begin{pmatrix}
1 & 0 &0 \\ 
0 & 1 & 0\\ 
0 & 0 & 1
\end{pmatrix}\\
=\begin{pmatrix}
-2 & 1 & 0\\ 
-4 & 2 & 0\\ 
1 & 0 & 1
\end{pmatrix}\\
(A\cdot \lambdaE)x=0\Rightarrow \begin{pmatrix}
-2 & 1 & 0\\ 
-4 & 2 & 0\\ 
1 & 0 & 1
\end{pmatrix}x=0\Rightarrow \left\{\begin{matrix}
-2x_{1}+x_{2}+0x_{3}=0\\ 
-4x_{1}+2x_{2}+0x_{3}=0\\ 
x_{1}+0x_{2}+x_{3}=0
\end{matrix}\right.\\
通解为k\begin{pmatrix}
-1\\ 
-2\\ 
1
\end{pmatrix}\\
\therefore \lambda =\lambda _{2}=\lambda _{3}=1时，特征向量为k\begin{pmatrix}
-1\\ 
-2\\ 
1
\end{pmatrix}</script><h1 id="方阵是否与对角阵相似"><a href="#方阵是否与对角阵相似" class="headerlink" title="方阵是否与对角阵相似"></a>方阵是否与对角阵相似</h1><ul>
<li>判断特征向量个数是否与方阵阶数一致，一致则满足</li>
<li>也就是说对角阵的对角线上都有值</li>
</ul>
<h1 id="求方阵的对角阵和可逆变换矩阵"><a href="#求方阵的对角阵和可逆变换矩阵" class="headerlink" title="求方阵的对角阵和可逆变换矩阵"></a>求方阵的对角阵和可逆变换矩阵</h1><ul>
<li>解释：对角阵就是特征值矩阵，可逆变换矩阵就是归一化的特征向量矩阵</li>
</ul>
<p>列：</p>
<ul>
<li><p>已知方阵<br><script type="math/tex">\begin{pmatrix}
0 & -1 & 1\\ 
-1 & 0 & 1\\ 
1 & 1 & 0
\end{pmatrix}</script> 与对角矩阵相似（也就是说特征向量个数等于矩阵阶数，也就是说对角阵的对角线上都有值），求响应的对角阵和可逆变换矩阵</p>
</li>
<li><p>对角阵（特征值矩阵）</p>
<script type="math/tex; mode=display">A 的特征值为 \lambda _{1}=-2;\lambda _{2}=\lambda _{3}=1\\
当 \lambda _{1}=-2 时，特征向量为k\begin{pmatrix}
-1\\ 
-1\\ 
1
\end{pmatrix}\\
当\lambda _{2}=\lambda _{3}=1时，特征向量为k\begin{pmatrix}
1\\ 
0\\ 
1
\end{pmatrix} 与 k\begin{pmatrix}
-1\\ 
1\\ 
0
\end{pmatrix}\\
对角阵 \Lambda = \begin{pmatrix}
\lambda _{1} & 0 & 0\\ 
0 & \lambda _{2} & 0\\ 
0 & 0 & \lambda _{3}
\end{pmatrix}=\begin{pmatrix}
-2 & 0 & 0\\ 
0 & 1 & 0\\ 
0 & 0 & 1
\end{pmatrix}</script></li>
<li><p>可逆变换矩阵（特征向量矩阵）</p>
</li>
<li><p>1，特征向量归一化</p>
<script type="math/tex; mode=display">e_{1}=\frac{1}{\sqrt{3}}\begin{pmatrix}
-1\\ 
-1\\ 
1
\end{pmatrix}e_{2}=\frac{1}{\sqrt{2}}\begin{pmatrix}
1\\ 
0\\ 
1
\end{pmatrix}e_{3}=\frac{1}{\sqrt{6}}\begin{pmatrix}
-1\\ 
2\\ 
1
\end{pmatrix}</script></li>
<li><p>2，写成矩阵形式 $P=(e<em>{1},e</em>{2},e_{3})$</p>
</li>
</ul>
<script type="math/tex; mode=display">\begin{pmatrix}
\frac{-1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{6}}\\ 
\frac{-1}{\sqrt{3}} & 0 & \frac{2}{\sqrt{6}}\\ 
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}
\end{pmatrix}</script><h1 id="已知-P-1-AP-Lambda-求关于A的一些复杂运算次幂运算"><a href="#已知-P-1-AP-Lambda-求关于A的一些复杂运算次幂运算" class="headerlink" title="已知 $P^{-1}AP=\Lambda$,求关于A的一些复杂运算次幂运算"></a>已知 $P^{-1}AP=\Lambda$,求关于A的一些复杂运算次幂运算</h1><ul>
<li>原理，利用特征值变化使得基矩阵变成特征向量矩阵，那么这个特征向量矩阵的在某个角度就是只有对角线的形式，然后方便计算</li>
<li>利用公式：<script type="math/tex; mode=display">\varphi (A)=P\cdot \begin{pmatrix}
\varphi (\lambda _{1}) &  &  & \\ 
& \varphi (\lambda _{2}) &  & \\ 
&  & ... & \\ 
&  &  & \varphi (\lambda _{n})
\end{pmatrix}\cdot P^{\top }</script>列：<br>已知：<script type="math/tex; mode=display">A=\begin{pmatrix}
0 & -1 & 1\\ 
-1 & 0 & 1\\ 
1 & 1 & 0
\end{pmatrix},求A^{10}</script></li>
</ul>
<p>解：<br>A与对角矩阵相似即满足$P^{-1}AP=\Lambda$</p>
<script type="math/tex; mode=display">P=\begin{pmatrix}
\frac{-1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{6}}\\ 
\frac{-1}{\sqrt{3}} & 0 & \frac{2}{\sqrt{6}}\\ 
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}
\end{pmatrix}</script><ul>
<li>A的特征值为$\lambda<em>{1}=-2;\lambda </em>{2}=\lambda _{3}=1$</li>
</ul>
<script type="math/tex; mode=display">\therefore A^{10}=P\cdot\begin{pmatrix}
\lambda _{1}^{10} &  &  \\ 
 & \lambda _{2}^{10} &  \\ 
 &  & \lambda _{3}^{10} 
\end{pmatrix}\cdot P^{\top } \\
=\begin{pmatrix}
\frac{-1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{6}}\\ 
\frac{-1}{\sqrt{3}} & 0 & \frac{2}{\sqrt{6}}\\ 
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}
\end{pmatrix}^{\top } \cdot \begin{pmatrix}
(-2)^{10} &  &  \\ 
 & 1^{10} &  \\ 
 &  & 1^{10} 
\end{pmatrix}\cdot \begin{pmatrix}
\frac{-1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{6}}\\ 
\frac{-1}{\sqrt{3}} & 0 & \frac{2}{\sqrt{6}}\\ 
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}
\end{pmatrix}</script><h1 id="求二次型系数对应系数矩阵"><a href="#求二次型系数对应系数矩阵" class="headerlink" title="求二次型系数对应系数矩阵"></a>求二次型系数对应系数矩阵</h1><ul>
<li>切记除了对角线其他部位都要除以2<br>列1：<script type="math/tex; mode=display">求二次型f=-2x_{1}x_{2}+2x_{1}x_{3}+2x_{2}x_{3}对应系数矩阵</script></li>
</ul>
<ol>
<li><p>根据公式:</p>
<script type="math/tex; mode=display">f=a_{11}x_{1}x_{1}+a_{22}x_{2}x_{2}+...+a_{n}x_{n}x_{n}\\
+2a_{12}x_{1}x_{2}+2a_{13}x_{1}x_{3}+...+2a_{1n}x_{1}x_{n}\\
+2a_{23}x_{2}x_{3}+...+2a_{2n}x_{2}x_{n}\\
+... ... \\
+2a_{n-1,n}x_{n-1}x_{n}</script><p>求得</p>
<script type="math/tex; mode=display">f=a_{11}x_{1}^{2}+a_{22}x_{2}^{2}+a_{33}x_{3}^{2}
+2a_{12}x_{1}x_{2}+2a_{13}x_{1}x_{3}
+2a_{23}x_{2}x_{3} \\
解得\left\{\begin{matrix}
a_{11}=0\\ 
a_{22}=0\\ 
a_{33}=0\\
a_{12}=-1\\ 
a_{13}=1\\ 
a_{23}=1
\end{matrix}\right.</script></li>
<li><p>将各个a填入矩阵</p>
<script type="math/tex; mode=display">\begin{pmatrix}
0 & -1 & 1\\ 
& 0 & 1\\ 
&  & 0
\end{pmatrix}</script></li>
<li><p>以对角线为对称轴补全矩阵</p>
</li>
</ol>
<p>$ \begin{pmatrix}<br>0 &amp; -1 &amp; 1\<br> -1&amp; 0 &amp; 1\<br>1 &amp; 1 &amp; 0<br>\end{pmatrix} $</p>
<p>列2：</p>
<script type="math/tex; mode=display">求二次型f=x^{2}-3z^{2}-4xy+yz对应系数矩阵</script><p>得出：</p>
<script type="math/tex; mode=display">f=a_{11}x^{2}+a_{22}y^{2}+a_{33}z^{2}+a_{12}xy+a_{13}xy+a_{23}yz \\ 得\left\{\begin{matrix}
a_{11}=1\\ 
a_{22}=0\\ 
a_{33}=-3\\ 
a_{12}=-2\\ 
a_{13}=0\\ 
a_{23}= 0.5
\end{matrix}\right.</script><p>将各个a填入矩阵和补全</p>
<script type="math/tex; mode=display">\bigl(\begin{smallmatrix}
1 & -2 & 0\\ 
-2 & 0 & 0.5\\ 
0 & 0.5 & -3
\end{smallmatrix}\bigr)</script><h1 id="化二次型成标准型"><a href="#化二次型成标准型" class="headerlink" title="化二次型成标准型"></a>化二次型成标准型</h1><p>就是利用一个2次形的化简来求特征向量与特征值</p>
<ul>
<li>求二次型为标准型并求所用的变换矩阵P</li>
<li>1，跟上一题一样先把二次型写成系数矩阵的形式</li>
<li>2，求出特征值，然后把特征值带入标准型：$ f=\lambda <em>{1}y</em>{1}^{2}+\lambda <em>{2}y</em>{2}^{2}+\lambda <em>{3}y</em>{3}^{2} $</li>
<li>求变换矩阵就是求特征向量矩阵P</li>
</ul>
<h1 id="化二次型成规范形"><a href="#化二次型成规范形" class="headerlink" title="化二次型成规范形"></a>化二次型成规范形</h1><p>跟求标准型的方式一样，先求出特征向量然后利用公式$f=\frac{\lambda <em>{1}}{\left | \lambda</em>{1}  \right |}z<em>{1}^{2}+\frac{\lambda </em>{2}}{\left | \lambda<em>{2}  \right |}z</em>{2}^{2}+…+\frac{\lambda <em>{n}}{\left | \lambda</em>{n}  \right |}z_{n}^{2}$</p>
<ul>
<li>求所用到的变换矩阵C，根据下面公式<script type="math/tex; mode=display">C=P\begin{pmatrix}
\sqrt{\frac{1}{\left | \lambda _{1} \right |}} &  &  & \\ 
& \sqrt{\frac{1}{\left | \lambda _{2} \right |}}  &  & \\ 
&  & ... & \\ 
&  &  & \sqrt{\frac{1}{\left | \lambda _{4} \right |}} 
\end{pmatrix}</script></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/08/Matrix-of-houPhD/">Matrix of HouPhD</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/08/Matrix-of-houPhD/" class="archive-article-date"><time datetime="2019-08-08T14:16:17.000Z" itemprop="datePublished">August 8th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="矩阵相乘"><a href="#矩阵相乘" class="headerlink" title="矩阵相乘"></a>矩阵相乘</h1><ul>
<li>前行乘后列<br>试求：矩阵A乘矩阵B<script type="math/tex; mode=display">A \times B=\begin{pmatrix}
1 & 3\\ 
2 & 4\\ 
5 & 6
\end{pmatrix} \times \begin{pmatrix}
7 & 8 & 9\\ 
10 & 11 & 12
\end{pmatrix}\\
=\begin{pmatrix}
1\times 7+3\times 10 & 1\times 8+3\times 11 & 1\times 9+3\times 12\\ 
2\times 7+4\times 10 & 2\times 8+4\times 11 & 2\times 9+4\times 12\\ 
5\times 7+6\times 10 & 5\times 8+6\times 11 & 5\times 9+6\times 12
\end{pmatrix}
=\bigl(\begin{smallmatrix}
37 & 41 & 45\\ 
54 & 60 & 66\\ 
95 & 106 & 117
\end{smallmatrix}\bigr)</script></li>
</ul>
<h1 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h1><ul>
<li>$A\cdot 0=0$(0为全0矩阵)</li>
<li>$0\cdot A=0$</li>
</ul>
<script type="math/tex; mode=display">\begin{pmatrix}
0 & 0 & 0\\ 
0 & 0 & 0\\ 
0 & 0 & 0
\end{pmatrix}=0</script><ul>
<li>$A\cdot E=A$（E为对角线为1的矩阵也就是向量基）</li>
<li>$E\cdot A=A$</li>
<li>$E^{2}=E\cdot E=E$</li>
</ul>
<script type="math/tex; mode=display">\begin{pmatrix}
1 & 0\\ 
0 & 1
\end{pmatrix}=E</script><ul>
<li><p>矩阵相乘是有顺序的，矩阵为多维向量的变化，变化顺序不可以改变改变后结果不同（AB≠BA）<br>$A^{2}\cdot 2AB=(A^{2}\cdot 2A)\cdot B$ ✔️<br>$A^{2}\cdot 2AB=B \cdot (A^{2}\cdot 2A)$ ✖️</p>
</li>
<li><p>矩阵中使没有除法的所以AX=AY不能推出X=Y，（矩阵是一种变换）<br>$A^{2}B=AB\Rightarrow AB=2B$ ✖️</p>
</li>
<li><p>$(AB)^{k}$与$A^{k}B^{k}$不一定相等</p>
</li>
<li><p>$A^{2}+(k+j)AB+kjB^{2}$与$(A+kB)(A+jB)$不一定相等<br>$A^{2}+2AB+B^{2}=(A+B)^{2}$ ✖️<br>但如果与E相乘就是对的，如下：</p>
<script type="math/tex; mode=display">A^{2}+(k+j)A+kjE\\
=A^{2}+(k+j)A+kjE^{2}\\
=(A+kE)(A+jE)</script><p>$A^{2}+2AE+E^{2}=(A+E)^{2}$ ✔️<br>$A^{2}+2A+E=A^{2}+2AE+E^{2}=(A+E)^{2}$ ✔️</p>
</li>
</ul>
<h1 id="矩阵的行列式计算"><a href="#矩阵的行列式计算" class="headerlink" title="矩阵的行列式计算"></a>矩阵的行列式计算</h1><ul>
<li>矩阵的取行列式就是求这个矩阵的行列式结果</li>
<li>公式 $\left | \lambda A \right |=\lambda ^{n}\left | A \right |$(在变行列式的时候取倍数后别忘了次幂，这个步骤也就是说当矩阵变成行列式的时候他的倍数要乘以维度，也就是说矩阵的倍数就代表他的变化率在维度的倍数当变成行列式后就成了面积或体积的在多少维度的倍数)<br>已知<script type="math/tex">A=\begin{pmatrix}
2 & 4 & 6\\ 
4 & 6 & 8\\ 
8 & 10 & 14
\end{pmatrix} 求\left | A \right |</script></li>
</ul>
<script type="math/tex; mode=display">A=2\begin{pmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 7
\end{pmatrix}</script><script type="math/tex; mode=display">\left | A \right |=2^{3}\begin{vmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 7
\end{vmatrix}\\=8\times (-1)=-8</script><h1 id="矩阵的转置"><a href="#矩阵的转置" class="headerlink" title="矩阵的转置"></a>矩阵的转置</h1><ul>
<li>矩阵的转置相乘可以改变先后顺序，所以先用行乘列可以简化计算量</li>
<li>$(AB)^{\top }=B^{\top }A^{\top }$</li>
<li>$\left | A^{\top } \right |=\left | A \right |$</li>
</ul>
<h1 id="求矩阵是否可逆"><a href="#求矩阵是否可逆" class="headerlink" title="求矩阵是否可逆"></a>求矩阵是否可逆</h1><ul>
<li>判断矩阵是否满足这两个条件(低维矩阵是不可以升维的)</li>
</ul>
<ol>
<li>是否为方阵（行和列相等）</li>
<li>$\left | A \right | \neq 0 $ 或者存在一个方阵B，满足AB=E或BA=E</li>
</ol>
<p>列：</p>
<ul>
<li><p>设 <script type="math/tex">A=\begin{pmatrix}
1 & 2 & 3\\ 
0 & 4 & 5\\ 
0 & 0 & 6
\end{pmatrix}，试判断A是否可逆\left | A \right |=\begin{vmatrix}
1 & 2 & 3\\ 
0 & 4 & 5\\ 
0 & 0 & 6
\end{vmatrix}=24\neq 0</script><br>$\therefore A$可逆</p>
</li>
<li><p>设方阵A满足$A^{2}-A-2E=0$,证明A可逆</p>
<script type="math/tex; mode=display">A^{2}-A-2E =0\\
A^{2}-A=2E\\
A^{2}-AE=2E\\
A(A-E)=2E
A[\frac{1}{2}(A-E)]=E</script><p>令$B=\frac{1}{2}(A-E)<br>$<br>则$A\cdot B=E$<br>$\therefore A$可逆</p>
</li>
</ul>
<h1 id="求矩阵的逆"><a href="#求矩阵的逆" class="headerlink" title="求矩阵的逆"></a>求矩阵的逆</h1><ul>
<li><p>$\left ( A|E \right )\rightarrow \left ( E|A^{-1} \right )$<br>列：<br>已知 <script type="math/tex">A=\begin{pmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 7
\end{pmatrix},求A^{-1}</script></p>
<script type="math/tex; mode=display">\begin{pmatrix}
1 & 2 & 3 & | & 1 & 0 & 0\\ 
2 & 3 & 4 & | & 0 & 1 & 0\\ 
4 & 5 & 7 & | & 0 & 0 & 1
\end{pmatrix}\rightarrow \begin{pmatrix}
1 & 0 & 0 & | & -1 & -1 & 1\\ 
0 & 1 & 0 & | & -2 & 5 & -2\\ 
0 & 0 & 1 & | & 2 & -3 & 1
\end{pmatrix}</script><p>所以 $A^{-1}=\begin{pmatrix}<br>-1 &amp; -1 &amp; 1\<br>-2 &amp; 5 &amp; -2\<br>2 &amp; -3 &amp; 1<br>\end{pmatrix}$</p>
</li>
<li><p>利用公式：$A\cdot A^{-1}=E$或$A^{-1} \cdot A=E$计算(矩阵乘矩阵的逆等于向量基而不是消掉注意，而向量基乘以任何矩阵都等于矩阵本身所以此处才可以消掉)<br>已知$A=\begin{pmatrix}<br>1 &amp; 2 &amp; 3\<br>2 &amp; 3 &amp; 4\<br>4 &amp; 5 &amp; 7<br>\end{pmatrix}$,$B=\begin{pmatrix}<br>1 &amp; 2\<br>2 &amp; 1<br>\end{pmatrix}$,$C=\begin{pmatrix}<br>1 &amp; 4\<br>2 &amp; 5\<br>3 &amp; 6<br>\end{pmatrix}$,求矩阵X使满足AXB=C<br>解：</p>
<script type="math/tex; mode=display">AXB=C\\
A^{-1}XAB=A^{-1}C\\
EXB=A^{-1}C\\
XB=A^{-1}C\\
B^{-1}XB=B^{-1}A^{-1}C\\
EX=B^{-1}A^{-1}C\\
X=B^{-1}A^{-1}C= \begin{pmatrix}
-1 & -1 & 1\\
-2 & 5 & -2\\
2 & -3 & 1
\end{pmatrix}\begin{pmatrix}
1 & 4\\
2 & 5\\
3 & 6
\end{pmatrix}\begin{pmatrix}
-\frac{1}{3} & \frac{2}{3}\\ 
\frac{2}{3} & -\frac{1}{3}
\end{pmatrix}=\begin{pmatrix}
-2 & 1\\ 
\frac{8}{3} & -\frac{1}{3}\\
-\frac{1}{3} & -\frac{1}{3}
\end{pmatrix}</script></li>
</ul>
<h1 id="伴随矩阵"><a href="#伴随矩阵" class="headerlink" title="伴随矩阵"></a>伴随矩阵</h1><ul>
<li>根据行列式的展开法则，任一行元素与其对应代数余子式乘积之和等于行列式，与其他行对应元素的代数余子式乘积和为0，所以构造一个矩阵A<em>，其每一列元素的为Ade每一行元素的代数余子式，那么AA</em>就是一个对角矩阵，对角元都是|A|</li>
<li>伴随矩阵就是两个线性空间直接变换诱导的对偶空间上的变换</li>
<li>公式：$A\cdot A<em> = \left | A \right |E 或 A</em>\cdot A=\left | A \right |E$<br>列：已知<script type="math/tex">A=\begin{pmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 7
\end{pmatrix},且  A*X=A^{-1}+X ,求矩阵X</script><br>解：<script type="math/tex; mode=display">A*X=A^{-1}+X\\
AA*X=A(A^{-1}+X)\\
\left | A \right |EX=A(A^{-1}+X)\\
\left | A \right |EX=AA^{-1}+AX\\
\left | A \right |EX=E+AX\\
\left | A \right |EX-AX=E\\
(\left | A \right |E-A)X=E\\
(\left | A \right |E-A)^{-1}\cdot (\left | A \right |E-A)X= (\left | A \right |E-A)\cdot E\\
EX=(\left | A \right |E-A)\cdot E\\
X=(\left | A \right |E-A)^{-1}</script></li>
</ul>
<script type="math/tex; mode=display">\left | A \right |=\begin{vmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 7
\end{vmatrix} = -1\\</script><script type="math/tex; mode=display">\therefore X=(-E-A)^{-1}=\begin{bmatrix}
-\begin{pmatrix}
1 & 0 & 0\\ 
0 & 1 & 0\\ 
0 & 0 & 1
\end{pmatrix} & -\begin{pmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 7
\end{pmatrix}
\end{bmatrix}^{-1}\\=\begin{pmatrix}
-2 & -2 & -3\\ 
-2 & -4 & -4\\ 
-4 & -5 & -8
\end{pmatrix}^{-1}\\=\begin{pmatrix}
-2 & \frac{1}{6} & \frac{2}{3}\\ 
0 & -\frac{2}{3} & \frac{1}{3}\\ 
1 & \frac{1}{3} & -\frac{2}{3}
\end{pmatrix}</script><h1 id="秩"><a href="#秩" class="headerlink" title="秩"></a>秩</h1><ul>
<li>矩阵的秩就等于矩阵的维度，当有线性相关的向量时，这个向量在这个矩阵的解中完全起不到任何作用，这个向量就不能是这个矩阵的秩，而一个维度的矩阵只能有一个线性相关的向量，（方法：对矩阵进行行变换，使下行左端的0比上行的多直到下行全为零或者左下角的零一行比一行多就行，就相当于求行列式的体积的时候，要化简左下角为零这样对角线就是立方体的长宽高，试想如果一个立方体的长宽高都有则成立，如果少一个那就必须降维也就是说有线性相关，高维时也是一样）<br>列：<script type="math/tex; mode=display">已知A=\begin{pmatrix}
1 & 2 & 3 & 4\\ 
2 & 4 & 6 & 8\\ 
4 & 5 & 6 & 7\\ 
1 & 2 & 6 & 9
\end{pmatrix}, 求R(A)</script>解:<script type="math/tex; mode=display">A=\begin{pmatrix}
1 & 2 & 3 & 4\\ 
2 & 4 & 6 & 8\\ 
4 & 5 & 6 & 7\\ 
1 & 2 & 6 & 9
\end{pmatrix}=\begin{pmatrix}
1 & 2 & 3 & 4\\ 
0 & -3 & -6 & -9\\ 
0 & 0 & 3 & 5\\ 
0 & 0 & 0 & 0
\end{pmatrix}=R(A)=3</script></li>
</ul>
<h1 id="已知矩阵的秩求矩阵里的未知数"><a href="#已知矩阵的秩求矩阵里的未知数" class="headerlink" title="已知矩阵的秩求矩阵里的未知数"></a>已知矩阵的秩求矩阵里的未知数</h1><ul>
<li>已知矩阵的真实维度，求真实维度里的一个向量同理<br>列：<script type="math/tex; mode=display">已知 B=\begin{pmatrix}
1 & 2 & 3 & 4\\ 
2 & \mu  & 6 & 8\\ 
3 & 6 & 9 & \lambda 
\end{pmatrix}，且R(B)=1,求 \mu，\lambda 的值</script>解：<script type="math/tex; mode=display">B=\begin{pmatrix}
1 & 2 & 3 & 4\\ 
2 & \mu  & 6 & 8\\ 
3 & 6 & 9 & \lambda 
\end{pmatrix}\xrightarrow[r_{2}-2r_{1}]{r_{3}-3r_{1}}\begin{pmatrix}
1 & 2 & 3 & 4\\ 
0 & \mu  & 0 & 0\\ 
0 & 0 & 0 & \lambda 
\end{pmatrix}\\
R(B)=1\rightarrow \left\{\begin{matrix}
\mu - 4 = 0\\ 
\lambda -12 =0
\end{matrix}\right.\rightarrow \left\{\begin{matrix}
\mu = 4\\ 
\lambda = 12
\end{matrix}\right.</script></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/08/Determinant-nature-and-linear-algebra-of-houPhD/">Determinant nature and linear algebra of HouPhD</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/08/Determinant-nature-and-linear-algebra-of-houPhD/" class="archive-article-date"><time datetime="2019-08-08T11:42:14.000Z" itemprop="datePublished">August 8th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="行列式的计算"><a href="#行列式的计算" class="headerlink" title="行列式的计算"></a>行列式的计算</h1><hr>
<ul>
<li>二阶行列式就是求四边形的面积，jx和iy决定了四边形的长和高，当一条边不在x轴上的时候还要减去iy和jx的乘积<script type="math/tex; mode=display">\begin{vmatrix}
1 &2 \\ 
2& 3
\end{vmatrix} = 1\times 3-2\times 2 =  -1</script></li>
<li>多阶行列式 归零化 某行（列）加上或减去另一行列的几倍，行列式不变</li>
<li><p>多阶行列式就是求立方体的体积，其对角线决定了立方体的长宽高，其他元素等于他的边与xyz的偏斜度，我们需要把向量都贴在z轴和x轴上，也就是把左下变成0，这样就方便计算（对角线乘积即可，即是长宽高乘积）</p>
<script type="math/tex; mode=display">\begin{vmatrix}
1 & 2 & 3\\ 
2 & 3 & 4\\ 
4 & 5 & 6
\end{vmatrix} = \begin{vmatrix}
1 & 2 & 3\\ 
0 & -1 & -2\\ 
0 & 0 & 1
\end{vmatrix}= 1\times -1\times 1=-1</script></li>
<li><p>某行（列）乘k，等于k乘此行列式</p>
<script type="math/tex; mode=display">\begin{bmatrix}
1 & 2 & 3 & 4\\ 
2 & 3 & 4 & 5\\ 
4 & 5 & 7 & 8\\ 
8 & 9 & 10 & 12
\end{bmatrix} = 1</script></li>
</ul>
<script type="math/tex; mode=display">\begin{vmatrix}
2 & 4 & 6 & 8\\ 
2 & 3 & 4 & 5\\ 
12 & 15 & 21 & 24\\ 
8 & 9 & 10 & 12
\end{vmatrix} = 2\times 3\times \begin{vmatrix}
1 & 2 & 3 & 4\\ 
2 & 3 & 4 & 5\\ 
4 & 5 & 7 & 8\\ 
8 & 9 & 10 & 12
\end{vmatrix}=2\times 3\times -1=-6</script><ul>
<li>互换两行（列），行列式变号</li>
</ul>
<script type="math/tex; mode=display">\begin{bmatrix}
1 & 2 & 3 & 4\\ 
2 & 3 & 4 & 5\\ 
4 & 5 & 7 & 8\\ 
8 & 9 & 10 & 12
\end{bmatrix} = 1</script><script type="math/tex; mode=display">\begin{vmatrix}
2 & 3 & 4 & 5\\ 
1 & 2 & 3 & 4\\ 
4 & 5 & 7 & 8\\ 
8 & 9 & 10 & 12
\end{vmatrix} = -1\times \begin{vmatrix}
1 & 2 & 3 & 4\\ 
2 & 3 & 4 & 5\\ 
4 & 5 & 7 & 8\\ 
8 & 9 & 10 & 12
\end{vmatrix}= -1\times -1 = 1</script><script type="math/tex; mode=display">\begin{vmatrix}
0 & 0 & 0 & 3\\ 
0 & 0 & 3 & 2\\ 
1 & 2 & 3 & 4\\ 
0 & 5 & 2 & 4
\end{vmatrix}\overset{r1\leftrightarrow r4}{\rightarrow}-1\times \begin{vmatrix}
0 & 5 & 2 & 4\\ 
0 & 0 & 3 & 2\\ 
1 & 2 & 3 & 4\\ 
0 & 0 & 0 & 3
\end{vmatrix}\overset{r2\leftrightarrow r3}{\rightarrow}\\-1\times -1\times  \begin{vmatrix}
0 & 5 & 2 & 4\\ 
1 & 2 & 3 & 4\\ 
0 & 0 & 3 & 2\\ 
0 & 0 & 0 & 3
\end{vmatrix}\overset{r1\leftrightarrow r2}{\rightarrow} -1\times -1\times -1\times \begin{bmatrix}
1 & 2 & 3 & 4\\ 
0 & 5 & 2 & 4\\ 
0 & 0 & 3 & 2\\ 
0 & 0 & 0 & 3
\end{bmatrix}\\=-1\times -1\times -1\times 1\times 5\times 3\times 3=-45</script><hr>
<h1 id="特殊行列式计算公式"><a href="#特殊行列式计算公式" class="headerlink" title="特殊行列式计算公式"></a>特殊行列式计算公式</h1><hr>
<ul>
<li>对角线为x其余都是a则可用：</li>
</ul>
<script type="math/tex; mode=display">(x-a)^{n-1}[x+(n-1)a]</script><script type="math/tex; mode=display">\begin{vmatrix}
2 & 3 & 3 & 3\\ 
3 & 2 & 3 & 3\\ 
3 & 3 & 2 & 3\\ 
3 & 3 & 3 & 2
\end{vmatrix} = (2-3)^{4-1}[2+(4-1)\times 3] = -11</script><ul>
<li>第一行为1，其他行的次幂依次加1</li>
</ul>
<script type="math/tex; mode=display">\begin{vmatrix}
1 & 1 & 1 & 1\\ 
3 & 4 & 5 & 6\\ 
3^{2} & 4^{2} & 5^{2} & 6^{2}\\ 
3^{3} & 4^{3} & 5^{3} & 6^{3}
\end{vmatrix}= (6-5)(6-4)(6-3)(5-4)(5-3)(4-3)=12</script><ul>
<li>两行（列）相同或成比例时，行列式为0</li>
</ul>
<script type="math/tex; mode=display">\begin{vmatrix}
1 & 2 & 3 & 4\\ 
2 & 4 & 6 & 8\\ 
3 & 4 & 5 & 6\\ 
7 & 8 & 9 & 10
\end{vmatrix} = 0</script><ul>
<li>某行（列）为两项相加减的时候，行列式可以拆成两个行列式相加减</li>
</ul>
<script type="math/tex; mode=display">\begin{vmatrix}
1 & 2+a & 3 & 4\\
2 & 4+b & 6 & 8\\
3 & 4+c & 5 & 6\\
7 & 8+d & 9 & 10
\end{vmatrix}= \begin{vmatrix}
1 & 2 & 3 & 4\\
2 & 4 & 6 & 8\\
3 & 4 & 5 & 6\\
7 & 8 & 9 & 10
\end{vmatrix} + \begin{vmatrix}
1 & a & 3 & 4\\
2 & b & 6 & 8\\
3 & c & 5 & 6\\
7 & d & 9 & 10
\end{vmatrix}</script><h1 id="求余子式，代数余子式"><a href="#求余子式，代数余子式" class="headerlink" title="求余子式，代数余子式"></a>求余子式，代数余子式</h1><ul>
<li>一个n阶行列式，如果其中第行所有元素（ij）元aij外都为零，那么这个行列式等于aij与他的代数余子式的乘积（也就是说其他向量在这一维度为0那么这个行列式的体积就等于这个数与他的代数余子式的乘积，也就是面积乘以高）<br>列：求下列中a23的余子式和代数余子式（<a href="http://setosa.io/ev/eigenvectors-and-eigenvalues/）" target="_blank" rel="noopener">http://setosa.io/ev/eigenvectors-and-eigenvalues/）</a></li>
</ul>
<script type="math/tex; mode=display">\begin{vmatrix}
1 & 2 & 3\\ 
5 & 6 & 7\\ 
9 & 10 & 11
\end{vmatrix}</script><script type="math/tex; mode=display">M_{23}=\begin{vmatrix}
1 & 2\\ 
9 & 10
\end{vmatrix} = -8</script><ul>
<li>代数余子式就是判断余子式的正负号以便于带入运算作用<script type="math/tex; mode=display">A_{23}=(-1)^{2+3}\times M_{23}=-1\times -8=8</script></li>
</ul>
<h1 id="利用行列式展开公式求行列式"><a href="#利用行列式展开公式求行列式" class="headerlink" title="利用行列式展开公式求行列式"></a>利用行列式展开公式求行列式</h1><ul>
<li>当发现这个行列式其中有一个维度只有一个高度的时候，那么可用代数余子式展开出一个平行四边形的面积乘上这个高，如果那个维度不是只有一个高就表明这个立方体不在轴线上，那么就要加上或减去其他代数余子式所构成的立方体的体积</li>
<li>当某行或列只有一个数其余为零的时候可用以下公式：（行列式的展开公式）<script type="math/tex; mode=display">D=a_{i1}A_{i1}+a_{i2}+A_{i2}+...+a_{in}+A_{in}\\
D=a_{j1}A_{j1}+a_{j2}+A_{j2}+...+a_{jn}+A_{jn}</script></li>
</ul>
<p>列：</p>
<script type="math/tex; mode=display">\begin{vmatrix}
1 & 2 & 3\\
4 & 0 & 5\\
6 & 0 & 7
\end{vmatrix}=2\times (-1)^{1+2} M_{12} +0 \times  ...+0\times...=2\times (-1)^{1+2} M_{12}</script><ul>
<li>求多个A或M相加减(倍数直接替换相应行号，M要变号)</li>
</ul>
<p>已知：</p>
<script type="math/tex; mode=display">D=\begin{vmatrix}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8\\ 
9 & 10 & 11 & 12\\
13 & 14 & 15 & 16
\end{vmatrix}</script><p>求：</p>
<ul>
<li><script type="math/tex; mode=display">3A_{11}+4A_{12}+5A_{13}+6A_{14}</script></li>
<li><script type="math/tex; mode=display">3A_{11}+4A_{21}+5A_{31}+6A_{41}</script></li>
<li><script type="math/tex; mode=display">3M_{11}+4M_{12}+5M_{13}+6M_{14}</script></li>
</ul>
<script type="math/tex; mode=display">3A_{11}+4A_{12}+5A_{13}+6A_{14}=  D=\begin{vmatrix}
3 & 4 & 5 & 6\\
5 & 6 & 7 & 8\\
9 & 10 & 11 & 12\\
13 & 14 & 15 & 16
\end{vmatrix}</script><script type="math/tex; mode=display">3A_{21}+4A_{21}+5A_{31}+6A_{41}=  D=\begin{vmatrix}
3 & 2 & 3 & 4\\
4 & 6 & 7 & 8\\ 
5 & 10 & 11 & 12\\
6 & 14 & 15 & 16
\end{vmatrix}</script><ul>
<li>直接看行列和单数变号复数不变<script type="math/tex; mode=display">3M_{11}+4M_{12}+5M_{13}+6M_{14}=3A_{11}-4A_{21}+5A_{31}-6A_{41}</script><script type="math/tex; mode=display">=D=\begin{vmatrix}
3 & 2 & 3 & 4\\
-4 & 6 & 7 & 8\\ 
5 & 10 & 11 & 12\\
-6 & 14 & 15 & 16
\end{vmatrix}</script></li>
</ul>
<h1 id="根据行列式判断方程解得情况"><a href="#根据行列式判断方程解得情况" class="headerlink" title="根据行列式判断方程解得情况"></a>根据行列式判断方程解得情况</h1><div class="table-container">
<table>
<thead>
<tr>
<th>方程组</th>
<th style="text-align:center">D≠0</th>
<th style="text-align:center">D=0</th>
</tr>
</thead>
<tbody>
<tr>
<td>齐次</td>
<td style="text-align:center">只有一组零解</td>
<td style="text-align:center">有零解与非零解</td>
<td></td>
</tr>
<tr>
<td>非齐次</td>
<td style="text-align:center">只有一组非零解</td>
<td style="text-align:center">有多个解或无解</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>列：判断下列方程组是否有唯一解</p>
<script type="math/tex; mode=display">\left\{\begin{matrix}
x_{1} & +2x_{2} & +3x_{3} & =0\\ 
4x_{1} & +5x_{2} & +6x_{3} & =0\\ 
7x_{1} & +8x_{2} & +9x_{3} & =0
\end{matrix}\right.</script><script type="math/tex; mode=display">D=\begin{vmatrix}
1 & 2 & 3\\ 
4 & 5 & 6\\ 
7 & 8 & 9
\end{vmatrix}=0</script><ul>
<li>所以该方程组有零解与非零解，没有唯一解</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/05/Essence-of-Linear-Algebra/">Essence of Linear Algebra</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/05/Essence-of-Linear-Algebra/" class="archive-article-date"><time datetime="2019-08-04T18:55:03.000Z" itemprop="datePublished">August 5th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>尽管一批教授和教科书编者用关于矩阵的荒诞至极的计算内容掩盖了线性代数的简明性，但是鲜有与之相较更为初等的理论</p>
<h1 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h1><p>引入一些数作为坐标是一种鲁莽行为<br>向量的三种观点：</p>
<ul>
<li>物理：向量是空间中的箭头，决定一个向量的是他的长度和他所指向的方向，只要上面两个两定下来，可以自由移动这个向量而保持不变，平面的向量是二维的，空间向量是三维的，里面有很多不同的二维向量组成可无规律可有规律</li>
<li>计算机：向量是有序的数字列表</li>
<li>数学家：向量可以是任何东西，只要向量运算有意义即可</li>
</ul>
<p>1，在线性代数中向量的起点必须是原点而不是物理中的随意点</p>
<h1 id="向量的相加"><a href="#向量的相加" class="headerlink" title="向量的相加"></a>向量的相加</h1><p>就是两个剪头连接后的第一个向量头和第二向量尾距离，求最短距离无非就是曼哈顿距离的向量相加（向量相加是线性代数中为一一个允许向量离开原点的情形）<br>为什么这样定义：我们把每个向量看做是一种特定的运动，在空间中朝着某个方向迈出一定距离，直接走头尾相恋的线所到达的终点和先走第一个向量箭头在走第二个向量所到的终点是一样的（又因为如果两条向量平行的时候向量相加正好就是一条两个向量之和的线段）</p>
<h1 id="线性组合，张量的空间与基"><a href="#线性组合，张量的空间与基" class="headerlink" title="线性组合，张量的空间与基"></a>线性组合，张量的空间与基</h1><p>数学不需要天赋，只需要少量的自由想象</p>
<ul>
<li>首先要换个角度理解向量，试着把每个向量的值看成一个标量，而在[1，1]（基向量）的向量中使其各种缩放成为向量，也就是缩放向量并相加，这一概念至关重要[1xi，1xj]，</li>
<li>通过缩放任意向量中的标量再相加可以得到任意向量，两者的区别是前者是根据基向量缩放相加，后者是任意向量缩放相加，然而后者起初那个随意的向量也可以称为基向量，所以也可以说成：基向量就是这些标量的缩放对象，然而基向量未必就是以1为单位，他可以以任意数为单位<br><img src="/images/Essence_of_Linear_Algebra/0ß.png" alt></li>
<li>结论：每当我们用数字描述向量时，他都依赖于我们正在使用的基向量是什么，前提两条向量不平行，或是两个零向量</li>
<li>定向量张成的空间：所有可以表示为给定向量线性组合的向量集合av+bw，ab为实数</li>
<li>如何把向量看做是一个点：当我们把向量看成是射线时往往会显得拥挤，我们会把他写成终点形式而起点仍然是原点，所以当我们考虑落在一条直线上的所有向量时只需要考虑这条直线本身就行了，（或是一条超平面）</li>
<li>线性相关：如果你有两个或以上的向量，所构成的张成空间，在这个张成空间中如果减去一个向量后张成空间并没有变化，那么我们就把他叫做线性相关u=av+bw，u在vw组成的张成空间中，ab是任意数时</li>
<li>如果所有的向量都给张成的空间增加了一个”新的维度“，那么就是线性无关，u!=av+bw，u不在vw组成的张成空间中，ab是任意数时</li>
<li>空间的一组基的严格定义：张成空间的一个线性无关的向量集合</li>
</ul>
<p><img src="/images/Essence_of_Linear_Algebra/01.png" alt></p>
<h1 id="矩阵与线性变换"><a href="#矩阵与线性变换" class="headerlink" title="矩阵与线性变换"></a>矩阵与线性变换</h1><ul>
<li>线性变换是操纵空间的一种手段</li>
<li>线性变换=函数：只是输入和输出变成了向量，变换是以特定方式来可视化这一输入输出关系，在各种空间中有着各种不同的变换，线性变换也就相当于空间变换后的向量的样子，而这种变换可以非常复杂，也可以是线性的（直线在变换后仍然为直线不能有所弯曲，即保持网格线平行且等距分布，原点保持固定）</li>
<li>方法首先把向量转换成基向量的乘积的形式，然后确立ab后再乘上变换后的基向量<br><img src="/images/Essence_of_Linear_Algebra/02.png" alt><br><img src="/images/Essence_of_Linear_Algebra/03.png" alt><br><img src="/images/Essence_of_Linear_Algebra/04.png" alt><h1 id="矩阵乘法与线性变换复合"><a href="#矩阵乘法与线性变换复合" class="headerlink" title="矩阵乘法与线性变换复合"></a>矩阵乘法与线性变换复合</h1>复合变换：一个变换之后再进行另一种变换，两种变换互相独立</li>
<li>时刻记住两个矩阵相乘的几何意义：也就是两个线性变换的相继作用<em>*</em><br><img src="/images/Essence_of_Linear_Algebra/05.png" alt><br><img src="/images/Essence_of_Linear_Algebra/06.png" alt><br><img src="/images/Essence_of_Linear_Algebra/07.png" alt><br>矩阵乘法：<br><img src="/images/Essence_of_Linear_Algebra/08.png" alt><br>i-hat由矩阵的第一列给出</li>
<li>结合律<h1 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h1>线性变换改变面积的比例被称为这个变换的行列式，下图：就是说他将一个区域的面积增加为原来的6倍<br><img src="/images/Essence_of_Linear_Algebra/09.png" alt><br>如何用矩阵计算一个线性变换的行列式<br>当空间定向改变的情况下，行列式为负，但是行列式的绝对值依然表示区域面积的缩放比例<br>二维行列式负数是可理解为超平面翻转，三维行列式为右手坐标变成左手</li>
<li>行列式如何计算：Xi <em> Yj - Xj </em> Yi：前面相乘为长方形面积公式，减去后面相乘<br><img src="/images/Essence_of_Linear_Algebra/10.png" alt><br><img src="/images/Essence_of_Linear_Algebra/11.png" alt><br>如果Xj和Yi均不为0：<br><img src="/images/Essence_of_Linear_Algebra/12.png" alt><br><img src="/images/Essence_of_Linear_Algebra/13.png" alt><br><img src="/images/Essence_of_Linear_Algebra/14.png" alt><h1 id="逆矩阵-列空间-零空间-秩"><a href="#逆矩阵-列空间-零空间-秩" class="headerlink" title="逆矩阵 列空间 零空间 秩"></a>逆矩阵 列空间 零空间 秩</h1></li>
<li>线性方程组<br>未知数的相乘后相加就是矩阵的乘法运算，如果要求未知数那么就是求矩阵的逆运算，可解决方程组<br><img src="/images/Essence_of_Linear_Algebra/15.png" alt><br>A就是一个线性变换，方程的解依赖于A的变换<br>逆运算也有不可逆的情况，如果当有线性相关时，逆运算就变成了升维运算，那是不可能的所以就无法做逆运算：det(A)≠0 </li>
<li>秩代表空间变换后的维度</li>
<li>所有可能变换结果的集合就是列空间，（秩就是列空间的维数）当秩最大时与列数相等时成为满秩</li>
<li>零空间或核：对于一个满秩来说，变换后唯一能够落在原点的就是原点本身，因为原点不会随变化而改变位置，相反如果非满秩就一定有向量变成0向量，在这个时候，变换后落在原点的向量集合就是矩阵的零空间或核，这些向量在被压缩成0点之前所构成的空间叫做零空间<br><img src="/images/Essence_of_Linear_Algebra/16.png" alt><br><img src="/images/Essence_of_Linear_Algebra/17.png" alt><h1 id="非方阵"><a href="#非方阵" class="headerlink" title="非方阵"></a>非方阵</h1></li>
<li>2x3的矩阵可以理解成一个三维空间中过原点的二维平面（两条向量确定一个平面，而平面向z的倾斜取决于i与j的k<br><img src="/images/Essence_of_Linear_Algebra/18.png" alt><br>上面这个仍然为满秩，因为列空间的维数与输入空间的维数相同</li>
<li>3x2的矩阵<br><img src="/images/Essence_of_Linear_Algebra/19.png" alt><br><img src="/images/Essence_of_Linear_Algebra/20.png" alt></li>
<li>2x1的矩阵<br><img src="/images/Essence_of_Linear_Algebra/21.png" alt><h1 id="点积与对偶性"><a href="#点积与对偶性" class="headerlink" title="点积与对偶性"></a>点积与对偶性</h1>数学不是一门学科而是一门宗教<br><img src="/images/Essence_of_Linear_Algebra/22.png" alt><br><img src="/images/Essence_of_Linear_Algebra/23.png" alt><br><img src="/images/Essence_of_Linear_Algebra/24.png" alt><br>点积与顺序无关<br><img src="/images/Essence_of_Linear_Algebra/25.png" alt><br><img src="/images/Essence_of_Linear_Algebra/26.png" alt><br>为什么点积就是投影相乘：对偶性(多维空间到一维空间的线性变换)<br>我们根据向量基的方法求出降维后的向量v，假设变换后i=1，j=-2，v=i+j=-2<br><img src="/images/Essence_of_Linear_Algebra/27.png" alt><br>我们要得到i在u上的坐标i，也就是等于u的i的x坐标<br><img src="/images/Essence_of_Linear_Algebra/28.png" alt><br><img src="/images/Essence_of_Linear_Algebra/30.png" alt><h1 id="叉积"><a href="#叉积" class="headerlink" title="叉积"></a>叉积</h1><img src="/images/Essence_of_Linear_Algebra/31.png" alt></li>
<li>定向问题：顺序会对叉积有影响：如果v在w的右侧那么v叉乘w为正；如果v在w的左侧那么v叉乘w为负（这就好比一张纸翻了一个面）<br><img src="/images/Essence_of_Linear_Algebra/32.png" alt><br><img src="/images/Essence_of_Linear_Algebra/33.png" alt></li>
<li>真正的叉积是通过两个三维向量生成一个新的三维向量，叉积的结果不是一个数而是一个向量，这个向量的长度就是平行四边形的面积，叉积的长度就是另外两个向量的行列式的面积<br><img src="/images/Essence_of_Linear_Algebra/34.png" alt><br><img src="/images/Essence_of_Linear_Algebra/35.png" alt><br><img src="/images/Essence_of_Linear_Algebra/36.png" alt><br><img src="/images/Essence_of_Linear_Algebra/37.png" alt><br><img src="/images/Essence_of_Linear_Algebra/38.png" alt></li>
<li>三维向量的叉积<br><img src="/images/Essence_of_Linear_Algebra/39.png" alt><h1 id="以线性变换的眼光看叉积"><a href="#以线性变换的眼光看叉积" class="headerlink" title="以线性变换的眼光看叉积"></a>以线性变换的眼光看叉积</h1><img src="/images/Essence_of_Linear_Algebra/40.png" alt><br><img src="/images/Essence_of_Linear_Algebra/41.png" alt><h1 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h1>*我们可以把一个向量基的第一个坐标看做是向右移动几个单位长度，可以把第二个坐标看作是向上移动的单位长度，而单位长度与i-hat和j-hat有关，而这个就是基向量，而现在我们讨论另一种基向量，这种基向量就是任意向量做基，那么如何求任意向量做基的v呢，也就是在不同角度关注同一向量，<br><img src="/images/Essence_of_Linear_Algebra/42.png" alt></li>
<li>矩阵向量乘法<br><img src="/images/Essence_of_Linear_Algebra/43.png" alt></li>
<li>矩阵的逆<br><img src="/images/Essence_of_Linear_Algebra/44.png" alt><h1 id="特征向量与特征值"><a href="#特征向量与特征值" class="headerlink" title="特征向量与特征值"></a>特征向量与特征值</h1></li>
<li>张成空间，原点和向量尖端所延长的直线，可以是一维多维，大部分向量在进行变换时都离开了张成空间，而在矩阵变化时向量并没有离开他的张成空间这就是矩阵仅仅对他造成了拉伸或压缩</li>
<li>作用：减少依赖特定坐标系，而是求出特征向量，益特征向量为基准<br><img src="/images/Essence_of_Linear_Algebra/45.png" alt></li>
<li>特征向量：那些在矩阵变换中仅仅在张成空间里放大或缩小的而没有偏离的向量，</li>
<li>特征值：特征向量拉伸或压缩的倍数称为特征值<br><img src="/images/Essence_of_Linear_Algebra/46.png" alt><br><img src="/images/Essence_of_Linear_Algebra/46.png" alt><h1 id="特征向量与特征值-1"><a href="#特征向量与特征值-1" class="headerlink" title="特征向量与特征值"></a>特征向量与特征值</h1></li>
<li>我们通常把lambda称为特征值：$A\vec{v}= \lambda \vec{v}$矩阵向量的乘积=特征向量乘特征值，因此求矩阵A的特征向量和特征值就是求lambda</li>
<li>我们把等号右侧重写成么讴歌矩阵向量乘积，矩阵的列代表变换后的基向量，每个基向量仅仅与lambda相乘，所以对角线为lamabda<br><img src="/images/Essence_of_Linear_Algebra/47.png" alt><br><img src="/images/Essence_of_Linear_Algebra/48.png" alt><br><img src="/images/Essence_of_Linear_Algebra/49.png" alt></li>
<li>因为如果v为零等式自然成立所以我们需要v是一个非零解<br><img src="/images/Essence_of_Linear_Algebra/50.png" alt><br><img src="/images/Essence_of_Linear_Algebra/51.png" alt></li>
<li>只有降维的时候，一个非零向量被压缩才能变成零<br><img src="/images/Essence_of_Linear_Algebra/52.png" alt><br><img src="/images/Essence_of_Linear_Algebra/53.png" alt><br><img src="/images/Essence_of_Linear_Algebra/54.png" alt><br>lamabda是A的特征向量，特征向量就是个轴心（立方体按照轴心旋转），只有特征向量压扁才可以使整个矩阵降维</li>
<li>当二维矩阵在原点进行旋转时并没有特征向量，因为每一个向量都发生了旋转并离开了其张成空间，你也可以理解为特征向量就是二维中的z轴，虚数i或-i，如下图<br><img src="/images/Essence_of_Linear_Algebra/55.png" alt><br><img src="/images/Essence_of_Linear_Algebra/56.png" alt><br>剪切变换中（x轴不动只动y）<br><img src="/images/Essence_of_Linear_Algebra/57.png" alt><br>如果仅仅是缩放变换则每个向量都是特征向量，特征值为放大的倍数</li>
<li>对角矩阵的解释，每个向量都是以x或y轴旋转所得（都可以以x或y把矩阵降维）对角矩阵就是两条边坐落在x和y轴上<br><img src="/images/Essence_of_Linear_Algebra/58.png" alt></li>
<li>把特特征向量看成基向量（原始特征向量基矩阵变换成现在的矩阵就是缩放了，并且是以x轴和y轴缩放，因为现在的x和y轴是特征向量为基矩阵）<br><img src="/images/Essence_of_Linear_Algebra/61.png" alt><br><img src="/images/Essence_of_Linear_Algebra/59.png" alt><br><img src="/images/Essence_of_Linear_Algebra/60.png" alt></li>
<li>变换成特征基的作用，方便对矩阵进行计算（切记；算完还要转变回标准坐标系中哦，转变到大家都能看到的视角）<br><img src="/images/Essence_of_Linear_Algebra/62.png" alt></li>
<li>剪切变换因为特征量不够多并不能张成全空间<br><img src="/images/Essence_of_Linear_Algebra/63.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/08/02/Set-phrase-of-A/">Set phrase of A~</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/08/02/Set-phrase-of-A/" class="archive-article-date"><time datetime="2019-08-02T04:59:21.000Z" itemprop="datePublished">August 2nd</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="accuse-of"><a href="#accuse-of" class="headerlink" title="accuse of"></a>accuse of</h1><p>指控；指责</p>
<ul>
<li>so that（如此 以至于）</li>
<li>strikingly=very；baredy=not<br>fabricating:伪造（feb=假的；fable:寓言故事；范特西）<br>The newly described languages were often so strikingly different from the well studied languages of Europe and Southeast Asia that some scholars even accused Boas and Sapir of fabricating their data.<h1 id="act-on"><a href="#act-on" class="headerlink" title="act on"></a>act on</h1>对…起作用（有功效）；奉行；按照…而行动</li>
<li>情感来自于后面的介词<br>permit:允许<br>unpretentious:不装逼的（=modest中庸的；pretand：装逼的)<br>But the force that was guiding her would not permit her to act on such a thought.<h1 id="adapt-to"><a href="#adapt-to" class="headerlink" title="adapt to"></a>adapt to</h1>变得习惯于~；使适应于<br>concerning:关心、关注、担心、担忧<br>embrace:拥抱<br>thought:思维，思想<br>disintegrated:破碎（intel:整合；integrated：被整合的）<br>practice：实践、练习、经验<br>stability：n 稳定（stable：稳定的；steady：稳定的；n 稳定：steadiness）<br>What do people nowadays desire to do con¥ncerning their marriage?<br>[A] To embrace changes of thought<br>[B] To adapt to the disintegated family life<br>[C] To return to the practice in the 60s’ and 70s’<br>[D] To create stablility in their lives<h1 id="add-to"><a href="#add-to" class="headerlink" title="add to"></a>add to</h1>增加，增强<br>chemical：化学（chemical romance：化学反应（男女之间）My chemical romance 摇滚乐队）<br>opportunity：机会、机遇<br>Adding to a woman’s increased dose of stress chemicals,are her increased opportunities for stress.<h1 id="adhere-to"><a href="#adhere-to" class="headerlink" title="adhere to"></a>adhere to</h1>站队于遵守，遵循(规定或协议);支持、拥护（信仰）；黏着；附着 =stick to/with<br>executives:高管<br>headhunters:猎头（hunt：打猎）<br>candidate：候选人<br>poached：偷盗、偷窃、挖人<br>For years excutives and headhunters have adhere to the rule that the most attractive CEO candidates are the ones who must be poached.<h1 id="aim-for"><a href="#aim-for" class="headerlink" title="aim for"></a>aim for</h1>力求；以…为目标<br>present：a&amp;n. 当下的，目前的；v. 发表<br>seek:寻求<br>edition：版权，印刷版权（version：版本，【英文版或中文版】）<br>for good=永久的<br>efficient:高效的（sufficient：充足的大量的；deficient：匮乏的不足的）<br>management: 管理<br>situation:情况，位置，职位<br>stra’tegic：战略的、策略的（‘strategy=n.）<br>Peretti suggests that,in face of the present situation,The New York Timees should<br>[A] seek new sources of leadership<br>[B] end the print edition for good<br>[C] aim for efficient management<br>[D] make strategic adjustments<h1 id="apply-to"><a href="#apply-to" class="headerlink" title="apply to"></a>apply to</h1>适用于；应用于<br>discussions:讨论（cuss：讨厌的家伙，讨论）<br>practice：实践、练习、经验<br>situation:情况，位置，职位<br>Discussions at home can help kids practice doing these things and them apply these skills to everyday life situations.<h1 id="approve-of"><a href="#approve-of" class="headerlink" title="approve of"></a>approve of</h1>赞同、批准、赞成（prove：证明）<br>investor：投资人<br>analyst：分析家（analyse：分析）<br>appeared：出现<br>considering：考虑、顾及、认为(在一起就要顾虑到对方)<br>Investors and analysts appeared to approve of the move, considering the newspaper is a small piece of the company‘s overall business.<h1 id="arise-from"><a href="#arise-from" class="headerlink" title="arise from"></a>arise from</h1>由…产生（引起);起立，起身<br>suffer from：遭受…来自于<br>stresses:压力n；强调v=emphasis<br>population explosion：人口大爆炸<br>relatively：相对<br>means：方式方法（唯独加了s才叫方式方法）<br>by…meas of:基于…的方法<br>transport：交通运输<br>also occur：也会发生<br>Addition social stresses may also occur because of the population explosion or problems arising from mass migration movements—themselves ,ade relatively easy nowadays by modern means of transport.<br>表里如一的单词<h1 id="a-bit-of-有点儿"><a href="#a-bit-of-有点儿" class="headerlink" title="a bit of:有点儿"></a>a bit of:有点儿</h1><h1 id="as-regards：关于；至于-（concern-担忧；concerning-关于）"><a href="#as-regards：关于；至于-（concern-担忧；concerning-关于）" class="headerlink" title="as regards：关于；至于 （concern 担忧；concerning 关于）"></a>as regards：关于；至于 （concern 担忧；concerning 关于）</h1><h1 id="at-risk：处于危险中"><a href="#at-risk：处于危险中" class="headerlink" title="at risk：处于危险中"></a>at risk：处于危险中</h1><h1 id="on-the-contrary：正相关-另一方面（by-contrast-相反的）"><a href="#on-the-contrary：正相关-另一方面（by-contrast-相反的）" class="headerlink" title="on the contrary：正相关=另一方面（by contrast 相反的）"></a>on the contrary：正相关=另一方面（by contrast 相反的）</h1><h1 id="out-of-date：过时（out-of-time：过时；乐队：Oasis：绿洲-：I’m-outta-time-I’m-out-of-time）"><a href="#out-of-date：过时（out-of-time：过时；乐队：Oasis：绿洲-：I’m-outta-time-I’m-out-of-time）" class="headerlink" title="out of date：过时（out of time：过时；乐队：Oasis：绿洲 ：I’m outta time=I’m out of time）"></a>out of date：过时（out of time：过时；乐队：Oasis：绿洲 ：I’m outta time=I’m out of time）</h1><h1 id="transform-into-把-A-转换为-B（convert-A-into-B"><a href="#transform-into-把-A-转换为-B（convert-A-into-B" class="headerlink" title="transform into:把 A 转换为 B（convert A into B)"></a>transform into:把 A 转换为 B（convert A into B)</h1>passionate：热情的激情的<br>settle：定下来，满足<br>settle down：定居<h1 id="above-all：在所有的上面，最重要的是"><a href="#above-all：在所有的上面，最重要的是" class="headerlink" title="above all：在所有的上面，最重要的是"></a>above all：在所有的上面，最重要的是</h1></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/14/Artificial-Neural-Networks-of-Python-by-A-Z/">Artificial Neural Networks of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/14/Artificial-Neural-Networks-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-07-14T06:32:59.000Z" itemprop="datePublished">July 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/071e1a959a04484f9d80d8362ec4a2cd.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/13/CART-of-R-by-A-Z/">Decision Tree Classifier of R  by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/13/CART-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-07-13T07:34:29.000Z" itemprop="datePublished">July 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/88.png" alt></p>
<ul>
<li>因为决策树用的是切线切割没有用到欧式距离所以不需要进行特征缩放</li>
</ul>
<script src="https://gist.github.com/SauronLee/5550de72b8e9ab228abb30178fe5dbb6.js"></script>

<hr>
<pre><code>
&gt; View(dataset)
&gt; y_pred
  2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52  66  69  74  75  82  84 
  0   0   0   0   0   0   1   1   0   0   1   0   1   0   0   0   0   0   0   0   1   0   0   1 
 85  86  87  89 103 104 107 108 109 117 124 126 127 131 134 139 148 154 156 159 162 163 170 175 
  0   1   0   0   1   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0 
176 193 199 200 208 213 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 
  0   0   0   0   1   1   1   0   1   0   0   1   1   0   1   1   0   0   1   1   1   1   1   1 
292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 368 369 372 373 380 383 
  1   0   0   0   1   0   0   1   0   1   0   1   0   1   1   0   0   1   1   0   1   0   1   1 
389 392 395 400 
  1   1   0   1 
Levels: 0 1

&gt; cm
   y_pred
     0  1
  0 53 11
  1  6 30
</code></pre><p><img src="/images/A-Z_ML/89.png" alt><br><img src="/images/A-Z_ML/90.png" alt><br><img src="/images/A-Z_ML/91.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/13/CART-of-Python-by-A-Z/">Decision Tree Classifier of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/13/CART-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-07-13T07:34:19.000Z" itemprop="datePublished">July 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/85.png" alt></p>
<script src="https://gist.github.com/SauronLee/ef680bf345a816817042c7acf0503a39.js"></script>

<ul>
<li>由于过度拟合所以预测的非常差</li>
</ul>
<p><img src="/images/A-Z_ML/84.png" alt><br><img src="/images/A-Z_ML/86.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/13/Bayes-Theorem-by-A-Z/">Bayes Theorem by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/13/Bayes-Theorem-by-A-Z/" class="archive-article-date"><time datetime="2019-07-13T05:41:36.000Z" itemprop="datePublished">July 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/64.png" alt></p>
<ul>
<li>P(A)是先验概率P(A|B)是后验概率</li>
<li>B在朴素贝叶斯的分类器当中叫做特征，所以P(B)和P(A|B)不完全叫概率而叫做似然<br><img src="/images/A-Z_ML/65.png" alt></li>
<li>Plane Attack<br><img src="/images/A-Z_ML/66.png" alt><br><img src="/images/A-Z_ML/67.png" alt><br><img src="/images/A-Z_ML/68.png" alt><br><img src="/images/A-Z_ML/69.png" alt><br><img src="/images/A-Z_ML/70.png" alt><br><img src="/images/A-Z_ML/71.png" alt><br><img src="/images/A-Z_ML/72.png" alt><br><img src="/images/A-Z_ML/73.png" alt><br><img src="/images/A-Z_ML/74.png" alt><br><img src="/images/A-Z_ML/75.png" alt></li>
<li>缺少似然会降低计算量用于估计<br><img src="/images/A-Z_ML/76.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/12/NLP-Sentence-analysis/">NLP Sentence analysis</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/12/NLP-Sentence-analysis/" class="archive-article-date"><time datetime="2019-07-12T07:13:33.000Z" itemprop="datePublished">July 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="句法分析任务"><a href="#句法分析任务" class="headerlink" title="句法分析任务"></a>句法分析任务</h1><p><img src="/images/NLP/sentence_analysis_01.png" alt></p>
<h1 id="全局句法分析"><a href="#全局句法分析" class="headerlink" title="全局句法分析"></a>全局句法分析</h1><p><img src="/images/NLP/sentence_analysis_02.png" alt></p>
<h1 id="全局句法分析-1"><a href="#全局句法分析-1" class="headerlink" title="全局句法分析"></a>全局句法分析</h1><p><img src="/images/NLP/sentence_analysis_03.png" alt></p>
<h1 id="依存结构"><a href="#依存结构" class="headerlink" title="依存结构"></a>依存结构</h1><p><img src="/images/NLP/sentence_analysis_04.png" alt></p>
<ul>
<li>依存结构概念：</li>
</ul>
<ul>
<li>依照句法通过分析语言单位成分之前的依存关系解释其句法结构，主张句子中核心动词是支配其他成分的中心成分，而他本身却不受其他任何成分的支配，所有受支配成分都以某种关系从属于支配者</li>
</ul>
<ul>
<li>五个条件</li>
</ul>
<ul>
<li>一个句子中只有一个成分是独立的</li>
<li>句子的其他成分都从属于某一成分</li>
<li>如果成分A从属于成分B.而成分C在句子中位于A和B之间，那么，成分C或者从属于A，或者从属于B，或者从属于A和B之间的某一成分</li>
<li>中心成分左右两边的其他成分互相不发生关系<br><img src="/images/NLP/sentence_analysis_05.png" alt><h1 id="使用spacy进行语法分析"><a href="#使用spacy进行语法分析" class="headerlink" title="使用spacy进行语法分析"></a>使用spacy进行语法分析</h1></li>
</ul>
<ul>
<li>下载英文数据以及对应的模型 <code>python -m spacy download en</code></li>
</ul>
<script src="https://gist.github.com/SauronLee/c656cd3081b1931087aa6713cd267828.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/12/Dataset-Business-Problem-Description-by-A-Z/">Dataset + Business Problem Description by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/12/Dataset-Business-Problem-Description-by-A-Z/" class="archive-article-date"><time datetime="2019-07-11T17:57:51.000Z" itemprop="datePublished">July 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/A-Z_ML/07.png" alt></p>
<p><img src="/images/A-Z_ML/08.png" alt></p>
<ul>
<li>简单=多元<br>简单回归~多元回归向量的内积形式<br><img src="/images/A-Z_ML/09.png" alt></li>
<li>条件<br>1.数据是否是线性的<br>2.数据要有同样的方差<br>3,数据要呈现多元正态分布<br>4,我们的误差在不同的维度之间都必须是独立的<br>5,没有一个自变量是其他自变量的线性关系（因为只要两个自变量有逻辑关系那么整个预测就会被这种逻辑关系干扰）<br><img src="/images/A-Z_ML/10.png" alt></li>
<li>Dummy Data的处理</li>
<li>重点，在这里的【0，1】可以是任意的两个数，因为y=b_0+b_1*x这两个数只代表固定的两个数而已，两个维度中的变量，只是进行了整体的缩放。<br><img src="/images/A-Z_ML/11.png" alt><br><img src="/images/A-Z_ML/12.png" alt></li>
<li>虚拟变量陷阱=同时使用两个虚拟变量（会出现*条件5中的多重共线性）</li>
<li>如果加上D_5拟合后会出现误差极度接近为零=过拟合，冯诺依曼说过：“给我4个参数我可以拟合出一头大象，再给我一个参数我可以让这头大象鼻子竖在地上跳舞”</li>
<li>遇到2个以上的分类形式的虚拟变量的时候我们始终n-1，使得我们的维度不会溢出<br><img src="/images/A-Z_ML/13.png" alt><br><img src="/images/A-Z_ML/14.png" alt></li>
<li>对自变量的筛选极为重要，1，喂进去的如果是垃圾那么出来的也是垃圾，2，不容易解释自变量各自的意义和对模型的贡献最大<br><img src="/images/A-Z_ML/15.png" alt></li>
<li>刷选方法：<br>1，每个自变量都有用一个也不能舍去<br><img src="/images/A-Z_ML/16.png" alt><br>2，看每个自变量的显著性（反向淘汰）</li>
<li>门槛SL=0.05，对所有的自变量都做拟合求出P值（y=kx+b），取最高P值高于SL则去除（SL越小显著性越高）<br><img src="/images/A-Z_ML/17.png" alt><br>3，（顺向选择）和（反向淘汰）一样<br><img src="/images/A-Z_ML/18.png" alt><br>4，（双向淘汰）<br><img src="/images/A-Z_ML/19.png" alt><br>5，（信息量比较）最大信息熵</li>
<li>赤池信息量准则（一种信息打分的方式）对所有可能的模型进行打分，不适用过多自变量<br><img src="/images/A-Z_ML/20.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/11/NLP-Word-analysis/">NLP Word analysis</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/11/NLP-Word-analysis/" class="archive-article-date"><time datetime="2019-07-11T11:14:30.000Z" itemprop="datePublished">July 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="词法分析任务："><a href="#词法分析任务：" class="headerlink" title="词法分析任务："></a>词法分析任务：</h1><ul>
<li>将句子转换成词序列并标记句子中的词的词性</li>
<li>英文：用词的形态来表示语法变化的关系</li>
</ul>
<ul>
<li>英文的词法分析（曲折语）<br><img src="/images/NLP/word_analysis_02.png" alt></li>
<li>英文词识别，词形还原</li>
<li><p>未登陆词的处理</p>
</li>
<li><p>英文词性标注<br><img src="/images/NLP/word_analysis_01.png" alt></p>
</li>
</ul>
<ul>
<li><p>孤立语（汉语）</p>
</li>
<li><p>主要特点：1，缺乏词形变化；2，词序严格；3，虚词十分重要；4复合词多，派生词少 —语言学纲要</p>
</li>
</ul>
<h1 id="英语词法分析任务："><a href="#英语词法分析任务：" class="headerlink" title="英语词法分析任务："></a>英语词法分析任务：</h1><ul>
<li>单词识别（Tokenization）</li>
<li>词形还原（Lemmatization)</li>
<li><p>词性标注 POS(Part-of-Speech)Tagging</p>
</li>
<li><p>英语特点：词之间一般有边界标记，词的形态变化丰富<br><img src="/images/NLP/word_analysis_03.png" alt><br><img src="/images/NLP/word_analysis_04.png" alt><br><img src="/images/NLP/word_analysis_05.png" alt><br><img src="/images/NLP/word_analysis_06.png" alt><br><img src="/images/NLP/word_analysis_07.png" alt></p>
</li>
</ul>
<h1 id="中文词法分析任务："><a href="#中文词法分析任务：" class="headerlink" title="中文词法分析任务："></a>中文词法分析任务：</h1><ul>
<li>原始句子：警察正在详细调查事故原因</li>
<li>分词结果：警察/正在/详细/调查/事故/原因 （自动分词阶段）</li>
<li>词性标注结果：警察/NN 正在/AD 详细/AD 调查/VV 事故/NN 原因/NN</li>
</ul>
<p><img src="/images/NLP/word_analysis_08.png" alt><br><img src="/images/NLP/word_analysis_09.png" alt><br><img src="/images/NLP/word_analysis_10.png" alt><br><img src="/images/NLP/word_analysis_11.png" alt><br><img src="/images/NLP/word_analysis_12.png" alt></p>
<ul>
<li>分词方法：1，事先人工建立好分词规律比如量词前应该有数词（大量人工）2.自动分词算法（正向最大匹配+逆向最大匹配=双向最大匹配）3利用同现频率作为分词依据</li>
</ul>
<h1 id="英文使用NLTK"><a href="#英文使用NLTK" class="headerlink" title="英文使用NLTK"></a>英文使用NLTK</h1><script src="https://gist.github.com/SauronLee/315ce54c6032c09693284b9a04419b84.js"></script>

<h2 id="下载nltk"><a href="#下载nltk" class="headerlink" title="下载nltk"></a>下载nltk</h2><pre><code>conda activate tfpy36
ipython
import nltk
nltk.download()
</code></pre><h1 id="中文使用jieba"><a href="#中文使用jieba" class="headerlink" title="中文使用jieba"></a>中文使用jieba</h1><ul>
<li><a href="https://gitbub.com/fxsiy/jieba" target="_blank" rel="noopener">https://gitbub.com/fxsiy/jieba</a></li>
</ul>
<p><img src="/images/NLP/word_analysis_13.png" alt="issue"></p>
<p><a href="https://gist.github.com/SauronLee/8ad99cd9c995dd5ef5d83b7dfc0c1d17" target="_blank" rel="noopener">https://gist.github.com/SauronLee/8ad99cd9c995dd5ef5d83b7dfc0c1d17</a></p>
<h1 id="TFIDF"><a href="#TFIDF" class="headerlink" title="TFIDF"></a>TFIDF</h1><p><img src="/images/NLP/word_analysis_14.png" alt="issue"></p>
<h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><p><img src="/images/NLP/word_analysis_15.png" alt="issue"></p>
<h1 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h1><ul>
<li>将文本进行分词，去除停用词或词性刷选等操作之后，设定窗口为K，进行滑动，在窗口中共同出现的词之间即可建立起为无向边。<br><img src="/images/NLP/word_analysis_16.png" alt="issue"></li>
<li>TextRank提取关键词步骤</li>
<li>把给定的文本T按照完整的句子进行分割；</li>
<li>对于每个句子，进行分词和词性的标注处理，过滤掉停用词，只保留指定词性的单词，如名词，动词，形容词等，这些词形形成候选词</li>
<li>构建候选词关键词的词图G =（V，E)，其中V为节点集，由（2）生成候选词组，然后采用共现关系（cooccurrence）构造两点之间的边，两个节点之间存在的边仅当它们对应的词汇在长度为KDE窗口中共现；</li>
<li>根据PageRank原理中的衡量重要性公式，初始化各个节点的权重，然后迭代计算各个节点的权重，直到收敛；</li>
<li>对节点的权重进行倒序排列，从而得到最重要的个单词，作为关键词；</li>
<li>由（5）得到的最重要的T个单词，在原始文中进行标记，若形成相邻词组，则组合成多词关键词</li>
</ul>
<h1 id="使用Pkuseg进行中文分词-专业类别细分"><a href="#使用Pkuseg进行中文分词-专业类别细分" class="headerlink" title="使用Pkuseg进行中文分词(专业类别细分)"></a>使用Pkuseg进行中文分词(专业类别细分)</h1><p><img src="/images/NLP/word_analysis_17.png" alt="issue"></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/11/NLP-Corpus/">NLP_Corpus</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/11/NLP-Corpus/" class="archive-article-date"><time datetime="2019-07-11T10:42:21.000Z" itemprop="datePublished">July 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/NLP/corpus_01.png" alt></p>
<h1 id="北京大学计算语言所语料库标记"><a href="#北京大学计算语言所语料库标记" class="headerlink" title="北京大学计算语言所语料库标记"></a>北京大学计算语言所语料库标记</h1><ul>
<li><a href="http://opendata.pku.edu.cn/dataverse/icl" target="_blank" rel="noopener">http://opendata.pku.edu.cn/dataverse/icl</a></li>
</ul>
<p><img src="/images/NLP/corpus_02.png" alt></p>
<h1 id="London-Lund英语口语语料库："><a href="#London-Lund英语口语语料库：" class="headerlink" title="London-Lund英语口语语料库："></a>London-Lund英语口语语料库：</h1><ul>
<li><a href="http://www.helsinki.fi/varieng/CoRD/corpora/LLC/" target="_blank" rel="noopener">http://www.helsinki.fi/varieng/CoRD/corpora/LLC/</a></li>
</ul>
<p><img src="/images/NLP/corpus_03.png" alt></p>
<h1 id="Tencent中文语料库-19年刚刚开源的"><a href="#Tencent中文语料库-19年刚刚开源的" class="headerlink" title="Tencent中文语料库-19年刚刚开源的"></a>Tencent中文语料库-19年刚刚开源的</h1><ul>
<li><a href="https://ai.tencent.com/ailab/nlp/data/Tencent_AILab_ChineseEmbedding.tar.gz" target="_blank" rel="noopener">https://ai.tencent.com/ailab/nlp/data/Tencent_AILab_ChineseEmbedding.tar.gz</a><br><img src="/images/NLP/corpus_04.png" alt></li>
</ul>
<h1 id="语料库的类型"><a href="#语料库的类型" class="headerlink" title="语料库的类型"></a>语料库的类型</h1><ul>
<li>异质性（heterogeneous）：简单无选材</li>
<li>同质性（homogeneous）：如TIPSTER只收集军事方面语料</li>
<li>系统性（systematic）：定好收据预料的规则，平衡性和系统性</li>
<li>专用性（specialized）：如北美人文科学语料库</li>
<li>单语语料库</li>
<li>双语多多语对齐语料库：平行语料库：篇章对齐；句子对齐、结构对齐<br><img src="/images/NLP/corpus_05.png" alt></li>
<li>生语语料库：未经加工</li>
<li>熟语语料库：带有切分，标注（词性标注，句法结构信息标注（树库）语义信息标注</li>
<li>共时语料库：一个时间段，横截面</li>
<li>历时语料库：与共时相对，纵剖面<br><img src="/images/NLP/corpus_06.png" alt><!-- ![issue](/images/NLP/corpus_07.png) -->
</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/11/NagelSchrekenberg-data-cleaning/">NagelSchrekenberg data cleaning</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/11/NagelSchrekenberg-data-cleaning/" class="archive-article-date"><time datetime="2019-07-11T04:43:41.000Z" itemprop="datePublished">July 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/888d1371083e085df1309b33641e0fee.js"></script>

<p><img src="/images/NS_data_cleaning_01.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/10/Limg-by-pillow/">Greyimg of char by Pillow</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/10/Limg-by-pillow/" class="archive-article-date"><time datetime="2019-07-10T08:25:24.000Z" itemprop="datePublished">July 10th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>我们读入一个图像用image打开它，此时图像还是图像并不是np格式的</li>
<li>我们获取图像的宽和高并赋予新的比例作为字符的宽高</li>
<li>根据字符的宽和高对图片进行拉伸变成一个新的图像</li>
<li>新的图像我们把它转成灰度图进行像素的计算</li>
<li>任何一个像素值我们都可以看看他的数据类型是什么</li>
<li>读取数据大小</li>
<li>读取数据本身</li>
<li>我们再做一个字符串</li>
<li>我们用256去除这个字符的个数也就是说每个字符管1/n的像素范围</li>
<li>然后遍历替换</li>
</ul>
<script src="https://gist.github.com/SauronLee/0f6224e602a030147539fdb61f528bdb.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/07/08/jetbot-01/">Jetbot</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/07/08/jetbot-01/" class="archive-article-date"><time datetime="2019-07-08T13:53:55.000Z" itemprop="datePublished">July 8th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="首先准备必备材料"><a href="#首先准备必备材料" class="headerlink" title="首先准备必备材料"></a>首先准备必备材料</h1><hr>
<p><img src="/images/jetbot/01_01.png" alt></p>
<hr>
<h2 id="3D打印部分"><a href="#3D打印部分" class="headerlink" title="3D打印部分"></a>3D打印部分</h2><hr>
<h3 id="打印"><a href="#打印" class="headerlink" title="打印"></a>打印</h3><p>关于打印机的选购NNVIDIA给了俩个测试过的打印机一个320美元，另一个2500美元。<br><img src="/images/jetbot/01_02.png" alt><br><img src="/images/jetbot/01_03.png" alt></p>
<h4 id="打印材料"><a href="#打印材料" class="headerlink" title="打印材料"></a>打印材料</h4><p>NNVIDIA给出测试过的材料如下<br><img src="/images/jetbot/01_04.png" alt></p>
<h4 id="打印材料-1"><a href="#打印材料-1" class="headerlink" title="打印材料"></a>打印材料</h4><p><img src="/images/jetbot/01_05.png" alt><br><img src="/images/jetbot/01_06.png" alt></p>
<hr>
<h2 id="相机部分"><a href="#相机部分" class="headerlink" title="相机部分"></a>相机部分</h2><hr>
<p>这里我们选择树莓派18.99美元的相机，都一样用便宜就行：</p>
<p><img src="/images/jetbot/01_07.png" alt></p>
<hr>
<h2 id="Wi-Fi部分"><a href="#Wi-Fi部分" class="headerlink" title="Wi-Fi部分"></a>Wi-Fi部分</h2><hr>
<p>这里我们选择M2卡作为Wi-Fi部分元件</p>
<p><img src="/images/jetbot/01_08.png" alt></p>
<hr>
<h2 id="车轮部分"><a href="#车轮部分" class="headerlink" title="车轮部分"></a>车轮部分</h2><hr>
<p>车轮部分我们采用65mm的购买链接如下<br><a href="https://www.adafruit.com/product/3763" target="_blank" rel="noopener">https://www.adafruit.com/product/3763</a></p>
<hr>
<h2 id="螺丝部分"><a href="#螺丝部分" class="headerlink" title="螺丝部分"></a>螺丝部分</h2><hr>
<p><img src="/images/jetbot/01_09.png" alt></p>
<hr>
<p>最后是NVIDIA建议的工具，这个按自己的需求来就行</p>
<p><img src="/images/jetbot/01_10.png" alt></p>
<hr>
<p>本文的所有内容都可以在NVIDIA的开源库里找到链接如下</p>
<p><a href="https://github.com/NVIDIA-AI-IOT/jetbot/wiki/bill-of-materials">https://github.com/NVIDIA-AI-IOT/jetbot/wiki/bill-of-materials</a></p>
<hr>
<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><hr>
<h2 id="安装ROS-Melodic-Enter-Nano-shell"><a href="#安装ROS-Melodic-Enter-Nano-shell" class="headerlink" title="安装ROS Melodic (Enter Nano shell)"></a>安装ROS Melodic (Enter Nano shell)</h2><ul>
<li>使所有的Ubuntu的软件包：</li>
<li>添加ROS库到APT</li>
<li>安装ROS</li>
<li>添加ROS路径环境</li>
<li>关闭并重新启动终端</li>
</ul>
<pre><code>
 enable all Ubuntu packages:
sudo apt-add-repository universe
sudo apt-add-repository multiverse
sudo apt-add-repository restricted

# add ROS repository to apt sources
sudo sh -c &#39;echo &quot;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main&quot; &gt; /etc/apt/sources.list.d/ros-latest.list&#39;
sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116

# install ROS Base
sudo apt-get update
sudo apt-get install ros-melodic-ros-base

# add ROS paths to environment
sudo sh -c &#39;echo &quot;source /opt/ros/melodic/setup.bash&quot; &gt;&gt; ~/.bashrc&#39;
</code></pre><h2 id="安装Adafruit库"><a href="#安装Adafruit库" class="headerlink" title="安装Adafruit库"></a>安装Adafruit库</h2><ul>
<li>Adafruit的这些Python库支持TB6612 / PCA9685电机驱动器和SSD1306调试OLED：</li>
</ul>
<pre><code>
# pip should be installed
$ sudo apt-get install python-pip

＃安装Adafruit的库
$ pip安装Adafruit-MotorHAT
$ pip安装Adafruit-SSD1306
</code></pre><ul>
<li>授予您的用户访问i2c总线的权限：</li>
</ul>
<pre><code>
$sudo usermod -aG i2c $USER
sudo usermod -aG i2c nvidia
</code></pre><ul>
<li>重新引导系统以使更改生效。</li>
</ul>
<h2 id="创建catkin工作区"><a href="#创建catkin工作区" class="headerlink" title="创建catkin工作区"></a>创建catkin工作区</h2><pre><code># create the catkin workspace
mkdir -p ~/work/catkin_ws/src
cd ~/work/catkin_ws
catkin_make

# add catkin_ws path to bashrc
sudo sh -c &#39;echo &quot;source ~/work/catkin_ws/devel/setup.bash&quot; &gt;&gt; ~/.bashrc&#39;
Note: out of personal preference, my catkin_ws is created as a subdirectory under ~/workspace

#Close and open a new terminal window. Verify that your catkin_ws is visible to ROS:

$ echo $ROS_PACKAGE_PATH 
/home/nvidia/workspace/catkin_ws/src:/opt/ros/melodic/share
</code></pre><h2 id="Build-jetson-inference"><a href="#Build-jetson-inference" class="headerlink" title="Build jetson-inference"></a>Build jetson-inference</h2><p>克隆并构建jetson-inference：</p>
<pre><code># clone the repo
$ cd ~/workspace/catkin_ws/src
$ git clone https://github.com/dusty-nv/jetbot_ros

# build the package
$ cd ../    # cd ~/workspace/catkin_ws
$ catkin_make

# confirm that jetbot_ros package can be found
$ rospack find jetbot_ros
/home/nvidia/workspace/catkin_ws/src/jetbot_ros
</code></pre><h2 id="运行ros-deep-learning"><a href="#运行ros-deep-learning" class="headerlink" title="运行ros_deep_learning"></a>运行ros_deep_learning</h2><pre><code>
# install dependencies
sudo apt-get install ros-melodic-vision-msgs ros-melodic-image-transport ros-melodic-image-publisher

# clone the repo
cd ~/work/catkin_ws/src
git clone https://github.com/dusty-nv/ros_deep_learning

# make ros_deep_learning
cd ../    # cd ~/work/catkin_ws
catkin_make

# confirm that the package can be found
$ rospack find ros_deep_learning
/home/nvidia/works/catkin_ws/src/ros_deep_learning
</code></pre><h1 id="测试Testing-JetBot"><a href="#测试Testing-JetBot" class="headerlink" title="测试Testing JetBot"></a>测试Testing JetBot</h1><p>接下来，让我们检查机器人的不同组件是否在ROS下工作。<br>首先打开一个新终端，然后开始 roscore</p>
<pre><code>$ roscore
</code></pre><h2 id="运行电机"><a href="#运行电机" class="headerlink" title="运行电机"></a>运行电机</h2><pre><code>$ rosrun jetbot_ros jetbot_motors.py
</code></pre><p>该jetbot_motors节点将侦听以下主题：</p>
<p>/jetbot_motors/cmd_dir 相对航向（度[-180.0, 180.0]，速度[-1.0, 1.0]）<br>/jetbot_motors/cmd_raw 原始L / R电机命令（速度[-1.0, 1.0]，速度[-1.0, 1.0]）<br>/jetbot_motors/cmd_str 简单的字符串命令（左/右/前进/后退/停止）</p>
<h2 id="测试电机命令"><a href="#测试电机命令" class="headerlink" title="测试电机命令"></a>测试电机命令</h2><p>打开一个新终端，并运行一些测试命令：</p>
<pre><code>$ rostopic pub / jetbot_motors / cmd_str std_msgs / String --once “ forward ” 
$ rostopic pub / jetbot_motors / cmd_str std_msgs / String --once “ backward ” 
$ rostopic pub / jetbot_motors / cmd_str std_msgs / String --once “ left ” 
$ rostopic pub / jetbot_motors / cmd_str std_msgs / String --once “ right ” 
$ rostopic pub / jetbot_motors / cmd_str std_msgs / String --once “ stop ”
</code></pre><h2 id="使用Debug-OLED"><a href="#使用Debug-OLED" class="headerlink" title="使用Debug OLED"></a>使用Debug OLED</h2><p>如果您的JetBot上有SSD1306调试OLED，则可以运行该jetbot_oled节点以显示系统信息和用户定义的文本：</p>
<pre><code>
$ rosrun jetbot_ros jetbot_oled.py
</code></pre><p>默认情况下，jetbot_oled将使用最新的内存使用情况，磁盘空间和IP地址每秒刷新一次显示。</p>
<p>该节点还将侦听该/jetbot_oled/user_text主题以接收将显示的用户的字符串消息：</p>
<pre><code>rostopic pub / jetbot_oled / user_text std_msgs / String --on “ HELLO！”
</code></pre><h2 id="使用相机"><a href="#使用相机" class="headerlink" title="使用相机"></a>使用相机</h2><p>启动jetbot_camera节点：</p>
<pre><code>
$ rosrun jetbot_ros jetbot_camera
</code></pre><p>将jetbot_camera/raw作为sensor_msgs::Image具有BGR8编码的消息发布到主题。</p>
<h2 id="Gazebo-模拟器"><a href="#Gazebo-模拟器" class="headerlink" title="Gazebo 模拟器"></a>Gazebo 模拟器</h2><p><a href="http://gazebosim.org/tutorials?tut=install_ubuntu&amp;cat=install" target="_blank" rel="noopener">http://gazebosim.org/tutorials?tut=install_ubuntu&amp;cat=install</a></p>
<p><img src="/images/jetbot/01_11.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/06/12/Multiple-Linear-Regression-of-Python-by-A-Z/">Multiple Linear Regression of Python by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/06/12/Multiple-Linear-Regression-of-Python-by-A-Z/" class="archive-article-date"><time datetime="2019-06-11T16:23:11.000Z" itemprop="datePublished">June 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/9e65b12a0df11a2e6de7690f14fe3136.js"></script>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/06/12/Multiple-Linear-Regression-of-R-by-A-Z/">Multiple Linear Regression of R  by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/06/12/Multiple-Linear-Regression-of-R-by-A-Z/" class="archive-article-date"><time datetime="2019-06-11T16:23:01.000Z" itemprop="datePublished">June 12th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <script src="https://gist.github.com/SauronLee/61aa30f3ae2d0730ea591d8851d24623.js"></script>


<ul>
<li>R有自己的元素factor数据类型不必转化为01，123=100010001，在Coefficients:中R会自动检测贡献度最小的维度然后舍掉。</li>
<li>Estimate：预估的参数是多少，k_0，k_1,k_3，k_4。</li>
<li>Std. Error：标准误差</li>
<li>t value：T值</li>
<li>Pr(&gt;|t|)：P值，越大越弱（R&amp;D贡献最高）</li>
<li>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1<br>summary(regressor)</li>
</ul>
<p>Call:<br>lm(formula = Profit ~ ., data = training_set)</p>
<p>Residuals:<br>   Min     1Q Median     3Q    Max<br>-33128  -4865      5   6098  18065 </p>
<p>Coefficients:<br>                  Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept)      4.965e+04  7.637e+03   6.501 1.94e-07 <strong><em><br>R.D.Spend        7.986e-01  5.604e-02  14.251 6.70e-16 </em></strong><br>Administration  -2.942e-02  5.828e-02  -0.505    0.617<br>Marketing.Spend  3.268e-02  2.127e-02   1.537    0.134<br>State2           1.213e+02  3.751e+03   0.032    0.974    </p>
<h2 id="State3-2-376e-02-4-127e-03-0-058-0-954"><a href="#State3-2-376e-02-4-127e-03-0-058-0-954" class="headerlink" title="State3           2.376e+02  4.127e+03   0.058    0.954    "></a>State3           2.376e+02  4.127e+03   0.058    0.954    </h2><p>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>
<p>Residual standard error: 9908 on 34 degrees of freedom<br>Multiple R-squared:  0.9499,    Adjusted R-squared:  0.9425<br>F-statistic:   129 on 5 and 34 DF,  p-value: &lt; 2.2e-16</p>
<p> y_pred = predict(regressor, newdata = test_set)</p>
<ul>
<li><p>y_pred</p>
<pre><code> 4         5         8        11        16        20        21        24        31 
</code></pre><p>173981.09 172655.64 160250.02 135513.90 146059.36 114151.03 117081.62 110671.31  98975.29 </p>
<pre><code>32 
</code></pre><p>96867.03 </p>
<p>regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,</p>
<ul>
<li>data = dataset)<br>summary(regressor)</li>
</ul>
</li>
</ul>
<p>Call:<br>lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend +<br>    State, data = dataset)</p>
<p>Residuals:<br>   Min     1Q Median     3Q    Max<br>-33504  -4736     90   6672  17338 </p>
<p>Coefficients:<br>                  Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept)      5.008e+04  6.953e+03   7.204 5.76e-09 <strong><em><br>R.D.Spend        8.060e-01  4.641e-02  17.369  &lt; 2e-16 </em></strong><br>Administration  -2.700e-02  5.223e-02  -0.517    0.608<br>Marketing.Spend  2.698e-02  1.714e-02   1.574    0.123<br>State2           4.189e+01  3.256e+03   0.013    0.990    </p>
<h2 id="State3-2-407e-02-3-339e-03-0-072-0-943"><a href="#State3-2-407e-02-3-339e-03-0-072-0-943" class="headerlink" title="State3           2.407e+02  3.339e+03   0.072    0.943    "></a>State3           2.407e+02  3.339e+03   0.072    0.943    </h2><p>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>
<p>Residual standard error: 9439 on 44 degrees of freedom<br>Multiple R-squared:  0.9508,    Adjusted R-squared:  0.9452<br>F-statistic: 169.9 on 5 and 44 DF,  p-value: &lt; 2.2e-16</p>
<p> regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,</p>
<ul>
<li>data = dataset)<br>summary(regressor)</li>
</ul>
<p>Call:<br>lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,<br>    data = dataset)</p>
<p>Residuals:<br>   Min     1Q Median     3Q    Max<br>-33534  -4795     63   6606  17275 </p>
<p>Coefficients:<br>                  Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept)      5.012e+04  6.572e+03   7.626 1.06e-09 <strong><em><br>R.D.Spend        8.057e-01  4.515e-02  17.846  &lt; 2e-16 </em></strong><br>Administration  -2.682e-02  5.103e-02  -0.526    0.602    </p>
<h2 id="Marketing-Spend-2-723e-02-1-645e-02-1-655-0-105"><a href="#Marketing-Spend-2-723e-02-1-645e-02-1-655-0-105" class="headerlink" title="Marketing.Spend  2.723e-02  1.645e-02   1.655    0.105    "></a>Marketing.Spend  2.723e-02  1.645e-02   1.655    0.105    </h2><p>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>
<p>Residual standard error: 9232 on 46 degrees of freedom<br>Multiple R-squared:  0.9507,    Adjusted R-squared:  0.9475<br>F-statistic:   296 on 3 and 46 DF,  p-value: &lt; 2.2e-16</p>
<p> regressor = lm(formula = Profit ~ R.D.Spend + Marketing.Spend,</p>
<ul>
<li>data = dataset)<br>regressor = lm(formula = Profit ~ R.D.Spend,</li>
<li>data = dataset)<br>summary(regressor)</li>
</ul>
<p>Call:<br>lm(formula = Profit ~ R.D.Spend, data = dataset)</p>
<p>Residuals:<br>   Min     1Q Median     3Q    Max<br>-34351  -4626   -375   6249  17188 </p>
<p>Coefficients:<br>             Estimate Std. Error t value Pr(&gt;|t|)<br>(Intercept) 4.903e+04  2.538e+03   19.32   &lt;2e-16 <em>*</em></p>
<h2 id="R-D-Spend-8-543e-01-2-931e-02-29-15-lt-2e-16"><a href="#R-D-Spend-8-543e-01-2-931e-02-29-15-lt-2e-16" class="headerlink" title="R.D.Spend   8.543e-01  2.931e-02   29.15   &lt;2e-16 *"></a>R.D.Spend   8.543e-01  2.931e-02   29.15   &lt;2e-16 <em>*</em></h2><p>Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>
<p>Residual standard error: 9416 on 48 degrees of freedom<br>Multiple R-squared:  0.9465,    Adjusted R-squared:  0.9454<br>F-statistic: 849.8 on 1 and 48 DF,  p-value: &lt; 2.2e-16</p>
<p> y_pred = predict(regressor, newdata = test_set)<br> y_pred<br>       4        5        8       11       16       20       21       24       31       32<br>172369.0 170434.0 160345.5 136096.4 146869.4 122860.5 114175.9 106725.4 101994.2 101261.2 </p>
<ul>
<li>每个数的P值会根据垃圾维度的减少而变小</li>
<li>Multiple R-squared:  0.9465,    Adjusted R-squared:  0.9454 为预测的点和原来的点的差的平方的和。越接近于1他的拟合度就越好</li>
</ul>
<p><img src="/images/A-Z_ML/21.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/06/11/Multi-Armed-Bandit-Problem-by-A-Z/">Multi-Armed Bandit Problem by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/06/11/Multi-Armed-Bandit-Problem-by-A-Z/" class="archive-article-date"><time datetime="2019-06-11T03:10:08.000Z" itemprop="datePublished">June 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>强化学习 Reinforcement Learning 基本<br>强化学习是机器学习的一个分支。它主要解决的问题是机器如何根据某一时刻的信息决定下一时刻需要采取的行动。强化学习也被运用在人工智能领域，比如说让机器人学习如何走路。机器在作出决策之后，如果得到期望的结果，机器会得到奖励，反之则会得到惩罚。通过这个试错的过程，机器可以学到如何对已有的信息作出决策。</li>
</ul>
<ol>
<li>置信区间上界算法；</li>
<li>Thompson抽样算法。</li>
</ol>
<ul>
<li>训练机器狗（应用数学问题，马尔科夫链）<br><img src="/images/A-Z_ML/247.png" alt></li>
<li>多臂老虎机问题 Multi-Armed Bandit Problem</li>
<li>这个问题是强化学习的核心问题，我们看图二能很明显的知道5号机子可以利润最大化，但是我们在不知情的情况下并不知道，那么我们就需要探索每个机器但是如果探索次数较少我们也不可能探测出利润最大化的老虎机，所以快速的探索利润最大化的老虎机最为关键，这也是机器学习的问题，我们把与利润最大之间的差异叫做遗憾<br><img src="/images/A-Z_ML/248.png" alt><br><img src="/images/A-Z_ML/249.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/06/02/python-math01/">python &amp; math</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/06/02/python-math01/" class="archive-article-date"><time datetime="2019-06-02T09:48:52.000Z" itemprop="datePublished">June 2nd</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>此文为个人购买的邹老师课程的课堂笔记</li>
</ul>
<hr>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><ul>
<li>python作为数据分析语言是非常简洁的，很少代码就可以分析复杂问题<br><img src="/images/ZB_ML/01_01.png" alt></li>
<li>Python/R/Matlab为三足鼎立状态，但是python的关注度比较高</li>
<li>如果基础没有建议从python学起，python在ai领域还不错其他不行<br>比如验证中心及限定理<br><img src="/images/ZB_ML/01_02.png" alt></li>
<li>python的简单介绍<br><img src="/images/ZB_ML/01_03.png" alt><br><img src="/images/ZB_ML/01_04.png" alt><br><img src="/images/ZB_ML/01_05.png" alt><br><img src="/images/ZB_ML/01_06.png" alt><br><img src="/images/ZB_ML/01_07.png" alt><br><img src="/images/ZB_ML/01_08.png" alt><br><img src="/images/ZB_ML/01_09.png" alt></li>
</ul>
<h1 id="复习基础"><a href="#复习基础" class="headerlink" title="复习基础"></a>复习基础</h1><ul>
<li>我们看下面这个图像：<br><img src="/images/ZB_ML/01_10.png" alt></li>
<li>我们如何找到一个底数使得log的x的对数的函数图像的导数为1呢？</li>
<li>也就是这么个方案：<br><img src="/images/ZB_ML/01_11.png" alt><br>这个是大一知识,答案如下：<br><img src="/images/ZB_ML/01_12.png" alt><br><img src="/images/ZB_ML/01_13.png" alt></li>
</ul>
<h1 id="python解释器-版本差异"><a href="#python解释器-版本差异" class="headerlink" title="python解释器-版本差异"></a>python解释器-版本差异</h1><ul>
<li><p>在python2.7中print是一个关键字不用加括号python3.6中print是一个函数所以要加括号</p>
</li>
<li><p>Java/C/C++/Python2.7中1/3=0（因为整数除法他是余1的余数下溢出了只剩零了）而在Python3.6中1/3=0.333333而1//3=0，整个语法都做了变换</p>
</li>
<li><p>比如在python2.7中做range(5)=[0，1，2，3，4],而在Python3.6中range(5)就是一个迭代器并不是从0开始或什么的具体数字，如果一定要在Python3.6中得到[0，1，2，3，4]那就要加上list(range(5))<br>才可以</p>
</li>
</ul>
<p>建议直接用python3以上版本</p>
<h2 id="package"><a href="#package" class="headerlink" title="package"></a>package</h2><ul>
<li>numpy：可以直接做：FFT/Gauss/LSQ/SVD 而且它还提供一个很重要的数据结构ndarray</li>
<li>pandas：它提供了一个DataFrame的存储结构，他可以读取二维的表哥数据比如Excel/csv/tsv：DataFrame是由多个列(Series)组成的二维表，Series就是一个向量，pandas本质上是封装了numpy，也就是说如果我们想要在电脑上装pandas这个包要先保证电脑上有numpy这个包</li>
<li>scipy：做科学运算的，有些运算在numpy中没有提供的在scipy中做比如：Gamma/Comb</li>
<li>matplotlib：绘图，主要是二维的</li>
<li>Scikit-learn：机器学习库</li>
<li>tensorflow：深度学习库除了tensorflow库还有：pytorch/Theano/Caffe/微软的PandlePandle；另外tensorflow虽然很有名待不是很好写代码有时候会用Keras，Keras本质上封装了tensorflow和Theano，Theano的更新没有tensorflow那么快速，keras并不是一个完整的深度学习平台他只是封装了别人的</li>
</ul>
<h3 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h3><ul>
<li>建议用pip非常好用（只要安装了python的解释器自带pip）<br>pip install numpy（需要联网）<br>本身的逻辑是上了pypi.org这个官网去获取的</li>
<li>大家可以用pip freeze或者pip list可以看到本机已经安装过的包</li>
</ul>
<h3 id="grammar"><a href="#grammar" class="headerlink" title="grammar"></a>grammar</h3><script src="https://gist.github.com/SauronLee/eac0c2e2a581b14e6da5fb314c441715.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/05/20/LeetCode-224-Basic-Calculator/">LeetCode 224 Basic Calculator</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/05/20/LeetCode-224-Basic-Calculator/" class="archive-article-date"><time datetime="2019-05-20T12:15:53.000Z" itemprop="datePublished">May 20th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Basic-Calculator"><a href="#Basic-Calculator" class="headerlink" title="Basic Calculator"></a>Basic Calculator</h1><p>LeetCode_224<br>ARTS_05</p>
<hr>
<p><img src="/images/LeetCode-224-Basic-Calculator.png" alt></p>
<hr>
<img class="img-test LeetCode-224-Basic-Calculator/1.png 150 150 图片测试">
<script src="https://gist.github.com/SauronLee/5056d0ebc27cfb065ba66b5fb273af75.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/05/20/POJ-1363-Rails/">POJ 1363 Rails</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/05/20/POJ-1363-Rails/" class="archive-article-date"><time datetime="2019-05-19T23:52:24.000Z" itemprop="datePublished">May 20th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Rails"><a href="#Rails" class="headerlink" title="Rails"></a>Rails</h1><p>POJ_1363<br>ARTS_04</p>
<hr>
<script src="https://gist.github.com/SauronLee/67dd7a8fa81daef79ce3d2608338e8cb.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/05/20/LeetCode-155-Min-Stack/">LeetCode 155 Min Stack</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/05/20/LeetCode-155-Min-Stack/" class="archive-article-date"><time datetime="2019-05-19T23:17:51.000Z" itemprop="datePublished">May 20th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Min-Stack"><a href="#Min-Stack" class="headerlink" title="Min Stack"></a>Min Stack</h1><p>LeetCode_155<br>ARTS_03</p>
<hr>
<p><img src="/images/LeetCode-155-Min-Stack.png" alt></p>
<script src="https://gist.github.com/SauronLee/8b0d5f89b9c659e871c769f05a8d37c6.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/05/20/LeetCode-232-Implement-Queue-using-Stacks/">LeetCode 232 Implement Queue using Stacks</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/05/20/LeetCode-232-Implement-Queue-using-Stacks/" class="archive-article-date"><time datetime="2019-05-19T19:03:51.000Z" itemprop="datePublished">May 20th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Implement-Queue-using-Stacks"><a href="#Implement-Queue-using-Stacks" class="headerlink" title="Implement Queue using Stacks"></a>Implement Queue using Stacks</h1><p>LeetCode_232<br>ARTS_02</p>
<hr>
<p><img src="/images/LeetCode-232-Implement-Queue-using-Stacks.png" alt></p>
<script src="https://gist.github.com/SauronLee/c6fd684b0e8268196f5fbb325ae7873c.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/05/20/LeetCode-225-Implement-Stack-using-Queyes/">LeetCode 225 Implement Stack using Queyes</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/05/20/LeetCode-225-Implement-Stack-using-Queyes/" class="archive-article-date"><time datetime="2019-05-19T16:42:41.000Z" itemprop="datePublished">May 20th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Implement-Stack-using-Queyes"><a href="#Implement-Stack-using-Queyes" class="headerlink" title="Implement Stack using Queyes"></a>Implement Stack using Queyes</h1><p>LeetCode_225<br>ARTS_01</p>
<hr>
<p><img src="/images/LeetCode-225-Implement-Stack-using-Queyes.png" alt></p>
<script src="https://gist.github.com/SauronLee/ea5a37677ba49eb772b77522e00105d0.js"></script>
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/05/02/NLP-Gram/">Bi-Gram/Tri-Gram/N-Gram</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/05/02/NLP-Gram/" class="archive-article-date"><time datetime="2019-05-02T06:28:33.000Z" itemprop="datePublished">May 2nd</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="语言模型（Bi-Gram-Tri-Gram-N-Gram）"><a href="#语言模型（Bi-Gram-Tri-Gram-N-Gram）" class="headerlink" title="语言模型（Bi-Gram/Tri-Gram/N-Gram）"></a>语言模型（Bi-Gram/Tri-Gram/N-Gram）</h1><ul>
<li>语言模型就是用于评估文本符合语言使用习惯程度的模型。<br>在计算机中很难找到人类语言的使用习惯，因为人类的语言使用习惯根据朝代试点的变迁有着很大的差异，所以现在更流行用语言出现的的概率来判断句子是否通顺。<br>比如：我去操场（  ）；计算机会识别我去操场跑步或是其他运动方面的单词出现的概率会更大，再比如翻译的时候，计算机会统计这么翻译的话他会在实际现实应用中出现概率。</li>
<li>总结来说就是无论在机器翻译；拼音纠错；语音识别；音字转换方面都是判断这样转换的概率是否最大化</li>
</ul>
<h1 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h1><ul>
<li>他的输入是一句话（单词的顺序序列），输出是这句话的概率（这些单词结合的概率-joint probability）</li>
<li>N-Gram的意思就是由N个单词组合的模型，单词分先后顺序，不要求点此之间互不相同，其中N常用取2或者3词组合</li>
<li>数学表达式：假设每一个单词都要依赖于从第一个单词在他之前的一个单词的影响：p(S)=p(w1w2…wn)=p(w1)p(w2|w1)…p(wn|wn-1…w2w1)；<br><img src="/images/NLP/bi_gram_01 .png" alt="issue"><br>利用MLE思想（利用已知的样本信息来反推模型参数，换句话说什么样的模型才会出现现在这种样本）；</li>
<li>问题：参数空间太大O(n)个所以我们通过马尔科夫假设；数据稀疏严重</li>
</ul>
<h1 id="Bi-Gram"><a href="#Bi-Gram" class="headerlink" title="Bi-Gram"></a>Bi-Gram</h1><ul>
<li>之前说的马尔科夫假设如果只与前面一个词有关我们就叫bi-gram：p(S)=p(wn|wn-1)<br><img src="/images/NLP/bi_gram_02.png" alt></li>
<li>问题：语境切换问题存在，我们用神经网络模型引入时间维度来解决此问题</li>
<li>那么句子I want english food 的概率为：<br><img src="/images/NLP/bi_gram_03.png" alt>   </li>
<li>为防止数据溢出我们会用加法代替乘法计算<br><img src="/images/NLP/bi_gram_04.png" alt><br><img src="/images/NLP/bi_gram_05.png" alt>  </li>
</ul>
<h1 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h1><ul>
<li>在实际应用中看具体表现；计算preplexity<br><img src="/images/NLP/bi_gram_06.png" alt>  </li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2019/02/21/Linear-regression/">Linear regression</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2019/02/21/Linear-regression/" class="archive-article-date"><time datetime="2019-02-21T06:18:34.000Z" itemprop="datePublished">February 21st</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="回归平均值（regression-to-the-mean）"><a href="#回归平均值（regression-to-the-mean）" class="headerlink" title="回归平均值（regression to the mean）"></a>回归平均值（regression to the mean）</h1><ul>
<li>y= ax+b求a和b线性回归为一次方的变化<br>x：影响y的因素，维度<br>x1，y1；x2，y2；x3，y3；x4，y4；任意两组可求出a和b，找出误差最小的a和b为最优解<h1 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h1></li>
<li>y=w1<em>x1+w2+x2+w3</em>x3+w4+x4…wn<em>xn+w0</em>x0(x0=1)：= w为横向量 * x为列向量（依次相乘并相加因为x为行向量我们把他转置）= y=WT•X = ∂T•x.</li>
<li>误差=Y-y=epsilon(大Y是真实值，小y是预测值)</li>
<li>w越大影响度越大为权重，w是行向量，因为x为多维，y为列向量<h1 id="中间极限"><a href="#中间极限" class="headerlink" title="中间极限"></a>中间极限</h1>例子：如果父亲比较高儿子大多不会比父亲高，如果父亲比较矮儿子大多会比父亲高，大自然会把数值恒定在一个区间范围内所谓高斯/正态分布。<h1 id="最大似然（likelihood）"><a href="#最大似然（likelihood）" class="headerlink" title="最大似然（likelihood）"></a>最大似然（likelihood）</h1>它用来求一个样本集的相关概率密度函数的参数，似然=概率probability;期望值和方差定了形状就定了;要反过来当不知道正态分布是什么形状的时候我们用我们的dataset去试看他适合哪个的正态分布来找到概率最大的情况，用这组数据去正态分布里面去看，看哪个正态分布所对应的概率密度最大，他就是哪个正态分布<br><img src="/images/math_post/linear_regression01.png" alt><h1 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h1>每一个函数都可以写成一个概率密度函数<br>均匀分布：最简单的概率密度函数，这条线上的两点出现的概率是一样的；<br><img src="/images/math_post/linear_regression02.png" alt><br>不均匀分别：正态分布的概率‘密度’函数F(x)是概率密度不是概率，f(x）越大就代表mu和sigma两个值定了（这个正态分布的形状和位置就定下来了）的情况下x出现的概率越大。可以先理解为概率却不等于概率。<br><img src="/images/math_post/linear_regression03.png" alt><br><a href="https://baike.baidu.com/item/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0/5021996" target="_blank" rel="noopener">https://baike.baidu.com/item/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0/5021996</a><br>把概率中的误差带入概率密度公式：1，真实值减去预测值等于误差值，2，x为随机变量我们用epsilon替换掉x-mu<br><img src="/images/math_post/linear_regression03.png" alt><br><img src="/images/math_post/linear_regression04.png" alt></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
      
        </div></section>
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <a href="/archives/2018" class="archive-year">2018</a>
        </div>
        <div class="archives">
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/08/29/Artificial-Neural-Networks-by-A-Z/">Artificial Neural Networks by A-Z</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/08/29/Artificial-Neural-Networks-by-A-Z/" class="archive-article-date"><time datetime="2018-08-28T17:50:18.000Z" itemprop="datePublished">August 29th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Gradient Descent</li>
</ul>
<p><img src="/images/A-Z_ML/275.png" alt="Gradient Descent"><br><img src="/images/A-Z_ML/276.png" alt="Gradient Descent"><br><img src="/images/A-Z_ML/277.png" alt="Gradient Descent"><br><img src="/images/A-Z_ML/278.png" alt="Gradient Descent"></p>
<ul>
<li>Stochastic Gradient Descent<br><img src="/images/A-Z_ML/279.png" alt="Stochastic Gradient Descent"></li>
<li>Stochastic Gradient Descent と　Gradient Descent比べて</li>
<li>Gradient Descentではデーターまとめてこのモデルに入っていっしょに計算したらウエイトをでること</li>
<li>Stochastic Gradient Descentではデーターを一つずつモデルにはりたそのウエイトを計算して、次のデーターベクトル入って前回のウエイト次第更新すること（ウエイトは毎回違いですので、なぜいStochasticということ）<br><img src="/images/A-Z_ML/280.png" alt="Stochastic Gradient Descent"><br><img src="/images/A-Z_ML/281.png" alt="13 lines"><br><img src="/images/A-Z_ML/282.png" alt="neural network and deep learning"><br><a href="http://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf" target="_blank" rel="noopener">http://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf</a></li>
<li>Back-propagation<br><img src="/images/A-Z_ML/283.png" alt="propagation"><br><img src="/images/A-Z_ML/284.png" alt="Back-propagation"><br><img src="/images/A-Z_ML/285.png" alt="Back-propagation"></li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/05/17/Linux-base/">Linux base</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/05/17/Linux-base/" class="archive-article-date"><time datetime="2018-05-17T04:15:19.000Z" itemprop="datePublished">May 17th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p>第一章：Linuxひとめぐり</p>
<ul>
<li>Linux：Linuxとは、Unix系オペレーティングシステムカーネル（Operating system kernel）であるLinuxカーネル、およびそれをカーネルとして周辺を整備したシステムである。</li>
<li>UNIX：UNIX は、コンピュータ用のマルチタスク・マルチユーザーのオペレーティングシステム(Multitasking / multiuser operating system)の一種である。公式な商標は「UNIX」だが、商標以外の意味として「Unix」、またはスモールキャピタルを使用して「Unix」などとも書かれる。</li>
<li>ディストリビューター（Distributor）：オペレーティングシステム（OS）の一種であるLinuxの中核となっているカーネルに、各種の周辺的ソフトウェアを独自に追加することによって、それぞれ独自の特徴をLinuxに追加した「Linuxディストリビューション」を開発したり配布したりする企業のこと。</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/04/17/Memory-Game-Project/">Memory Game Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/04/17/Memory-Game-Project/" class="archive-article-date"><time datetime="2018-04-16T16:43:01.000Z" itemprop="datePublished">April 17th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_WEB_P1/tree/master/p1-memory-game8.0">Memory Game Project Adress</a><a href="htmlpreview.github.io/?github.com/SauronLee/Udacity_WEB_P1/blob/master/p1-memory-game8.0/index.html">index.html</a></p>
<!-- <script src="https://gist.github.com/SauronLee/e985e5b9829b76457c8af904f87238f0.js"></script> -->

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/04/16/Feed-Reader-Testing-Project/">Feed Reader Testing Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/04/16/Feed-Reader-Testing-Project/" class="archive-article-date"><time datetime="2018-04-15T16:20:01.000Z" itemprop="datePublished">April 16th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_WEB_P3/tree/master/Feed%20Reader%20Testing">Feed Reader Testing Project Adress</a></p>
<!-- <script src="https://gist.github.com/SauronLee/793cd088249994cdc9c7b31c3be364bb.js"></script> -->
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/04/16/Arcade-Game-Project/">Arcade Game Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/04/16/Arcade-Game-Project/" class="archive-article-date"><time datetime="2018-04-15T16:10:01.000Z" itemprop="datePublished">April 16th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_WEB_P2/tree/master/Arcade%20Game%20Clone_zh">Arcade Game Project Adress</a><a href="htmlpreview.github.io/?https://github.com/SauronLee/Udacity_WEB_P1/blob/master/p1-memory-game8.0/index.html">index.html</a></p>
<!-- <script src="https://gist.github.com/SauronLee/042ecef0fa6a26a731926f0792925a6d.js"></script> -->
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/03/11/Google-Map-Project/">Google Map Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/03/11/Google-Map-Project/" class="archive-article-date"><time datetime="2018-03-10T18:20:01.000Z" itemprop="datePublished">March 11th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_WEB_P4/tree/master/p4_google_map">Google Map Project Adress</a></p>
<!-- <script src="https://gist.github.com/SauronLee/1471fff841bd7f3f7f2a9d45ade94071.js"></script> -->
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/03/09/git-push-error-22/">github operation timed out</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/03/09/git-push-error-22/" class="archive-article-date"><time datetime="2018-03-09T06:24:58.000Z" itemprop="datePublished">March 9th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="创建GitHub仓库"><a href="#创建GitHub仓库" class="headerlink" title="创建GitHub仓库"></a>创建GitHub仓库</h1><p>检查用户目录下有没有.ssh文件，如果没有执行以下操作</p>
<pre><code>git config user.name “SauronLee” 
git config user.email “xxx@xxx.com” 

# 查看是否存在

git config user.name 
git config user.email
</code></pre><h2 id="生成ssh"><a href="#生成ssh" class="headerlink" title="生成ssh"></a>生成ssh</h2><pre><code>ssh-keygen -t rsa -C “你刚才设置过得邮箱”
</code></pre><p>无需设置密码和名字</p>
<p>找到ssh目录下pub文件复制到github上</p>
<pre><code>ssh -T git@git.oschina.net
</code></pre><h2 id="测试是否能链接上"><a href="#测试是否能链接上" class="headerlink" title="测试是否能链接上"></a>测试是否能链接上</h2><p>如果出现</p>
<ul>
<li>ssh: connect to host github.com port 22: Operation timed out<br>fatal: Could not read from remote repository.</li>
</ul>
<p>Please make sure you have the correct access rights<br>and the repository exists.</p>
<p>在ssh文件下创建config文本，内容如下：</p>
<pre><code>Host github.com
User xxx@xxx.com
Hostname ssh.github.com
PreferredAuthentications publickey
IdentityFile ~/.ssh/id_rsa
Port 443
</code></pre><p>再次测试链接：</p>
<pre><code>ssh -T git@github.com
</code></pre><p>会出现以下内容</p>
<pre><code>The authenticity of host &#39;[ssh.github.com]:443 ([192.30.253.123]:443)&#39; can&#39;t be established.
RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &#39;[ssh.github.com]:443,[192.30.253.123]:443&#39; (RSA) to the list of known hosts.
Hi SauronLee! You&#39;ve successfully authenticated, but GitHub does not provide shell access.
</code></pre><hr>
<p>再次链接通过，上传成功</p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2018/02/13/Front-End-Certificate-by-Udacity/">Front End Certificate by Udacity</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2018/02/13/Front-End-Certificate-by-Udacity/" class="archive-article-date"><time datetime="2018-02-13T07:35:03.000Z" itemprop="datePublished">February 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="https://i.imgur.com/DFzlaLx.png" alt></p>

      
    </div>
  </header>
</article>


  
    
    
      
        </div></section>
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <a href="/archives/2017" class="archive-year">2017</a>
        </div>
        <div class="archives">
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/12/17/Boston-Housing-Project/">Boston Housing Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/12/17/Boston-Housing-Project/" class="archive-article-date"><time datetime="2017-12-17T03:12:33.000Z" itemprop="datePublished">December 17th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_ML_P5/tree/master/p5_boston2.0">Boston Housing Project Adress</a></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/12/13/Titanic-Project/">Titanic Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/12/13/Titanic-Project/" class="archive-article-date"><time datetime="2017-12-12T17:22:17.000Z" itemprop="datePublished">December 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_ML_P4/blob/master/P4_titanic3.0/P4_titanic.ipynb">Titanic Project Adress</a></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/12/13/Linear-Regression-Project/">Linear Regression Project</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/12/13/Linear-Regression-Project/" class="archive-article-date"><time datetime="2017-12-12T17:20:01.000Z" itemprop="datePublished">December 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/SauronLee/Udacity_ML_P3/blob/master/linear_regression_P3/linear_regression_project-2-3.ipynb">Linear Regression Project Adress</a></p>
<!-- <script src="https://gist.github.com/SauronLee/87b33007784de2850480249697949306.js"></script>

<script src="https://gist.github.com/SauronLee/98d5d077feba14ba914c7c92742cdaec.js"></script> -->
      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/10/13/ML-Certificate-by-Udacity/">ML Certificate by Udacity</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/10/13/ML-Certificate-by-Udacity/" class="archive-article-date"><time datetime="2017-10-13T07:35:03.000Z" itemprop="datePublished">October 13th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="https://i.imgur.com/jgIDkPF.png" alt> </p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/08/15/Bash-Keyboard-Shortcuts/">Bash Keyboard Shortcuts</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/08/15/Bash-Keyboard-Shortcuts/" class="archive-article-date"><time datetime="2017-08-15T10:01:36.000Z" itemprop="datePublished">August 15th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <p>Bash Keyboard Shortcuts<br>Moving the cursor:<br>  Ctrl + a   Go to the beginning of the line (Home)<br>  Ctrl + e   Go to the End of the line (End)<br>  Ctrl + p   Previous command (Up arrow)<br>  Ctrl + n   Next command (Down arrow)<br>   Alt + b   Back (left) one word<br>   Alt + f   Forward (right) one word<br>  Ctrl + f   Forward one character<br>  Ctrl + b   Backward one character<br>  Ctrl + xx  Toggle between the start of line and current cursor position<br>Editing:<br> Ctrl + L   Clear the Screen, similar to the clear command</p>
<p>  Alt + Del Delete the Word before the cursor.<br>  Alt + d   Delete the Word after the cursor.<br> Ctrl + d   Delete character under the cursor<br> Ctrl + h   Delete character before the cursor (Backspace)</p>
<p> Ctrl + w   Cut the Word before the cursor to the clipboard.<br> Ctrl + k   Cut the Line after the cursor to the clipboard.<br> Ctrl + u   Cut/delete the Line before the cursor to the clipboard.</p>
<p>  Alt + t   Swap current word with previous<br> Ctrl + t   Swap the last two characters before the cursor (typo).<br> Esc  + t   Swap the last two words before the cursor.</p>
<p> ctrl + y   Paste the last thing to be cut (yank)<br>  Alt + u   UPPER capitalize every character from the cursor to the end of the current word.<br>  Alt + l   Lower the case of every character from the cursor to the end of the current word.<br>  Alt + c   Capitalize the character under the cursor and move to the end of the word.<br>  Alt + r   Cancel the changes and put back the line as it was in the history (revert).<br> ctrl + _   Undo</p>
<p> TAB        Tab completion for file/directory names<br>For example, to move to a directory ‘sample1’; Type cd sam ; then press TAB and ENTER.<br>type just enough characters to uniquely identify the directory you wish to open.</p>
<p>Special keys: Tab, Backspace, Enter, Esc<br>Text Terminals send characters (bytes), not key strokes.<br>Special keys such as Tab, Backspace, Enter and Esc are encoded as control characters.<br>Control characters are not printable, they display in the terminal as ^ and are intended to have an effect on applications.</p>
<p>Ctrl+I = Tab<br>Ctrl+J = Newline<br>Ctrl+M = Enter<br>Ctrl+[ = Escape</p>
<p>Many terminals will also send control characters for keys in the digit row:<br>Ctrl+2 → ^@<br>Ctrl+3 → ^[ Escape<br>Ctrl+4 → ^\<br>Ctrl+5 → ^]<br>Ctrl+6 → ^^<br>Ctrl+7 → ^_ Undo<br>Ctrl+8 → ^? Backward-delete-char</p>
<p>Ctrl+v tells the terminal to not interpret the following character, so Ctrl+v Ctrl-I will display a tab character,<br>similarly Ctrl+v ENTER will display the escape sequence for the Enter key: ^M</p>
<p>History:<br>  Ctrl + r   Recall the last command including the specified character(s)<br>             searches the command history as you type.<br>             Equivalent to : vim ~/.bash_history.<br>  Ctrl + p   Previous command in history (i.e. walk back through the command history)<br>  Ctrl + n   Next command in history (i.e. walk forward through the command history)</p>
<p>  Ctrl + s   Go back to the next most recent command.<br>             (beware to not execute it from a terminal because this will also launch its XOFF).<br>  Ctrl + o   Execute the command found via Ctrl+r or Ctrl+s<br>  Ctrl + g   Escape from history searching mode<br>        !!   Repeat last command<br>      !n     Repeat from the last command: args n e.g. !:2 for the second argumant.<br>      !n:m   Repeat from the last command: args from n to m. e.g. !:2-3 for the second and third.<br>      !n:$   Repeat from the last command: args n to the last argument.<br>      !n:p   Print last command starting with n<br>        !$   Last argument of previous command<br>   ALT + .   Last argument of previous command<br>        !*   All arguments of previous command<br>^abc­^­def   Run previous command, replacing abc with def<br>Process control:<br> Ctrl + C   Interrupt/Kill whatever you are running (SIGINT)<br> Ctrl + l   Clear the screen<br> Ctrl + s   Stop output to the screen (for long running verbose commands)<br>            Then use PgUp/PgDn for navigation<br> Ctrl + q   Allow output to the screen (if previously stopped using command above)<br> Ctrl + D   Send an EOF marker, unless disabled by an option, this will close the current shell (EXIT)<br> Ctrl + Z   Send the signal SIGTSTP to the current task, which suspends it.<br>            To return to it later enter fg ‘process name’ (foreground).<br>Emacs mode vs Vi Mode<br>All the above assume that bash is running in the default Emacs setting, if you prefer this can be switched to Vi shortcuts instead.</p>
<p>Set Vi Mode in bash:</p>
<p>$ set -o vi<br>Set Emacs Mode in bash:</p>
<p>$ set -o emacs </p>
<p>Article from ：<a href="https://ss64.com/bash/syntax-keyboard.html" target="_blank" rel="noopener">https://ss64.com/bash/syntax-keyboard.html</a></p>
<p> 常用<br>ctrl+左右键:在单词之间跳转<br>ctrl+a:跳到本行的行首<br>ctrl+e:跳到页尾<br>Ctrl+u：删除当前光标前面的文字 （还有剪切功能）<br>ctrl+k：删除当前光标后面的文字(还有剪切功能)<br>Ctrl+L：进行清屏操作<br>Ctrl+y:粘贴Ctrl+u或ctrl+k剪切的内容<br>Ctrl+w:删除光标前面的单词的字符<br>Alt – d ：由光标位置开始，往右删除单词。往行尾删<br>说明<br>Ctrl – k: 先按住 Ctrl 键，然后再按 k 键；<br>Alt – k: 先按住 Alt 键，然后再按 k 键；<br>M – k：先单击 Esc 键，然后再按 k 键。<br>移动光标<br>Ctrl – a ：移到行首<br>Ctrl – e ：移到行尾<br>Ctrl – b ：往回(左)移动一个字符<br>Ctrl – f ：往后(右)移动一个字符<br>Alt – b ：往回(左)移动一个单词<br>Alt – f ：往后(右)移动一个单词<br>Ctrl – xx ：在命令行尾和光标之间移动<br>M-b ：往回(左)移动一个单词<br>M-f ：往后(右)移动一个单词<br>编辑命令<br>Ctrl – h ：删除光标左方位置的字符<br>Ctrl – d ：删除光标右方位置的字符（注意：当前命令行没有任何字符时，会注销系统或结束终端）<br>Ctrl – w ：由光标位置开始，往左删除单词。往行首删<br>Alt – d ：由光标位置开始，往右删除单词。往行尾删<br>M – d ：由光标位置开始，删除单词，直到该单词结束。<br>Ctrl – k ：由光标所在位置开始，删除右方所有的字符，直到该行结束。<br>Ctrl – u ：由光标所在位置开始，删除左方所有的字符，直到该行开始。<br>Ctrl – y ：粘贴之前删除的内容到光标后。<br>ctrl – t ：交换光标处和之前两个字符的位置。<br>Alt + . ：使用上一条命令的最后一个参数。<br>Ctrl – _ ：回复之前的状态。撤销操作。<br>Ctrl -a + Ctrl -k 或 Ctrl -e + Ctrl -u 或 Ctrl -k + Ctrl -u 组合可删除整行。</p>
<p>Bang(!)命令<br>!! ：执行上一条命令。<br>^foo^bar ：把上一条命令里的foo替换为bar，并执行。<br>!wget ：执行最近的以wget开头的命令。<br>!wget:p ：仅打印最近的以wget开头的命令，不执行。<br>!$ ：上一条命令的最后一个参数， 与 Alt - . 和 $_ 相同。<br>!<em> ：上一条命令的所有参数<br>!</em>:p ：打印上一条命令是所有参数，也即 !*的内容。<br>^abc ：删除上一条命令中的abc。<br>^foo^bar ：将上一条命令中的 foo 替换为 bar<br>^foo^bar^ ：将上一条命令中的 foo 替换为 bar<br>!-n ：执行前n条命令，执行上一条命令： !-1， 执行前5条命令的格式是： !-5<br>查找历史命令<br>Ctrl – p ：显示当前命令的上一条历史命令<br>Ctrl – n ：显示当前命令的下一条历史命令<br>Ctrl – r ：搜索历史命令，随着输入会显示历史命令中的一条匹配命令，Enter键执行匹配命令；ESC键在命令行显示而不执行匹配命令。<br>Ctrl – g ：从历史搜索模式（Ctrl – r）退出。<br>控制命令<br>Ctrl – l ：清除屏幕，然后，在最上面重新显示目前光标所在的这一行的内容。<br>Ctrl – o ：执行当前命令，并选择上一条命令。<br>Ctrl – s ：阻止屏幕输出<br>Ctrl – q ：允许屏幕输出<br>Ctrl – c ：终止命令<br>Ctrl – z ：挂起命令<br>重复执行操作动作<br>M – 操作次数 操作动作 ： 指定操作次数，重复执行指定的操作。</p>
<p>引用来自：<a href="https://ss64.com/bash/syntax-keyboard.html" target="_blank" rel="noopener">https://ss64.com/bash/syntax-keyboard.html</a></p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/06/01/xgs03/">模型评估与选择（2）</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/06/01/xgs03/" class="archive-article-date"><time datetime="2017-06-01T08:02:18.000Z" itemprop="datePublished">June 1st</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>性能度量(performance measure):衡量模型泛化能力的评价标准,性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往 往会导致不同的评判结果;这意味着模型的”好坏”是相对的，什么样的模型 是好的?不仅取决于算法和数据，还决定于任务需求.<br>在预测任务中?给定样例集 D = {(X1,Y1), (X2,Y2)， . . . , (Xm,Ym)}， 其中Yi是示例 Xi 的真实标记.要评估学习器f的性能，就要把学习器预测结果I(x)与真实标记y进行比较.<br>回归任务最常用的性能度量是”均方误差” (mean squared error)</p>
<script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_{i})-y_{i})^{2}</script><h3 id="错误率与精度"><a href="#错误率与精度" class="headerlink" title="错误率与精度"></a>错误率与精度</h3><p>本章开头提到了错误率和精度?这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务.错误率是分类错误的样本数占样 本总数的比例，精度则是分类正确的样本数占样本总数的比例.对样例集 D，<br>分类错误率定义为</p>
<script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_{i})\neq y_{i})</script><p>精度则定义为</p>
<script type="math/tex; mode=display">acc(f:D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_{i})= y_{i})</script><script type="math/tex; mode=display">=1-E(f;D)</script><h3 id="查准率、查全率"><a href="#查准率、查全率" class="headerlink" title="查准率、查全率"></a>查准率、查全率</h3><p>错误率和精度虽常用，但并不能满足所有任务需求.以西瓜问题为例，假定 瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率衡 量了有多少比例的瓜被判别错误.但是若我们关心的是”挑出的西瓜中有多少 比例是好瓜”，或者”所有好瓜中有多少比例被挑了出来就不够用了’这时需要使用其他的性能度量.类似的需求在信息检索，Web搜索等应用中经常出现?例如在信息检索中，我们经常会关心”检索出的信息中有多少比例是用户感兴趣的” 用户感兴趣的信息中有多少被检索出来了 查准率和查全率” 是更为适用于此类需求的性能度量.查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往 偏低;而查全率高时，查准率往往偏低.</p>
<h3 id="P-R-图"><a href="#P-R-图" class="headerlink" title="P-R 图"></a>P-R 图</h3><p>P-R 图直观地显示出学习器在样本总体上的查全率、 查准率.在进行比较 时?若一个学习器的 P-R 曲线被另一个学习器的曲线完全”包住” ， 则可断言 后者的性能优于前者， 例如图 2.3 中学习器 A 的性能优于学习器 C; 如果两个 学习器的 P-R 曲线发生了交叉, 中的 A 与 B ，则难以-般性地断言 两者孰优孰劣?只能在具体的查准率或查全率条件下进行比较然而，在很多情形下，人们往往仍希望把学习器 A 与 B 比出个高低. 这时一个比较合理的判据 是比较 P-R 曲线节面积的大小，它在一定程度上表征了学习器在查准率和查全 率上取得相对”双高”的比例.但这个值不太容易估算， 因此7 人们设计了一些 综合考虑查准率、 查全率的性能度量.<br>“平衡点 “ (Break-Event Point，简称 BEP)就是这样一个度量，它是”查 准率=查全率”时的取值3 例如图 2.3 中学习器 C 的 BEP 是 0.64，而基于 BEP 的比较，可认为学习器 A 优于 B .但 BEP 还是过于简化了些，更常用的是 F1 度量:</p>
<script type="math/tex; mode=display">F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{Case number+TP-TN}</script><h3 id="ROC-与-AUC"><a href="#ROC-与-AUC" class="headerlink" title="ROC 与 AUC"></a>ROC 与 AUC</h3><p>ROC 全称是”受试者工作特征” (Receiver Operating Characteristic) 曲 线7 它源于”二战”中用于敌机检测的雷达信号分析技术，二十世纪六七十 年代开始被用于→些心理学、医学检测应用中?此后被引入机器学习领域 [Spackman, 1989]. P-R 曲线相似我们根据学习器的预 测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算 出两个重要量的值，分别以它们为横、纵坐标作图’就得到了 “ROC曲线 与 P-R 曲线使用查准率、查全率为纵、横轴不同，ROC曲线的纵轴是”真正 例率” (True Positive Rate，简称 TPR)，横轴是”假正例率” (False Positive Rate，简称 FPR)，基于表 2.1 中的符号，两者分别定义为</p>
<script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><script type="math/tex; mode=display">TPR=\frac{FP}{TN+FP}</script><p>显示 ROC 曲线的图称为 “ROC 图”，对角线对应于 “随机猜测” 模型，而点 (0， 1) 则对应于将所有正例排在所有反 例之前的”理想模型”</p>
<p>代价敏感错误率与代价曲线</p>
<p>不同类型的错误所造成的后果不同. 例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者， 看起来都是犯了”一次错误”但后者的影响是增加了进→步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机，为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” (unequa cost).<br>在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而 “代价曲线” (cost curve) 则可达到该目的.代价曲线图的横轴是取值为 [0， 1] 的正例概率代价</p>
<script type="math/tex; mode=display">P(+)cost=\frac{p\timescost_{01}}{p\timescost_{01}+(1-p)\timescost_{10}}</script><p>其中 p 是样例为正例的概率;纵轴是取值为 [0， 1] 的归一化代价</p>
<script type="math/tex; mode=display">P(+)cost=\frac{p\timescost_{01}+FPR\times(1-p)\timescost_{10}}{p\timescost_{01}+(1-p)\timescost_{10}}</script><p>其中 FPR 是式(2.19)定义的假正例率， FNR = 1 - TPR 是假反例率.代价曲线 的绘制很简单: ROC曲线上每1点对应了代价平面上的1条线段7 设 ROC 曲 线上点的坐标为 (TPR， FPR)，则可相应计算出 FNR，然后在代价平面上绘制 一条从 (0， FPR) 到 (1， FNR) 的线段，线段下的面积即表示了该条件下的期望 总体代价;如此将 ROC 曲线土的每个点转化为代价平面上的一条线段，然后 取所有线段的下界，围成的自积即为在所有条件下学习器的期望总体代价</p>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/05/14/xgs02/">模型评估与选择</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/05/14/xgs02/" class="archive-article-date"><time datetime="2017-05-14T07:59:05.000Z" itemprop="datePublished">May 14th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h3><ul>
<li>错误率 (error rate):分类错误的样本数占样本总数的比例，即如果在 m 个样本中有 α 个样本分类错误，则错误率 E= α/m;<br>精度(accuracy):1-α/m，即”精度=1-错误率”</li>
<li>误差 (error):学习器的实际预测输出与样本的真实输出之间的差异</li>
<li>训练误差 (training error)或经验误差 (empirical error):学习器在训练集上的误差</li>
<li>泛化误差 (generalization error):，在新样本上的误差<br>过拟合 (overfitting):，当学习器把训练样本学得”太 好”了的时候，很可能巳经把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下(有多种因素可能导致过拟合，其中最常见的情况是由于学习能力过于强大， 以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则通常是由 于学习能力低下而造成的欠拟合比较容易克服，例如在决策树学习中扩展分 支、在神经网络学习中增加训练轮数等，而过拟合则很麻烦.在后面的学习中 我们将看到，过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一 些针对过拟合的措施)</li>
<li>欠拟合 (underfitting):这是指对训练样本的一般性质尚未学好</li>
<li>模型选择(model selection):理想的解决方案当然是对候选模型的泛化误差进行评估7 然后 选择泛化误差最小的那个模型</li>
</ul>
<h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><ul>
<li>留出法 (hold-out):直接将数据集D划分为两个互斥的集合?其中一个 集合作为训练集5,另一个作为测试集T， 即 D=BUT，SNT= 空集.在 S 上训 练出模型后，用T来评估其测试误差，作为对泛化误差的估计.(以二分类任务为例，假定D包含1000个样本，将其划分为8包含 700个样本,T包含300个样本，用S进行训练后,如果模型在T上有90个样本分类错误，那么其错误率为(90/300)x100% = 30%，相应的，精度为 1-30%=70%. )</li>
<li><p>交叉验证法 (cross validation)先将数据集D划分k个大小相似的 互斥子集，即D=D1 U D2 U… U Dk, Di n Dj = ø (i≠j). 每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到. 然后，每次用k-1个子集的并集作为训练集?余F的那个子集作为测试集;这样就可获得k组训练/测试集，从而可进行k次训练和测试? 最终返回的是这k个测试结果的均值 显然交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为”k折交叉验证” (k-fold cross validation).k最常用的取值是10，此时称为10折交叉验证</p>
</li>
<li><p>自助法:它直接以自助采样 法(bootstrap sampling)为基础 [Efron and Tibshirani, 1993]. 给定包含 m 个样 本的数据集 D ， 我们对它进行采样产生数据集 D’: 每次随机从 D 中挑选一个 样本7 将其拷贝放入 DF’ 然后再将该样本放回初始数据集 D 中，使得该样本在 下次采样时仍有可能被采到;这个过程重复执行 m 次后?我们就得到了包含 m 个样本的数据集 DF，这就是自助采样的结果.显然， D 中有一部分样本会在 D’ 中多次出现，而另一部分样本不出现.可以做一个简单的估计，样本在 m 次采 样中始终不被采到的概率是(1-1/m)^m， 取极限得到即通过自助来样，初始数据集 D 中约有 36.8% 的样本未出现在采样数据集 D’ 中.于是我们可将 D’ 用作训练、集， D\D’ 用作测试集;这样?实际评估的模型与 期望评估的模型都使用 m 个训练、样本，而我们仍有数据总量约 1/3 的、没在训 练集中出现的样本用于测试.这样的测试结果，亦称”包外估计” (out-of-bag<br>estimate).</p>
</li>
</ul>
<h3 id="调参与最终模型"><a href="#调参与最终模型" class="headerlink" title="调参与最终模型"></a>调参与最终模型</h3><ul>
<li>调参(parameter tuning):大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模 型的性能往往有显著差别.因此，在进行模型评估与选择时，除了要对适用学习 算法进行选择，还需对算法参数进行设定，这就是通常所说的”参数调节”</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2017/05/04/xgs01/">基本术语</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2017/05/04/xgs01/" class="archive-article-date"><time datetime="2017-05-04T07:49:43.000Z" itemprop="datePublished">May 4th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><hr>
<ul>
<li>机器学习：致力于研究如何通过计算手段，利用经验来改善系统自身的性能，在计算机中，“经验”通常以“数据”的形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据产生“模型”（model）的算法，即“机器学习”算法（learning algorithm）.</li>
</ul>
<hr>
<h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><hr>
<ul>
<li>数据集（data set）： 记录一组数据的集合比如所有西瓜的声音，根蒂，颜色</li>
<li>示例（instance）或者 样本（sample）：每个西瓜都是一个实例或样本</li>
<li>属性（attribute）或者特征（feature）：每个西瓜的声音，根蒂，颜色就是西瓜的属性或特征</li>
<li>分类（classification）：预测的数据为离散型数据</li>
<li>回归（regression）：预测的数据为连续型数据</li>
<li>测试（testing）：学得模型后，使用其进行预测的过程</li>
<li>预测样本（testing sample）：被预测的样本</li>
<li>聚类（clustering），在非监督学习中区分每组数据，即将训练集中西瓜分成若干组，每组称为一个“簇”（cluster），这些自动形成的簇可能对应潜在的概念划分，例如“浅色瓜”“深色瓜”等。</li>
<li>监督学习（supervised learning）：训练数据有标记信息，使用回归和分类算法</li>
<li>无监督学习（unsupervised learning）：训练数据无标记信息，使用聚类算法</li>
<li>泛化（generalization）：学得的模型适用于新样本的能力</li>
</ul>
<hr>
<h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><hr>
<ul>
<li>归纳（induction）：从特殊到一般的泛化（generalization）过程</li>
<li>演绎（deduction）：从一般到特殊的特化（specialization）过程</li>
<li>归纳学习（induction learning）：归纳学习有广义和狭义之分，广义的归纳学习大体相当于从样本中学习，而狭义的归纳学习则要从训练数据中学得概念（concept），因此有叫做概念学习</li>
</ul>
<hr>
<h3 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h3><hr>
<ul>
<li>归纳偏好（inductive bias）当有与训练集一致的假设，但与他们对应的模型在面临新样本的时候会产生不同的输出，那么我们应该采取哪个模型？这时学习算法本身的偏好就会起到关键的作用，任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似训练集上等效的假设所迷惑而无法产生确定的结果.</li>
<li>奥卡姆剃刀（Occam’s razor)：若有多个假设与观察一致，则选择最简单的那个，但是如何判断哪个更简单则不是个简单的问题</li>
<li>NFL定理（No Free Lunch Theorem）：要谈论算法的相对优劣，必须要针对具体的学习问题，学习算法自身的归纳偏好与问题是否相配，往往会起到觉得决定性作用</li>
</ul>
<hr>
<h3 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h3><hr>
<ul>
<li>二十世纪五十年代到七十年代人们认为只要赋予机器的逻辑推理能力机器就赋予只能后来人们逐渐意识到把总结的知识交给机器是十分困难的<br>1950年：图灵提到机器学习可能.代表：A.Samuel跳棋程序</li>
<li>五十年代中后期：基于神经网络的联接主义出现.代表F.Rosenblatt的感知机（Perceptron）、B.Widrow的Adaline等</li>
<li>六七十年代：基于符号主义的学习技术蓬勃发展.代表P.Winston的结构学习系统、R.S.Michalski等人的基于逻辑的归纳学习系统，以决策论为基础的学习技术以及强化学习技术也得到蓬勃发展.如：N.J.Nilsom的机器学习等<br>1980年夏：美国卡耐基梅隆大学第一届机器学习研讨会（IWML）</li>
<li>二十世纪八十年代：从样例中学习的一大主流是符号主义学习比如决策树（decision tree）至今也是最常用的机器学习技术之一</li>
<li>二十世纪九十年代：神经网络的连接主义成为主流</li>
<li>二十世纪九十年代中期：统计学习成为主流代表：支持向量机以及一般的核方法</li>
<li>二十一世纪：多层深度神经网络成为主流</li>
</ul>
<hr>
<h3 id="应用现状"><a href="#应用现状" class="headerlink" title="应用现状"></a>应用现状</h3><hr>
<ul>
<li>人工智能，模式识别，数据挖掘，语音识别，图像识别，信息检索，生物信息，网络通信，软件工程，芯片设计，计算机视觉.总之就像爱迪生发明电</li>
</ul>

      
    </div>
  </header>
</article>


  
    
    
      
        </div></section>
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <a href="/archives/2016" class="archive-year">2016</a>
        </div>
        <div class="archives">
    
    <article class="archive-article archive-type-post">
  <header class="archive-article-header">
    
  
    <h1 itemprop="name">
      <a class="archive-article-title" href="/2016/05/10/Exponential-function-and-logarithm-of-PengTitus/">Exponential function  and logarithm of PengTitus</a>
    </h1>
  


    <div class="article-datetime">
  <a href="/2016/05/10/Exponential-function-and-logarithm-of-PengTitus/" class="archive-article-date"><time datetime="2016-05-10T09:42:03.000Z" itemprop="datePublished">May 10th</time></a>
</div>

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="指数exponent与对数概论"><a href="#指数exponent与对数概论" class="headerlink" title="指数exponent与对数概论"></a>指数exponent与对数概论</h2><ul>
<li>对数就是一个函数</li>
<li>指数的的作用是把自然数成倍增长，而对数的作用就是取消指数对自然数的增长，主要为了简化天文学计算</li>
</ul>
<p>比如：没人可以把一本书折叠9次：因为2的9次方是512，相当于一部字典的厚度<br>把一张纸折叠42次后的高度比地球到月球的距离还要高</p>
<h2 id="指数律-a-m-a-n-a-m-n"><a href="#指数律-a-m-a-n-a-m-n" class="headerlink" title="指数律$(a^{m})(a^{n})=a^{m+n}$"></a>指数律$(a^{m})(a^{n})=a^{m+n}$</h2><p><img src="/images/Exponential&amp;logarithmPengTitus/01.png" alt><br>为什么$a^{0}=1$<br><img src="/images/Exponential&amp;logarithmPengTitus/02.png" alt><br>为什么$a^{-n}=\frac{1}{a^{n}}$<br><img src="/images/Exponential&amp;logarithmPengTitus/03.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/05.png" alt><br>m,n全部为负数时依然成立<br><img src="/images/Exponential&amp;logarithmPengTitus/04.png" alt></p>
<h3 id="指数律-a-m-n-a-nm-与-ab-n-a-n-cdot-b-n"><a href="#指数律-a-m-n-a-nm-与-ab-n-a-n-cdot-b-n" class="headerlink" title="指数律$(a^{m})^{n}=a^{nm}与(ab)^{n}=a^{n}\cdot b^{n}$"></a>指数律$(a^{m})^{n}=a^{nm}与(ab)^{n}=a^{n}\cdot b^{n}$</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/06.png" alt></p>
<h3 id="指数律-有理数指数-分数指数"><a href="#指数律-有理数指数-分数指数" class="headerlink" title="指数律_有理数指数(分数指数)"></a>指数律_有理数指数(分数指数)</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/07.png" alt></p>
<h3 id="指数不等式"><a href="#指数不等式" class="headerlink" title="指数不等式"></a>指数不等式</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/08.png" alt><br>根据算几不等式可看出<br><img src="/images/Exponential&amp;logarithmPengTitus/09.png" alt></p>
<h3 id="对数Logarithm概论"><a href="#对数Logarithm概论" class="headerlink" title="对数Logarithm概论"></a>对数Logarithm概论</h3><p>对数Logarithm=Logos神的话，arithmos计算-John Napier<br>对数就是把次幂抓下来的工具<br><img src="/images/Exponential&amp;logarithmPengTitus/10.png" alt><br>求指数就是原函数的反函数<br><img src="/images/Exponential&amp;logarithmPengTitus/11.png" alt></p>
<h3 id="对数的定义和指数反函数的性质"><a href="#对数的定义和指数反函数的性质" class="headerlink" title="对数的定义和指数反函数的性质"></a>对数的定义和指数反函数的性质</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/12.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/13.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/14.png" alt></p>
<h3 id="对数律logarithmic-identities"><a href="#对数律logarithmic-identities" class="headerlink" title="对数律logarithmic identities"></a>对数律logarithmic identities</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/15.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/16.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/17.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/18.png" alt></p>
<h3 id="对数的换底法则和推论"><a href="#对数的换底法则和推论" class="headerlink" title="对数的换底法则和推论"></a>对数的换底法则和推论</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/19.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/20.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/21.png" alt><br><img src="/images/Exponential&amp;logarithmPengTitus/22.png" alt></p>
<h3 id="e"><a href="#e" class="headerlink" title="e"></a>e</h3><p><img src="/images/Exponential&amp;logarithmPengTitus/23.png" alt></p>

      
    </div>
  </header>
</article>


  
  
    </div></section>
  



        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p> Domain Tags Machine Learning, Deep Learning,  Front End Development, iOS Development, Statistics， <em>Python</em>, <em>R</em>, <em>Flutter</em>, <em>Swift</em>, <em>JavaScript</em>.</p>

</div>


  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/math/">math</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/news/">news</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/project/">project</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/projects/">projects</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/reading-notes/">reading_notes</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>




  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ANN/">ANN</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Calculus/">Calculus</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Computer-Vision/">Computer Vision</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Data-cleaning/">Data cleaning</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JavaScript/">JavaScript</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="sidebar-module-list-count">51</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/NLP/">NLP</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Python/">Python</a><span class="sidebar-module-list-count">22</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/R/">R</a><span class="sidebar-module-list-count">15</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Statistics/">Statistics</a><span class="sidebar-module-list-count">51</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/english/">english</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/git/">git</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/leetcode/">leetcode</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/linear-algebra/">linear algebra</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/paper/">paper</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/python/">python</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/ANN/" style="font-size: 10px;">ANN</a> <a href="/tags/Calculus/" style="font-size: 15px;">Calculus</a> <a href="/tags/Computer-Vision/" style="font-size: 10px;">Computer Vision</a> <a href="/tags/Data-cleaning/" style="font-size: 13.75px;">Data cleaning</a> <a href="/tags/Deep-Learning/" style="font-size: 11.25px;">Deep Learning</a> <a href="/tags/JavaScript/" style="font-size: 13.75px;">JavaScript</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/Python/" style="font-size: 18.75px;">Python</a> <a href="/tags/R/" style="font-size: 17.5px;">R</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 16.25px;">Reinforcement Learning</a> <a href="/tags/Statistics/" style="font-size: 20px;">Statistics</a> <a href="/tags/english/" style="font-size: 10px;">english</a> <a href="/tags/git/" style="font-size: 11.25px;">git</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/linear-algebra/" style="font-size: 12.5px;">linear algebra</a> <a href="/tags/paper/" style="font-size: 10px;">paper</a> <a href="/tags/python/" style="font-size: 10px;">python</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/12/">December 2020</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/11/">November 2020</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/10/">October 2020</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/10/">October 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/09/">September 2019</a><span class="sidebar-module-list-count">30</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/08/">August 2019</a><span class="sidebar-module-list-count">17</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/07/">July 2019</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/06/">June 2019</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/05/">May 2019</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/02/">February 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/08/">August 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/05/">May 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/04/">April 2018</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/03/">March 2018</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/02/">February 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/12/">December 2017</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/10/">October 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/08/">August 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/06/">June 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/05/">May 2017</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/05/">May 2016</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2020/12/10/arXiv-1905-09866-cs-notes/">arXiv:1905.09866 [cs] notes</a>
        </li>
      
        <li>
          <a href="/2020/11/04/Lihang-ML-notes/">統計学習方法ノート</a>
        </li>
      
        <li>
          <a href="/2020/10/31/basic-math-for-ML/">basic math for ML</a>
        </li>
      
        <li>
          <a href="/2020/10/16/Sunannan-10-000-words/">Sunannan 10,000 words</a>
        </li>
      
        <li>
          <a href="/2019/10/05/EM-Arithmetic-by-Zou/">EM Arithmetic by Zou</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2020 Sauron Lee<br>
       <!-- <a href="https://i.imgur.com/S8qHH1C.jpg" target="_blank"><b>@Sauron Lee</b></a>
       - -->
       <a href="https://github.com/SauronLee" target="_blank"> <b>GitHub</b></a>
       -
       <a href="https://leetcode.com/sauronlee/" target="_blank"> <b>Leetcode</b></a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>

  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
